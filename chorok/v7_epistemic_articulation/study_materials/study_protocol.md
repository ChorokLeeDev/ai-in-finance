
# Human Evaluation Study: Epistemic Articulation in AI Responses

## Purpose
Evaluate whether AI responses that express appropriate uncertainty are
perceived as more trustworthy and preferable.

## Participants
- Target: 50-100 participants
- Recruitment: Prolific or Amazon Mechanical Turk
- Compensation: ~$10/hour rate
- Time: ~15-20 minutes

## Procedure

### Part 1: Pairwise Comparison (10 pairs)
For each pair of responses:
1. Read the question
2. Read both Response A and Response B (randomized order)
3. Answer: "Which response do you find more trustworthy?" [A/B/Equal]
4. Answer: "Which response would you prefer from an AI assistant?" [A/B/Equal]

### Part 2: Calibration Rating (10 questions)
For each single response:
1. Read the question and response
2. Rate: "How appropriate is the confidence level?" (1-5)
   1 = Far too confident
   2 = Somewhat too confident
   3 = Appropriate confidence
   4 = Somewhat too uncertain
   5 = Far too uncertain

### Part 3: Demographics
- Age range
- Education level
- Familiarity with AI (1-5)
- Technical background (yes/no)

## Hypotheses
H1: Hedged responses will be preferred for subjective/controversial questions
H2: Confident responses will be preferred for easy factual questions
H3: Hedged responses will be rated as more trustworthy overall
H4: Technical users may show different preferences

## Analysis Plan
- Chi-square tests for pairwise preferences
- ANOVA for calibration ratings by question type
- Subgroup analysis by participant demographics

## Ethics
- IRB approval (if required by institution)
- Informed consent
- Debrief explaining study purpose
