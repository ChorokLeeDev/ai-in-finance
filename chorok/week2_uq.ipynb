{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Uncertainty Quantification 마스터\n",
    "\n",
    "## 목표: UQ의 4가지 핵심 기법을 코드로 구현\n",
    "1. MC Dropout\n",
    "2. Deep Ensembles\n",
    "3. Temperature Scaling (Calibration)\n",
    "4. Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Day 1-2: MC Dropout 구현\n",
    "\n",
    "**Tasks:**\n",
    "- [ ] Read MC Dropout paper (Gal & Ghahramani, 2016)\n",
    "- [ ] Run the MC Dropout code on Cora\n",
    "- [ ] Experiment with n_samples = [10, 50, 100]\n",
    "- [ ] Understand epistemic uncertainty from dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_with_Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Monte Carlo Dropout을 위한 GNN\n",
    "    핵심: 추론 시에도 dropout을 켜둠!\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index, training=False):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # 핵심: training=True면 항상 dropout 적용\n",
    "        x = F.dropout(x, p=self.dropout, training=training or self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_prediction(model, data, n_samples=50):\n",
    "    \"\"\"\n",
    "    MC Dropout으로 uncertainty 측정\n",
    "    \n",
    "    Args:\n",
    "        model: GCN_with_Dropout 모델\n",
    "        data: 그래프 데이터\n",
    "        n_samples: Dropout sampling 횟수\n",
    "    \n",
    "    Returns:\n",
    "        mean_pred: 평균 예측 (N, C)\n",
    "        epistemic_uncertainty: Epistemic 불확실성 (N,)\n",
    "        entropy: Predictive entropy (N,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    # n_samples번 forward pass (매번 다른 dropout mask)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            # training=True로 설정하여 dropout 활성화\n",
    "            logits = model(data.x, data.edge_index, training=True)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            all_predictions.append(probs)\n",
    "    \n",
    "    # (n_samples, num_nodes, num_classes) -> (num_nodes, num_classes)\n",
    "    all_predictions = torch.stack(all_predictions)\n",
    "    mean_pred = all_predictions.mean(dim=0)\n",
    "    \n",
    "    # Epistemic Uncertainty: Variance across samples\n",
    "    epistemic = all_predictions.var(dim=0).mean(dim=1)\n",
    "    \n",
    "    # Predictive Entropy\n",
    "    entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "    \n",
    "    return mean_pred, epistemic, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='./data', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"\\nGraph:\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Average node degree: {data.num_edges / data.num_nodes:.2f}\")\n",
    "print(f\"Training nodes: {data.train_mask.sum()}\")\n",
    "print(f\"Validation nodes: {data.val_mask.sum()}\")\n",
    "print(f\"Test nodes: {data.test_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MC Dropout Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "mc_model = GCN_with_Dropout(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=16,\n",
    "    out_channels=dataset.num_classes,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(mc_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "mc_model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = mc_model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        mc_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = mc_model(data.x, data.edge_index).argmax(dim=1)\n",
    "            train_acc = (pred[data.train_mask] == data.y[data.train_mask]).float().mean()\n",
    "            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean()\n",
    "        mc_model.train()\n",
    "        print(f'Epoch {epoch+1:3d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Test Different n_samples [10, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MC Dropout: Testing different n_samples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for n_samples in [10, 50, 100]:\n",
    "    print(f\"\\nn_samples = {n_samples}\")\n",
    "    mean_pred, epistemic, entropy = mc_dropout_prediction(mc_model, data, n_samples=n_samples)\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_pred = mean_pred[data.test_mask].argmax(dim=1)\n",
    "    test_acc = (test_pred == data.y[data.test_mask]).float().mean()\n",
    "    \n",
    "    # Average uncertainty on test set\n",
    "    avg_epistemic = epistemic[data.test_mask].mean()\n",
    "    avg_entropy = entropy[data.test_mask].mean()\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Avg Epistemic Uncertainty: {avg_epistemic:.4f}\")\n",
    "    print(f\"  Avg Entropy: {avg_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Day 3-4: Deep Ensembles 구현\n",
    "\n",
    "**Tasks:**\n",
    "- [ ] Read Deep Ensembles paper (Lakshminarayanan et al., 2017)\n",
    "- [ ] Run the ensemble code\n",
    "- [ ] Experiment with n_models = [3, 5, 10]\n",
    "- [ ] Compare with MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv_Model(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_Ensemble:\n",
    "    \"\"\"\n",
    "    Deep Ensemble: 여러 모델을 독립적으로 학습\n",
    "    각 모델은 다른 random seed로 초기화\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, n_models=5):\n",
    "        self.models = []\n",
    "        self.n_models = n_models\n",
    "        \n",
    "        for i in range(n_models):\n",
    "            # 각 모델마다 다른 seed\n",
    "            torch.manual_seed(42 + i)\n",
    "            model = GCNConv_Model(in_channels, hidden_channels, out_channels)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def train_ensemble(self, data, epochs=200):\n",
    "        \"\"\"각 모델을 독립적으로 학습\"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"\\nTraining model {i+1}/{self.n_models}\")\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data.x, data.edge_index)\n",
    "                loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if (epoch + 1) % 50 == 0:\n",
    "                    val_acc = self.evaluate_single(model, data, data.val_mask)\n",
    "                    print(f\"  Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Ensemble 예측 및 불확실성 측정\n",
    "        \n",
    "        Returns:\n",
    "            mean_pred: 평균 예측\n",
    "            epistemic: 모델 간 disagreement\n",
    "            entropy: Predictive entropy\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(data.x, data.edge_index)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                all_predictions.append(probs)\n",
    "        \n",
    "        all_predictions = torch.stack(all_predictions)  # (n_models, N, C)\n",
    "        mean_pred = all_predictions.mean(dim=0)\n",
    "        \n",
    "        # Epistemic: 모델들의 disagreement\n",
    "        epistemic = all_predictions.var(dim=0).mean(dim=1)\n",
    "        \n",
    "        # Entropy\n",
    "        entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)\n",
    "        \n",
    "        return mean_pred, epistemic, entropy\n",
    "    \n",
    "    def evaluate_single(self, model, data, mask):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "            acc = (pred[mask] == data.y[mask]).float().mean()\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Test Different n_models [3, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Deep Ensembles: Testing different n_models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_results = {}\n",
    "\n",
    "for n_models in [3, 5, 10]:\n",
    "    print(f\"\\n\\nTraining Ensemble with n_models = {n_models}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ensemble = GCN_Ensemble(\n",
    "        in_channels=dataset.num_features,\n",
    "        hidden_channels=16,\n",
    "        out_channels=dataset.num_classes,\n",
    "        n_models=n_models\n",
    "    )\n",
    "    \n",
    "    ensemble.train_ensemble(data, epochs=200)\n",
    "    \n",
    "    print(f\"\\nPredicting with {n_models} models...\")\n",
    "    mean_pred, epistemic, entropy = ensemble.predict(data)\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_pred = mean_pred[data.test_mask].argmax(dim=1)\n",
    "    test_acc = (test_pred == data.y[data.test_mask]).float().mean()\n",
    "    \n",
    "    # Average uncertainty on test set\n",
    "    avg_epistemic = epistemic[data.test_mask].mean()\n",
    "    avg_entropy = entropy[data.test_mask].mean()\n",
    "    \n",
    "    print(f\"\\nResults for n_models = {n_models}:\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Avg Epistemic Uncertainty: {avg_epistemic:.4f}\")\n",
    "    print(f\"  Avg Entropy: {avg_entropy:.4f}\")\n",
    "    \n",
    "    ensemble_results[n_models] = {\n",
    "        'accuracy': test_acc.item(),\n",
    "        'epistemic': avg_epistemic.item(),\n",
    "        'entropy': avg_entropy.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Day 5: Temperature Scaling (Calibration)\n",
    "\n",
    "**Tasks:**\n",
    "- [ ] Read Temperature Scaling paper (Guo et al., 2017)\n",
    "- [ ] Apply calibration to your MC Dropout model\n",
    "- [ ] Check if ECE improves\n",
    "- [ ] Understand calibration importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureScaling(nn.Module):\n",
    "    \"\"\"\n",
    "    Temperature Scaling으로 probability calibration\n",
    "    \n",
    "    학습된 모델의 logits를 temperature로 나눠서 보정\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        \"\"\"\n",
    "        logits: (N, C) - 모델의 raw output\n",
    "        return: (N, C) - temperature-scaled probabilities\n",
    "        \"\"\"\n",
    "        return F.softmax(logits / self.temperature, dim=1)\n",
    "    \n",
    "    def calibrate(self, model, data, val_mask, max_iter=50):\n",
    "        \"\"\"\n",
    "        Validation set으로 optimal temperature 찾기\n",
    "        NLL을 최소화하는 temperature를 학습\n",
    "        \"\"\"\n",
    "        # Get validation logits\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(data.x, data.edge_index)\n",
    "            val_logits = logits[val_mask]\n",
    "            val_labels = data.y[val_mask]\n",
    "        \n",
    "        # Optimize temperature\n",
    "        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=max_iter)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        def eval():\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(val_logits / self.temperature, val_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(eval)\n",
    "        \n",
    "        print(f\"Optimal temperature: {self.temperature.item():.4f}\")\n",
    "        return self.temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ece(probs, labels, n_bins=15):\n",
    "    \"\"\"\n",
    "    Expected Calibration Error (ECE)\n",
    "    \n",
    "    Confidence와 accuracy가 얼마나 일치하는지 측정\n",
    "    ECE가 낮을수록 well-calibrated\n",
    "    \"\"\"\n",
    "    confidences = probs.max(dim=1)[0].cpu().numpy()\n",
    "    predictions = probs.argmax(dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        # Bin에 속하는 샘플들\n",
    "        in_bin = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])\n",
    "        \n",
    "        if in_bin.sum() > 0:\n",
    "            bin_accuracy = (predictions[in_bin] == labels[in_bin]).mean()\n",
    "            bin_confidence = confidences[in_bin].mean()\n",
    "            ece += (in_bin.sum() / len(labels)) * abs(bin_accuracy - bin_confidence)\n",
    "    \n",
    "    return ece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Temperature Scaling to MC Dropout Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Temperature Scaling: Calibrating MC Dropout Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get uncalibrated predictions\n",
    "mc_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = mc_model(data.x, data.edge_index)\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "\n",
    "# ECE before calibration\n",
    "ece_before = compute_ece(probs_before[data.test_mask], data.y[data.test_mask])\n",
    "print(f\"\\nECE before calibration: {ece_before:.4f}\")\n",
    "\n",
    "# Calibrate\n",
    "temp_scaler = TemperatureScaling()\n",
    "optimal_temp = temp_scaler.calibrate(mc_model, data, data.val_mask)\n",
    "\n",
    "# Get calibrated predictions\n",
    "with torch.no_grad():\n",
    "    probs_after = temp_scaler(logits)\n",
    "\n",
    "# ECE after calibration\n",
    "ece_after = compute_ece(probs_after[data.test_mask], data.y[data.test_mask])\n",
    "print(f\"ECE after calibration: {ece_after:.4f}\")\n",
    "\n",
    "print(f\"\\nECE improvement: {ece_before - ece_after:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Day 6-7: Conformal Prediction\n",
    "\n",
    "**Tasks:**\n",
    "- [ ] Read Conformal Prediction tutorial\n",
    "- [ ] Run conformal prediction code\n",
    "- [ ] Check coverage guarantees\n",
    "- [ ] Understand distribution-free uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformalPredictor:\n",
    "    \"\"\"\n",
    "    Conformal Prediction: Distribution-free uncertainty\n",
    "    \n",
    "    핵심 아이디어:\n",
    "    - Calibration set에서 nonconformity score 계산\n",
    "    - Test time에 prediction set 생성 (guaranteed coverage)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.1):\n",
    "        \"\"\"\n",
    "        alpha: 유의수준 (1-alpha = coverage level)\n",
    "        alpha=0.1이면 90% coverage 보장\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.quantile = None\n",
    "    \n",
    "    def calibrate(self, model, data, cal_mask):\n",
    "        \"\"\"\n",
    "        Calibration set에서 nonconformity scores 계산\n",
    "        \n",
    "        Nonconformity score: 1 - P(y_true)\n",
    "        즉, 정답 클래스의 확률이 낮을수록 높은 score\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(data.x, data.edge_index)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            cal_probs = probs[cal_mask]\n",
    "            cal_labels = data.y[cal_mask]\n",
    "            \n",
    "            # Nonconformity scores\n",
    "            scores = 1 - cal_probs[torch.arange(len(cal_labels)), cal_labels]\n",
    "            \n",
    "            # (1-alpha) quantile 계산\n",
    "            n = len(scores)\n",
    "            q_level = np.ceil((n + 1) * (1 - self.alpha)) / n\n",
    "            self.quantile = torch.quantile(scores, q_level)\n",
    "            \n",
    "        print(f\"Conformal quantile (alpha={self.alpha}): {self.quantile:.4f}\")\n",
    "        return self.quantile\n",
    "    \n",
    "    def predict(self, model, data, test_mask):\n",
    "        \"\"\"\n",
    "        Prediction sets 생성\n",
    "        \n",
    "        Returns:\n",
    "            prediction_sets: List of sets, 각 노드마다 가능한 클래스들\n",
    "            set_sizes: 각 prediction set의 크기\n",
    "        \"\"\"\n",
    "        if self.quantile is None:\n",
    "            raise ValueError(\"먼저 calibrate()를 호출하세요!\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(data.x, data.edge_index)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            test_probs = probs[test_mask]\n",
    "            \n",
    "            # Prediction set: {y : 1 - P(y) <= quantile}\n",
    "            # 즉, P(y) >= 1 - quantile인 모든 클래스\n",
    "            threshold = 1 - self.quantile\n",
    "            prediction_sets = (test_probs >= threshold).cpu().numpy()\n",
    "            set_sizes = prediction_sets.sum(axis=1)\n",
    "            \n",
    "        return prediction_sets, set_sizes\n",
    "    \n",
    "    def evaluate_coverage(self, model, data, test_mask):\n",
    "        \"\"\"\n",
    "        Coverage 측정: 정답이 prediction set에 포함된 비율\n",
    "        이론적으로 (1-alpha) 이상이어야 함\n",
    "        \"\"\"\n",
    "        prediction_sets, set_sizes = self.predict(model, data, test_mask)\n",
    "        test_labels = data.y[test_mask].cpu().numpy()\n",
    "        \n",
    "        coverage = np.mean([prediction_sets[i, test_labels[i]] \n",
    "                           for i in range(len(test_labels))])\n",
    "        avg_set_size = np.mean(set_sizes)\n",
    "        \n",
    "        print(f\"Coverage: {coverage:.4f} (target: {1-self.alpha:.4f})\")\n",
    "        print(f\"Average prediction set size: {avg_set_size:.2f}\")\n",
    "        \n",
    "        return coverage, avg_set_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Conformal Prediction: Computing Prediction Sets\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize conformal predictor (90% coverage)\n",
    "conformal = ConformalPredictor(alpha=0.1)\n",
    "\n",
    "# Calibrate using validation set\n",
    "conformal.calibrate(mc_model, data, data.val_mask)\n",
    "\n",
    "# Evaluate on test set\n",
    "coverage, avg_set_size = conformal.evaluate_coverage(mc_model, data, data.test_mask)\n",
    "\n",
    "# Try different alpha values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing different alpha values (coverage levels)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for alpha in [0.05, 0.1, 0.2]:\n",
    "    print(f\"\\nalpha = {alpha} (target coverage: {1-alpha:.2f})\")\n",
    "    cp = ConformalPredictor(alpha=alpha)\n",
    "    cp.calibrate(mc_model, data, data.val_mask)\n",
    "    cov, size = cp.evaluate_coverage(mc_model, data, data.test_mask)\n",
    "    print(f\"  Actual coverage: {cov:.4f}\")\n",
    "    print(f\"  Avg set size: {size:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Final: Compare All Methods\n",
    "\n",
    "**Tasks:**\n",
    "- [ ] Run the complete pipeline\n",
    "- [ ] Compare accuracy, ECE, NLL, Brier Score\n",
    "- [ ] Generate all visualizations\n",
    "- [ ] Write summary of when to use each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nll(probs, labels):\n",
    "    \"\"\"\n",
    "    Negative Log-Likelihood\n",
    "    \n",
    "    확률 예측의 품질 측정\n",
    "    NLL이 낮을수록 좋은 확률 예측\n",
    "    \"\"\"\n",
    "    labels = labels.cpu()\n",
    "    probs = probs.cpu()\n",
    "    nll = -torch.log(probs[torch.arange(len(labels)), labels] + 1e-10).mean()\n",
    "    return nll.item()\n",
    "\n",
    "\n",
    "def compute_brier_score(probs, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Brier Score: 확률 예측의 정확도\n",
    "    \n",
    "    낮을수록 좋음 (0이 perfect)\n",
    "    \"\"\"\n",
    "    labels_onehot = F.one_hot(labels, num_classes=num_classes).float()\n",
    "    brier = ((probs - labels_onehot) ** 2).sum(dim=1).mean()\n",
    "    return brier.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON: All UQ Methods on Test Set\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# MC Dropout predictions\n",
    "mc_probs, mc_epistemic, mc_entropy = mc_dropout_prediction(mc_model, data, n_samples=50)\n",
    "\n",
    "# Ensemble predictions (using n_models=5)\n",
    "ensemble_5 = GCN_Ensemble(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=16,\n",
    "    out_channels=dataset.num_classes,\n",
    "    n_models=5\n",
    ")\n",
    "print(\"\\nTraining Ensemble (5 models) for final comparison...\")\n",
    "ensemble_5.train_ensemble(data, epochs=200)\n",
    "ens_probs, ens_epistemic, ens_entropy = ensemble_5.predict(data)\n",
    "\n",
    "# Comparison\n",
    "results = {\n",
    "    'MC Dropout': {\n",
    "        'probs': mc_probs,\n",
    "        'epistemic': mc_epistemic,\n",
    "        'entropy': mc_entropy\n",
    "    },\n",
    "    'Deep Ensemble': {\n",
    "        'probs': ens_probs,\n",
    "        'epistemic': ens_epistemic,\n",
    "        'entropy': ens_entropy\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "for method_name, result in results.items():\n",
    "    probs = result['probs'][data.test_mask]\n",
    "    labels = data.y[data.test_mask]\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = (probs.argmax(dim=1) == labels).float().mean()\n",
    "    \n",
    "    # UQ Metrics\n",
    "    ece = compute_ece(probs, labels)\n",
    "    nll = compute_nll(probs, labels)\n",
    "    brier = compute_brier_score(probs, labels, dataset.num_classes)\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Accuracy:       {acc:.4f}\")\n",
    "    print(f\"  ECE:            {ece:.4f}\")\n",
    "    print(f\"  NLL:            {nll:.4f}\")\n",
    "    print(f\"  Brier Score:    {brier:.4f}\")\n",
    "    print(f\"  Avg Epistemic:  {result['epistemic'][data.test_mask].mean():.4f}\")\n",
    "    print(f\"  Avg Entropy:    {result['entropy'][data.test_mask].mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n✅ Week 2 Complete! You've implemented all 4 UQ methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_diagram(probs, labels, n_bins=10, title=\"Reliability Diagram\"):\n",
    "    \"\"\"\n",
    "    Calibration plot: Confidence vs Accuracy\n",
    "    대각선에 가까울수록 well-calibrated\n",
    "    \"\"\"\n",
    "    confidences = probs.max(dim=1)[0].cpu().numpy()\n",
    "    predictions = probs.argmax(dim=1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_confidences = []\n",
    "    bin_accuracies = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])\n",
    "        if in_bin.sum() > 0:\n",
    "            bin_confidences.append(confidences[in_bin].mean())\n",
    "            bin_accuracies.append((predictions[in_bin] == labels[in_bin]).mean())\n",
    "            bin_counts.append(in_bin.sum())\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    plt.bar(bin_confidences, bin_accuracies, width=1/n_bins, alpha=0.7, \n",
    "            edgecolor='black', label='Model')\n",
    "    plt.xlabel('Confidence', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_uncertainty_vs_error(uncertainty, is_correct, title=\"Uncertainty vs Error\"):\n",
    "    \"\"\"\n",
    "    불확실성이 높은 샘플이 틀릴 확률이 높은가?\n",
    "    \"\"\"\n",
    "    uncertainty = uncertainty.cpu().numpy()\n",
    "    is_correct = is_correct.cpu().numpy()\n",
    "    \n",
    "    # Bin by uncertainty\n",
    "    n_bins = 10\n",
    "    bins = np.percentile(uncertainty, np.linspace(0, 100, n_bins + 1))\n",
    "    bin_error_rates = []\n",
    "    bin_centers = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        in_bin = (uncertainty >= bins[i]) & (uncertainty < bins[i+1])\n",
    "        if in_bin.sum() > 0:\n",
    "            error_rate = 1 - is_correct[in_bin].mean()\n",
    "            bin_error_rates.append(error_rate)\n",
    "            bin_centers.append((bins[i] + bins[i+1]) / 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(bin_centers, bin_error_rates, 'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Uncertainty (binned)', fontsize=14)\n",
    "    plt.ylabel('Error Rate', fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagram for MC Dropout\n",
    "plot_reliability_diagram(mc_probs[data.test_mask], data.y[data.test_mask], \n",
    "                         title=\"MC Dropout - Reliability Diagram\")\n",
    "\n",
    "# Plot uncertainty vs error\n",
    "is_correct = (mc_probs.argmax(dim=1) == data.y)[data.test_mask]\n",
    "plot_uncertainty_vs_error(mc_epistemic[data.test_mask], is_correct,\n",
    "                         title=\"MC Dropout - Uncertainty vs Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Week 2 Summary\n",
    "\n",
    "## When to use each UQ method?\n",
    "\n",
    "### 1. MC Dropout\n",
    "**Pros:**\n",
    "- Easy to implement (just turn on dropout at test time)\n",
    "- Single model training\n",
    "- Fast inference (can adjust n_samples)\n",
    "\n",
    "**Cons:**\n",
    "- Theoretical justification relies on approximation\n",
    "- Uncertainty quality depends on dropout rate\n",
    "\n",
    "**Use when:** You need quick uncertainty estimates with minimal changes to existing models\n",
    "\n",
    "### 2. Deep Ensembles\n",
    "**Pros:**\n",
    "- Strong empirical performance\n",
    "- Diversity from different initializations\n",
    "- Often best uncertainty estimates\n",
    "\n",
    "**Cons:**\n",
    "- Expensive (train multiple models)\n",
    "- More memory at inference\n",
    "\n",
    "**Use when:** You need high-quality uncertainty and have computational budget\n",
    "\n",
    "### 3. Temperature Scaling\n",
    "**Pros:**\n",
    "- Simple post-processing\n",
    "- Improves calibration significantly\n",
    "- Single parameter to tune\n",
    "\n",
    "**Cons:**\n",
    "- Doesn't capture epistemic uncertainty\n",
    "- Only improves calibration, not accuracy\n",
    "\n",
    "**Use when:** You need well-calibrated probabilities for decision-making\n",
    "\n",
    "### 4. Conformal Prediction\n",
    "**Pros:**\n",
    "- Distribution-free coverage guarantees\n",
    "- Works with any base model\n",
    "- Mathematically rigorous\n",
    "\n",
    "**Cons:**\n",
    "- Produces sets, not single predictions\n",
    "- Requires holdout calibration set\n",
    "\n",
    "**Use when:** You need guaranteed coverage (e.g., safety-critical applications)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts Learned\n",
    "\n",
    "**Epistemic vs Aleatoric Uncertainty:**\n",
    "- **Epistemic**: Model uncertainty (can be reduced with more data)\n",
    "- **Aleatoric**: Data uncertainty (irreducible)\n",
    "\n",
    "**Calibration:**\n",
    "- A model is well-calibrated if predicted probabilities match true frequencies\n",
    "- E.g., among all predictions with 80% confidence, 80% should be correct\n",
    "\n",
    "**Metrics:**\n",
    "- **ECE**: Measures calibration quality\n",
    "- **NLL**: Measures probability prediction quality\n",
    "- **Brier Score**: Measures overall prediction quality\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Week 2 Complete!** Ready for Week 3: GNN + UQ integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
