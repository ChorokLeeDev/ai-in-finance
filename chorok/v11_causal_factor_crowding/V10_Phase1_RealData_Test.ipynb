{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V10 Phase 1: Causal Factor Crowding - Real Data Test\n",
    "\n",
    "**Goal**: Test if causal/lead-lag relationships exist between factor crowding levels using real Fama-French data\n",
    "\n",
    "**Previous**: Validated methodology on synthetic data (MOM → SMB at lag 3 detected ✅)\n",
    "\n",
    "**Data**: Fama-French 5 Factors + Momentum (Daily, 1990-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install pandas-datareader statsmodels -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Fama-French Factor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "\n",
    "# Download Fama-French 5 Factors (Daily)\n",
    "print(\"Downloading Fama-French 5 Factors...\")\n",
    "ff5 = web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', 'famafrench', start='1990-01-01')[0]\n",
    "\n",
    "# Download Momentum Factor (Daily)\n",
    "print(\"Downloading Momentum Factor...\")\n",
    "mom = web.DataReader('F-F_Momentum_Factor_daily', 'famafrench', start='1990-01-01')[0]\n",
    "\n",
    "# Combine\n",
    "factors = ff5.join(mom, how='inner')\n",
    "factors.columns = ['MKT', 'SMB', 'HML', 'RMW', 'CMA', 'RF', 'MOM']\n",
    "\n",
    "# Remove risk-free rate\n",
    "factors['MKT'] = factors['MKT'] - factors['RF']\n",
    "factors = factors.drop('RF', axis=1)\n",
    "\n",
    "print(f\"\\nLoaded {len(factors)} days of factor data\")\n",
    "print(f\"Factors: {list(factors.columns)}\")\n",
    "print(f\"Date range: {factors.index[0]} to {factors.index[-1]}\")\n",
    "factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of factor returns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(factors.columns):\n",
    "    (factors[col] / 100).cumsum().plot(ax=axes[i], title=f'{col} Cumulative Return')\n",
    "    axes[i].set_ylabel('Cumulative Return')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Crowding Proxy\n",
    "\n",
    "Simple proxy: Rolling volatility (z-scored)\n",
    "- Higher volatility = potential crowding unwinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crowding_proxy(factor_returns, window=60):\n",
    "    \"\"\"\n",
    "    Simple crowding proxy: z-scored rolling volatility\n",
    "    Higher volatility = potential crowding unwinding risk\n",
    "    \"\"\"\n",
    "    crowding = pd.DataFrame(index=factor_returns.index)\n",
    "    \n",
    "    for col in factor_returns.columns:\n",
    "        vol = factor_returns[col].rolling(window).std()\n",
    "        # Z-score (rolling 252-day mean/std)\n",
    "        mean = vol.rolling(252).mean()\n",
    "        std = vol.rolling(252).std()\n",
    "        crowding[col] = (vol - mean) / std\n",
    "    \n",
    "    return crowding.dropna()\n",
    "\n",
    "# Compute crowding\n",
    "crowding = compute_crowding_proxy(factors, window=60)\n",
    "print(f\"Crowding data: {crowding.shape[0]} days, {crowding.shape[1]} factors\")\n",
    "print(f\"Date range: {crowding.index[0]} to {crowding.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize crowding proxy\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(crowding.columns):\n",
    "    crowding[col].plot(ax=axes[i], title=f'{col} Crowding Proxy', alpha=0.7)\n",
    "    axes[i].axhline(y=2, color='r', linestyle='--', alpha=0.5, label='High crowding')\n",
    "    axes[i].axhline(y=-2, color='g', linestyle='--', alpha=0.5, label='Low crowding')\n",
    "    axes[i].set_ylabel('Z-score')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Factor Crowding Proxy (Z-scored Rolling Volatility)', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test 1: Cross-Correlation Analysis\n",
    "\n",
    "Find lead-lag relationships between factor crowding levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_correlation(crowding, max_lag=30):\n",
    "    \"\"\"\n",
    "    Find lead-lag relationships via cross-correlation\n",
    "    Peak at positive lag = first variable leads second\n",
    "    \"\"\"\n",
    "    factors_list = crowding.columns.tolist()\n",
    "    results = []\n",
    "    \n",
    "    for i, f1 in enumerate(factors_list):\n",
    "        for j, f2 in enumerate(factors_list):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            \n",
    "            # Cross-correlation at different lags\n",
    "            x = crowding[f1].values\n",
    "            y = crowding[f2].values\n",
    "            \n",
    "            correlations = []\n",
    "            lags = range(-max_lag, max_lag + 1)\n",
    "            \n",
    "            for lag in lags:\n",
    "                if lag < 0:\n",
    "                    c = np.corrcoef(x[-lag:], y[:lag])[0, 1]\n",
    "                elif lag > 0:\n",
    "                    c = np.corrcoef(x[:-lag], y[lag:])[0, 1]\n",
    "                else:\n",
    "                    c = np.corrcoef(x, y)[0, 1]\n",
    "                correlations.append(c if not np.isnan(c) else 0)\n",
    "            \n",
    "            # Find peak\n",
    "            peak_idx = np.argmax(np.abs(correlations))\n",
    "            peak_lag = list(lags)[peak_idx]\n",
    "            peak_corr = correlations[peak_idx]\n",
    "            \n",
    "            results.append({\n",
    "                'factor1': f1,\n",
    "                'factor2': f2,\n",
    "                'peak_lag': peak_lag,\n",
    "                'peak_corr': peak_corr,\n",
    "                'correlations': correlations,\n",
    "                'lags': list(lags)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "xcorr_results = analyze_cross_correlation(crowding, max_lag=30)\n",
    "print(\"Cross-Correlation Results:\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show significant lead-lag relationships\n",
    "significant = xcorr_results[\n",
    "    (xcorr_results['peak_corr'].abs() > 0.15) & \n",
    "    (xcorr_results['peak_lag'].abs() > 2)\n",
    "].copy()\n",
    "\n",
    "if len(significant) > 0:\n",
    "    print(\"\\nSignificant Lead-Lag Relationships:\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in significant.iterrows():\n",
    "        if row['peak_lag'] > 0:\n",
    "            print(f\"  {row['factor1']} → {row['factor2']}: lag={row['peak_lag']} days, corr={row['peak_corr']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {row['factor2']} → {row['factor1']}: lag={-row['peak_lag']} days, corr={row['peak_corr']:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo significant lead-lag relationships found (threshold: |corr|>0.15, |lag|>2)\")\n",
    "\n",
    "# Show all pairs\n",
    "print(\"\\nAll Pair Results:\")\n",
    "print(xcorr_results[['factor1', 'factor2', 'peak_lag', 'peak_corr']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross-correlation functions for key pairs\n",
    "key_pairs = [('MOM', 'HML'), ('MOM', 'SMB'), ('SMB', 'CMA'), ('HML', 'CMA')]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (f1, f2) in enumerate(key_pairs):\n",
    "    row = xcorr_results[(xcorr_results['factor1'] == f1) & (xcorr_results['factor2'] == f2)]\n",
    "    if len(row) == 0:\n",
    "        row = xcorr_results[(xcorr_results['factor1'] == f2) & (xcorr_results['factor2'] == f1)]\n",
    "    \n",
    "    if len(row) > 0:\n",
    "        row = row.iloc[0]\n",
    "        axes[idx].bar(row['lags'], row['correlations'], alpha=0.7)\n",
    "        axes[idx].axvline(x=row['peak_lag'], color='r', linestyle='--', \n",
    "                         label=f'Peak: lag={row[\"peak_lag\"]}, corr={row[\"peak_corr\"]:.3f}')\n",
    "        axes[idx].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        axes[idx].set_title(f'{f1} vs {f2}')\n",
    "        axes[idx].set_xlabel('Lag (days)')\n",
    "        axes[idx].set_ylabel('Correlation')\n",
    "        axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Cross-Correlation Functions', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test 2: Granger Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def test_granger_causality(crowding, maxlag=20):\n",
    "    \"\"\"\n",
    "    Test if one factor's crowding Granger-causes another's\n",
    "    \"\"\"\n",
    "    factors_list = crowding.columns.tolist()\n",
    "    n = len(factors_list)\n",
    "    \n",
    "    p_values = pd.DataFrame(np.nan, index=factors_list, columns=factors_list)\n",
    "    best_lags = pd.DataFrame(np.nan, index=factors_list, columns=factors_list)\n",
    "    \n",
    "    print(\"Granger Causality Test Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"(H0: X does NOT Granger-cause Y)\")\n",
    "    print()\n",
    "    \n",
    "    for cause in factors_list:\n",
    "        for effect in factors_list:\n",
    "            if cause == effect:\n",
    "                continue\n",
    "            \n",
    "            # Granger test requires [effect, cause] order\n",
    "            data = crowding[[effect, cause]].dropna()\n",
    "            \n",
    "            if len(data) < maxlag + 10:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                result = grangercausalitytests(data, maxlag=maxlag, verbose=False)\n",
    "                \n",
    "                # Find minimum p-value across all lags\n",
    "                min_p = 1.0\n",
    "                best_lag = 0\n",
    "                for lag in range(1, maxlag + 1):\n",
    "                    p = result[lag][0]['ssr_ftest'][1]\n",
    "                    if p < min_p:\n",
    "                        min_p = p\n",
    "                        best_lag = lag\n",
    "                \n",
    "                p_values.loc[cause, effect] = min_p\n",
    "                best_lags.loc[cause, effect] = best_lag\n",
    "                \n",
    "                if min_p < 0.05:\n",
    "                    print(f\"  * {cause} → {effect}: p={min_p:.4f} at lag={best_lag} days\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return p_values, best_lags\n",
    "\n",
    "granger_pvalues, granger_lags = test_granger_causality(crowding, maxlag=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Granger causality matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Convert p-values to significance (-log10)\n",
    "significance = -np.log10(granger_pvalues.astype(float) + 1e-10)\n",
    "significance = significance.clip(upper=5)  # Cap at 5 for visualization\n",
    "\n",
    "im = ax.imshow(significance.values, cmap='Reds', aspect='auto')\n",
    "ax.set_xticks(range(len(granger_pvalues.columns)))\n",
    "ax.set_yticks(range(len(granger_pvalues.index)))\n",
    "ax.set_xticklabels(granger_pvalues.columns)\n",
    "ax.set_yticklabels(granger_pvalues.index)\n",
    "ax.set_xlabel('Effect (Y)')\n",
    "ax.set_ylabel('Cause (X)')\n",
    "ax.set_title('Granger Causality: -log10(p-value)\\n(Higher = More Significant)')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(granger_pvalues.index)):\n",
    "    for j in range(len(granger_pvalues.columns)):\n",
    "        p = granger_pvalues.iloc[i, j]\n",
    "        if not np.isnan(p):\n",
    "            text = f'{p:.3f}'\n",
    "            color = 'white' if p < 0.05 else 'black'\n",
    "            ax.text(j, i, text, ha='center', va='center', color=color, fontsize=9)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='-log10(p-value)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSignificant causal relationships (p < 0.05): {(granger_pvalues < 0.05).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test 3: VAR Impulse Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Fit VAR model\n",
    "print(\"Fitting VAR model...\")\n",
    "model = VAR(crowding)\n",
    "\n",
    "# Select optimal lag\n",
    "lag_order = model.select_order(maxlags=15)\n",
    "print(f\"\\nOptimal lag orders:\")\n",
    "print(f\"  AIC: {lag_order.aic}\")\n",
    "print(f\"  BIC: {lag_order.bic}\")\n",
    "\n",
    "# Fit with AIC-selected lag\n",
    "results = model.fit(lag_order.aic)\n",
    "print(f\"\\nFitted VAR({lag_order.aic})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute impulse response functions\n",
    "irf = results.irf(periods=30)\n",
    "\n",
    "# Plot impulse responses\n",
    "fig = irf.plot(orth=True, figsize=(16, 12))\n",
    "plt.suptitle('Orthogonalized Impulse Response Functions', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on key impulse responses (MOM shocks)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "shock_factor = 'MOM'\n",
    "shock_idx = list(crowding.columns).index(shock_factor)\n",
    "\n",
    "for i, response_factor in enumerate(crowding.columns):\n",
    "    response_idx = list(crowding.columns).index(response_factor)\n",
    "    \n",
    "    # Get impulse response\n",
    "    ir = irf.irfs[:, response_idx, shock_idx]\n",
    "    ir_lower = irf.irfs_conf_int()[:, response_idx, shock_idx, 0]\n",
    "    ir_upper = irf.irfs_conf_int()[:, response_idx, shock_idx, 1]\n",
    "    \n",
    "    periods = range(len(ir))\n",
    "    \n",
    "    axes[i].plot(periods, ir, 'b-', linewidth=2, label='IRF')\n",
    "    axes[i].fill_between(periods, ir_lower, ir_upper, alpha=0.3, label='95% CI')\n",
    "    axes[i].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[i].set_title(f'Shock: {shock_factor} → Response: {response_factor}')\n",
    "    axes[i].set_xlabel('Days')\n",
    "    axes[i].set_ylabel('Response')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Impulse Response to {shock_factor} Shock', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test 4: Event Study - Crowding Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_crowding_events(crowding, threshold=2.0, window=20):\n",
    "    \"\"\"\n",
    "    When one factor has a crowding spike (>2 std), what happens to others?\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for shock_factor in crowding.columns:\n",
    "        # Find spike days\n",
    "        spike_days = crowding.index[crowding[shock_factor] > threshold]\n",
    "        \n",
    "        if len(spike_days) < 5:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{shock_factor} spikes: {len(spike_days)} events\")\n",
    "        \n",
    "        for response_factor in crowding.columns:\n",
    "            if shock_factor == response_factor:\n",
    "                continue\n",
    "            \n",
    "            # Collect response after spikes\n",
    "            responses = []\n",
    "            for spike_day in spike_days:\n",
    "                try:\n",
    "                    idx = crowding.index.get_loc(spike_day)\n",
    "                    if idx + window >= len(crowding):\n",
    "                        continue\n",
    "                    \n",
    "                    # Response = max crowding in next N days\n",
    "                    future_max = crowding[response_factor].iloc[idx+1:idx+window+1].max()\n",
    "                    baseline = crowding[response_factor].iloc[idx]\n",
    "                    responses.append(future_max - baseline)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if len(responses) < 5:\n",
    "                continue\n",
    "            \n",
    "            mean_response = np.mean(responses)\n",
    "            t_stat, p_value = stats.ttest_1samp(responses, 0)\n",
    "            \n",
    "            results.append({\n",
    "                'shock': shock_factor,\n",
    "                'response': response_factor,\n",
    "                'n_events': len(responses),\n",
    "                'mean_increase': mean_response,\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                direction = \"increases\" if mean_response > 0 else \"decreases\"\n",
    "                print(f\"  → {response_factor} {direction}: mean={mean_response:.3f}, p={p_value:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "event_results = analyze_crowding_events(crowding, threshold=2.0, window=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"V10 VALIDATION SUMMARY: REAL FAMA-FRENCH DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cross-correlation summary\n",
    "sig_xcorr = xcorr_results[\n",
    "    (xcorr_results['peak_corr'].abs() > 0.15) & \n",
    "    (xcorr_results['peak_lag'].abs() > 2)\n",
    "]\n",
    "print(f\"\\n1. CROSS-CORRELATION: {len(sig_xcorr)} significant lead-lag relationships\")\n",
    "for _, row in sig_xcorr.iterrows():\n",
    "    if row['peak_lag'] > 0:\n",
    "        print(f\"   {row['factor1']} → {row['factor2']}: lag={row['peak_lag']}d, corr={row['peak_corr']:.3f}\")\n",
    "    else:\n",
    "        print(f\"   {row['factor2']} → {row['factor1']}: lag={-row['peak_lag']}d, corr={row['peak_corr']:.3f}\")\n",
    "\n",
    "# Granger causality summary\n",
    "n_granger = (granger_pvalues < 0.05).sum().sum()\n",
    "print(f\"\\n2. GRANGER CAUSALITY: {n_granger} significant causal relationships (p<0.05)\")\n",
    "for cause in granger_pvalues.index:\n",
    "    for effect in granger_pvalues.columns:\n",
    "        p = granger_pvalues.loc[cause, effect]\n",
    "        if p < 0.05:\n",
    "            lag = granger_lags.loc[cause, effect]\n",
    "            print(f\"   {cause} → {effect}: p={p:.4f}, lag={int(lag)}d\")\n",
    "\n",
    "# Event study summary\n",
    "if len(event_results) > 0:\n",
    "    sig_events = event_results[event_results['p_value'] < 0.05]\n",
    "    print(f\"\\n3. EVENT STUDY: {len(sig_events)} significant spillover effects\")\n",
    "    for _, row in sig_events.iterrows():\n",
    "        direction = \"↑\" if row['mean_increase'] > 0 else \"↓\"\n",
    "        print(f\"   {row['shock']} spike → {row['response']} {direction}: p={row['p_value']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_relationships = len(sig_xcorr) + n_granger\n",
    "if total_relationships > 3:\n",
    "    print(\"\\n✅ VALIDATION PASSED: Multiple causal/lead-lag relationships found!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Apply formal causal discovery (DYNOTEARS, PCMCI)\")\n",
    "    print(\"2. Build causal-aware prediction model\")\n",
    "    print(\"3. Compare with correlation-based baselines\")\n",
    "elif total_relationships > 0:\n",
    "    print(\"\\n⚠️ PARTIAL VALIDATION: Some relationships found, but limited\")\n",
    "    print(\"Consider improving crowding proxy\")\n",
    "else:\n",
    "    print(\"\\n❌ VALIDATION FAILED: No significant relationships found\")\n",
    "    print(\"Need to revise crowding proxy or methodology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for further analysis\n",
    "results_summary = {\n",
    "    'xcorr': xcorr_results,\n",
    "    'granger_pvalues': granger_pvalues,\n",
    "    'granger_lags': granger_lags,\n",
    "    'event_results': event_results,\n",
    "    'crowding': crowding,\n",
    "    'factors': factors\n",
    "}\n",
    "\n",
    "print(\"\\nResults saved to results_summary dict\")\n",
    "print(\"\\nKey findings for paper:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Most significant Granger relationships\n",
    "print(\"\\nTop Granger-causal relationships:\")\n",
    "granger_flat = []\n",
    "for cause in granger_pvalues.index:\n",
    "    for effect in granger_pvalues.columns:\n",
    "        if cause != effect:\n",
    "            granger_flat.append({\n",
    "                'cause': cause,\n",
    "                'effect': effect,\n",
    "                'p_value': granger_pvalues.loc[cause, effect],\n",
    "                'lag': granger_lags.loc[cause, effect]\n",
    "            })\n",
    "\n",
    "granger_df = pd.DataFrame(granger_flat).sort_values('p_value')\n",
    "print(granger_df.head(10).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
