{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy vs Consistency: Finding the Gap (v2 - Mistral-7B)\n",
        "\n",
        "**Research Question:** Does token entropy detect uncertainty that sample consistency misses?\n",
        "\n",
        "**Hypothesis:** Entropy can detect \"confident hallucination\" where consistency fails.\n",
        "\n",
        "**Model:** Mistral-7B-Instruct (4-bit quantized for T4 GPU)\n",
        "\n",
        "**Setup:** Runtime → Change runtime type → T4 GPU"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies - MUST restart runtime after this cell!\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q transformers accelerate datasets scipy\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"IMPORTANT: Go to Runtime -> Restart runtime\")\n",
        "print(\"Then skip this cell and run from the next one\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell AFTER restarting runtime\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Mistral-7B with 4-bit quantization (fits in T4 16GB)\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME} (4-bit quantized)...\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Metrics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_response_entropy(prompt, response):\n",
        "    \"\"\"Compute mean token entropy during response generation.\"\"\"\n",
        "    full_text = prompt + response\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[0]\n",
        "\n",
        "    entropies = []\n",
        "    for i in range(prompt_len - 1, len(logits) - 1):\n",
        "        probs = F.softmax(logits[i].float(), dim=-1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
        "        entropies.append(entropy)\n",
        "\n",
        "    return {\n",
        "        \"mean\": np.mean(entropies) if entropies else 0,\n",
        "        \"max\": np.max(entropies) if entropies else 0,\n",
        "        \"std\": np.std(entropies) if entropies else 0,\n",
        "    }\n",
        "\n",
        "\n",
        "def format_prompt(question):\n",
        "    \"\"\"Format for Mistral instruction format.\"\"\"\n",
        "    return f\"<s>[INST] {question} Answer briefly in one sentence. [/INST]\"\n",
        "\n",
        "\n",
        "def generate_response(prompt, temperature=0.7, max_tokens=50):\n",
        "    \"\"\"Generate a single response.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def compute_consistency(question, n_samples=5):\n",
        "    \"\"\"Compute consistency by generating N samples and measuring agreement.\"\"\"\n",
        "    prompt = format_prompt(question)\n",
        "    responses = [generate_response(prompt) for _ in range(n_samples)]\n",
        "\n",
        "    # Normalize responses for comparison (lowercase, strip)\n",
        "    normalized = [r.lower().strip()[:100] for r in responses]\n",
        "    counts = Counter(normalized)\n",
        "    most_common_count = counts.most_common(1)[0][1]\n",
        "    consistency_score = most_common_count / n_samples\n",
        "\n",
        "    return {\n",
        "        \"score\": consistency_score,\n",
        "        \"responses\": responses,\n",
        "        \"unique\": len(counts),\n",
        "        \"most_common\": responses[0],  # Return original (not normalized)\n",
        "        \"prompt\": prompt,\n",
        "    }"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Questions (Expanded)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_QUESTIONS = [\n",
        "    # === FACTUAL (should be confident and correct) ===\n",
        "    {\"q\": \"What is the capital of France?\", \"category\": \"factual\"},\n",
        "    {\"q\": \"What year did World War 2 end?\", \"category\": \"factual\"},\n",
        "    {\"q\": \"Who wrote Romeo and Juliet?\", \"category\": \"factual\"},\n",
        "    {\"q\": \"What is the chemical symbol for water?\", \"category\": \"factual\"},\n",
        "    {\"q\": \"What planet is closest to the Sun?\", \"category\": \"factual\"},\n",
        "    {\"q\": \"What is the largest ocean on Earth?\", \"category\": \"factual\"},\n",
        "    {\"q\": \"Who painted the Mona Lisa?\", \"category\": \"factual\"},\n",
        "    {\"q\": \"What is the speed of light in km/s?\", \"category\": \"factual\"},\n",
        "\n",
        "    # === HALLUCINATION-PRONE (model might confidently make things up) ===\n",
        "    {\"q\": \"What did Albert Einstein say about Bitcoin?\", \"category\": \"hallucination\"},\n",
        "    {\"q\": \"What is the phone number of the Eiffel Tower?\", \"category\": \"hallucination\"},\n",
        "    {\"q\": \"What was Barack Obama's favorite pizza topping?\", \"category\": \"hallucination\"},\n",
        "    {\"q\": \"What did Shakespeare tweet about?\", \"category\": \"hallucination\"},\n",
        "    {\"q\": \"What is the WiFi password at the White House?\", \"category\": \"hallucination\"},\n",
        "    {\"q\": \"What was Aristotle's email address?\", \"category\": \"hallucination\"},\n",
        "    {\"q\": \"What is my mother's name?\", \"category\": \"hallucination\"},\n",
        "    {\"q\": \"What did I have for breakfast today?\", \"category\": \"hallucination\"},\n",
        "\n",
        "    # === UNCERTAIN (model may not know) ===\n",
        "    {\"q\": \"What is the population of Liechtenstein?\", \"category\": \"uncertain\"},\n",
        "    {\"q\": \"Who was the 23rd President of the United States?\", \"category\": \"uncertain\"},\n",
        "    {\"q\": \"What year was the University of Bologna founded?\", \"category\": \"uncertain\"},\n",
        "    {\"q\": \"What is the GDP of Bhutan?\", \"category\": \"uncertain\"},\n",
        "    {\"q\": \"Who won the Nobel Prize in Chemistry in 1987?\", \"category\": \"uncertain\"},\n",
        "\n",
        "    # === SUBJECTIVE (no right answer) ===\n",
        "    {\"q\": \"What is the best programming language?\", \"category\": \"subjective\"},\n",
        "    {\"q\": \"What is the meaning of life?\", \"category\": \"subjective\"},\n",
        "    {\"q\": \"Should I eat pizza or salad?\", \"category\": \"subjective\"},\n",
        "    {\"q\": \"What is the best movie ever made?\", \"category\": \"subjective\"},\n",
        "    {\"q\": \"Is coffee better than tea?\", \"category\": \"subjective\"},\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(TEST_QUESTIONS)} test questions\")\n",
        "for cat in ['factual', 'hallucination', 'uncertain', 'subjective']:\n",
        "    count = len([q for q in TEST_QUESTIONS if q['category'] == cat])\n",
        "    print(f\"  {cat}: {count}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "N_SAMPLES = 5  # For consistency measurement\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Running experiment (this takes ~10-15 min with Mistral-7B)...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for item in tqdm(TEST_QUESTIONS):\n",
        "    question = item[\"q\"]\n",
        "\n",
        "    # Measure consistency (N samples)\n",
        "    consistency = compute_consistency(question, n_samples=N_SAMPLES)\n",
        "\n",
        "    # Measure entropy (on most common response)\n",
        "    response = consistency[\"most_common\"]\n",
        "    entropy = compute_response_entropy(consistency[\"prompt\"], response)\n",
        "\n",
        "    results.append({\n",
        "        \"question\": question,\n",
        "        \"category\": item[\"category\"],\n",
        "        \"response\": response[:60],\n",
        "        \"consistency\": consistency[\"score\"],\n",
        "        \"entropy_mean\": entropy[\"mean\"],\n",
        "        \"entropy_max\": entropy[\"max\"],\n",
        "        \"unique_responses\": consistency[\"unique\"],\n",
        "    })\n",
        "\n",
        "print(\"\\nDone!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"INDIVIDUAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "for _, row in df.iterrows():\n",
        "    print(f\"\\n[{row['category'].upper()}] {row['question'][:50]}\")\n",
        "    print(f\"  Response: {row['response'][:50]}...\")\n",
        "    print(f\"  Consistency: {row['consistency']:.2f} | Entropy: {row['entropy_mean']:.2f} | Unique: {row['unique_responses']}/{N_SAMPLES}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AGGREGATE BY CATEGORY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary = df.groupby(\"category\").agg({\n",
        "    \"consistency\": [\"mean\", \"std\"],\n",
        "    \"entropy_mean\": [\"mean\", \"std\"],\n",
        "    \"unique_responses\": \"mean\",\n",
        "}).round(3)\n",
        "\n",
        "print(summary.to_string())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY FINDING: ENTROPY vs CONSISTENCY CORRELATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "corr_pearson, p_pearson = pearsonr(df[\"consistency\"], df[\"entropy_mean\"])\n",
        "corr_spearman, p_spearman = spearmanr(df[\"consistency\"], df[\"entropy_mean\"])\n",
        "\n",
        "print(f\"Pearson correlation:  {corr_pearson:.3f} (p={p_pearson:.4f})\")\n",
        "print(f\"Spearman correlation: {corr_spearman:.3f} (p={p_spearman:.4f})\")\n",
        "\n",
        "if abs(corr_pearson) < 0.5:\n",
        "    print(\"\\n>>> WEAK CORRELATION: Entropy and consistency measure DIFFERENT things!\")\n",
        "elif abs(corr_pearson) < 0.7:\n",
        "    print(\"\\n>>> MODERATE CORRELATION: Some overlap but not redundant.\")\n",
        "else:\n",
        "    print(\"\\n>>> STRONG CORRELATION: Metrics may be redundant.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HALLUCINATION DETECTION COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from scipy.stats import ttest_ind, mannwhitneyu\n",
        "\n",
        "hallucination_df = df[df[\"category\"] == \"hallucination\"]\n",
        "factual_df = df[df[\"category\"] == \"factual\"]\n",
        "\n",
        "print(f\"\\nFactual questions (n={len(factual_df)}):\")\n",
        "print(f\"  Avg Consistency: {factual_df['consistency'].mean():.3f} +/- {factual_df['consistency'].std():.3f}\")\n",
        "print(f\"  Avg Entropy: {factual_df['entropy_mean'].mean():.3f} +/- {factual_df['entropy_mean'].std():.3f}\")\n",
        "\n",
        "print(f\"\\nHallucination-prone questions (n={len(hallucination_df)}):\")\n",
        "print(f\"  Avg Consistency: {hallucination_df['consistency'].mean():.3f} +/- {hallucination_df['consistency'].std():.3f}\")\n",
        "print(f\"  Avg Entropy: {hallucination_df['entropy_mean'].mean():.3f} +/- {hallucination_df['entropy_mean'].std():.3f}\")\n",
        "\n",
        "# Statistical tests\n",
        "_, p_entropy = mannwhitneyu(hallucination_df['entropy_mean'], factual_df['entropy_mean'], alternative='greater')\n",
        "_, p_consistency = mannwhitneyu(factual_df['consistency'], hallucination_df['consistency'], alternative='greater')\n",
        "\n",
        "entropy_gap = hallucination_df['entropy_mean'].mean() - factual_df['entropy_mean'].mean()\n",
        "consistency_gap = factual_df['consistency'].mean() - hallucination_df['consistency'].mean()\n",
        "\n",
        "print(f\"\\nSeparation power:\")\n",
        "print(f\"  Entropy gap (hallucination - factual): {entropy_gap:.3f} (p={p_entropy:.4f})\")\n",
        "print(f\"  Consistency gap (factual - hallucination): {consistency_gap:.3f} (p={p_consistency:.4f})\")\n",
        "\n",
        "print(f\"\\nEffect size (Cohen's d approximation):\")\n",
        "pooled_std_e = np.sqrt((factual_df['entropy_mean'].var() + hallucination_df['entropy_mean'].var()) / 2)\n",
        "pooled_std_c = np.sqrt((factual_df['consistency'].var() + hallucination_df['consistency'].var()) / 2)\n",
        "cohens_d_entropy = abs(entropy_gap) / pooled_std_e if pooled_std_e > 0 else 0\n",
        "cohens_d_consistency = abs(consistency_gap) / pooled_std_c if pooled_std_c > 0 else 0\n",
        "print(f\"  Entropy: d={cohens_d_entropy:.2f}\")\n",
        "print(f\"  Consistency: d={cohens_d_consistency:.2f}\")\n",
        "\n",
        "if entropy_gap > consistency_gap and p_entropy < 0.1:\n",
        "    print(\"\\n>>> ENTROPY BETTER at detecting hallucination-prone questions!\")\n",
        "elif consistency_gap > entropy_gap and p_consistency < 0.1:\n",
        "    print(\"\\n>>> CONSISTENCY BETTER at detecting hallucination-prone questions.\")\n",
        "else:\n",
        "    print(\"\\n>>> No clear winner (need more data or larger effect).\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIVERGENCE ANALYSIS: Where do metrics disagree?\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Normalize for comparison\n",
        "df[\"consistency_norm\"] = (df[\"consistency\"] - df[\"consistency\"].min()) / (df[\"consistency\"].max() - df[\"consistency\"].min() + 1e-10)\n",
        "df[\"entropy_norm\"] = (df[\"entropy_mean\"] - df[\"entropy_mean\"].min()) / (df[\"entropy_mean\"].max() - df[\"entropy_mean\"].min() + 1e-10)\n",
        "\n",
        "# High consistency + High entropy = Confident hallucination candidate\n",
        "df[\"confident_hallucination_score\"] = df[\"consistency_norm\"] * df[\"entropy_norm\"]\n",
        "\n",
        "print(\"\\nPotential CONFIDENT HALLUCINATIONS (high consistency + high entropy):\")\n",
        "candidates = df.nlargest(5, \"confident_hallucination_score\")\n",
        "for _, row in candidates.iterrows():\n",
        "    print(f\"\\n  [{row['category']}] {row['question'][:45]}\")\n",
        "    print(f\"    Consistency: {row['consistency']:.2f} | Entropy: {row['entropy_mean']:.2f}\")\n",
        "    print(f\"    Response: {row['response'][:50]}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary & Conclusions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "MODEL: {MODEL_NAME}\n",
        "QUESTIONS: {len(TEST_QUESTIONS)} ({len(factual_df)} factual, {len(hallucination_df)} hallucination-prone)\n",
        "\n",
        "1. CORRELATION (entropy vs consistency): {corr_pearson:.3f}\n",
        "   Interpretation: {'WEAK - different signals!' if abs(corr_pearson) < 0.5 else 'MODERATE/STRONG - some overlap'}\n",
        "\n",
        "2. HALLUCINATION DETECTION:\n",
        "   Entropy gap: {entropy_gap:.3f} (p={p_entropy:.4f}, d={cohens_d_entropy:.2f})\n",
        "   Consistency gap: {consistency_gap:.3f} (p={p_consistency:.4f}, d={cohens_d_consistency:.2f})\n",
        "   Winner: {'ENTROPY' if (entropy_gap > consistency_gap and cohens_d_entropy > cohens_d_consistency) else 'CONSISTENCY' if consistency_gap > entropy_gap else 'TIE'}\n",
        "\n",
        "3. COMPUTATIONAL COST:\n",
        "   Entropy: 1 forward pass\n",
        "   Consistency: {N_SAMPLES} forward passes ({N_SAMPLES}x more expensive)\n",
        "\n",
        "CONCLUSION:\n",
        "\"\"\")\n",
        "\n",
        "if abs(corr_pearson) < 0.5 and entropy_gap > 0 and cohens_d_entropy > 0.3:\n",
        "    print(\"STRONG SIGNAL: Entropy captures uncertainty that consistency misses.\")\n",
        "    print(\"This supports entropy-based training as a novel contribution.\")\n",
        "elif abs(corr_pearson) < 0.7 and entropy_gap > 0:\n",
        "    print(\"MODERATE SIGNAL: Some differentiation, worth exploring further.\")\n",
        "    print(\"Consider: more questions, different model, or combined approach.\")\n",
        "else:\n",
        "    print(\"WEAK SIGNAL: Metrics are too similar or entropy doesn't help.\")\n",
        "    print(\"May need to pivot research direction.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
