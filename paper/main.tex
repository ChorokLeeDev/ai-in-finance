\documentclass{article}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{xcolor}

% Page layout
\usepackage[margin=1in]{geometry}

% Comments for collaboration
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\feedback}[1]{\textcolor{blue}{[FEEDBACK: #1]}}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\title{RelUQ: Schema-Guided Uncertainty Attribution\\for Relational Databases}

\author{
  Author 1 \\
  Affiliation \\
  \texttt{email@example.com}
}

\date{\today}

\begin{document}

\maketitle

% ============================================================================
\begin{abstract}
% ~150 words
Machine learning models trained on relational databases often exhibit prediction uncertainty,
but existing attribution methods operate at the feature level, suffering from instability
due to multicollinearity and lacking actionable insights for practitioners.
%
We propose \textbf{RelUQ} (Relational Uncertainty Quantification), a framework that attributes
prediction uncertainty to foreign key (FK) groups---semantically meaningful clusters of features
derived from database schema relationships.
%
Our key insight is that FK groups provide a \emph{schema-defined hierarchy} enabling
drill-down analysis (FK $\to$ Feature $\to$ Entity) and \emph{intervention simulation}---predicting
how data quality improvements reduce uncertainty---because features within an FK group share
a common intervention point.
%
Experiments on three domains (motorsport, Q\&A, e-commerce) demonstrate that RelUQ achieves
stability $\rho \geq 0.85$, calibrated intervention predictions ($\rho = 0.80$),
and actionable insights (e.g., ``improving DRIVER data quality will reduce uncertainty by 25\%'').
\end{abstract}

% ============================================================================
\section{Introduction}
\label{sec:intro}

% Hook: Why uncertainty attribution matters
Uncertainty quantification (UQ) in machine learning has gained significant attention,
enabling practitioners to understand \emph{how confident} a model's predictions are.
However, knowing \emph{that} a prediction is uncertain is only half the story---practitioners
also need to know \emph{why} it is uncertain and \emph{what} to do about it.

% Problem: Feature-level attribution is unstable
Existing uncertainty attribution methods, such as variance-based feature importance
and InfoSHAP~\cite{infoshap2023}, operate at the individual feature level.
While intuitive, feature-level attribution suffers from two critical limitations:
\begin{enumerate}
    \item \textbf{Instability}: Multicollinearity among features causes attribution values
          to fluctuate significantly across random seeds.
    \item \textbf{Lack of actionability}: Knowing that ``feature \texttt{driverRef} contributes 4.2\%''
          does not tell a practitioner what business process to investigate.
\end{enumerate}

% Our solution: FK-level grouping
We observe that relational databases, the most common data source in enterprise ML,
provide a natural solution: \textbf{foreign key (FK) relationships}.
FK constraints define functional dependencies between tables, meaning that features
derived from the same FK relationship are semantically related and often correlated.
By grouping features according to their FK origin, we can:
\begin{itemize}
    \item Reduce the number of attribution targets (e.g., from 24 features to 5 FK groups)
    \item Increase stability by aggregating correlated features
    \item Provide actionable insights (e.g., ``DRIVER process is the main source of uncertainty'')
\end{itemize}

% Contributions
Our contributions are:
\begin{enumerate}
    \item We propose \textbf{RelUQ}, a framework for FK-level uncertainty attribution
          that leverages database schema as prior knowledge, providing a \textbf{fixed, multi-level hierarchy}
          (FK $\to$ Feature $\to$ Entity) for drill-down analysis.
    \item We show that FK grouping enables \textbf{intervention simulation}---predicting
          uncertainty reduction from data quality improvements---because FK groups share
          common intervention points, unlike data-driven groupings.
    \item We validate RelUQ on three diverse domains (motorsport, Q\&A, e-commerce),
          demonstrating stability $\rho \geq 0.85$, calibrated intervention predictions ($\rho = 0.80$),
          and actionable insights that map directly to business processes.
\end{enumerate}

\todo{Add figure showing feature-level vs FK-level comparison}

% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Uncertainty Quantification}
Deep ensembles~\cite{lakshminarayanan2017simple} provide a simple yet effective method
for estimating epistemic uncertainty via ensemble variance.
\todo{Add MC Dropout, Bayesian NN references}

\paragraph{Feature Attribution}
SHAP~\cite{lundberg2017unified} and permutation importance are standard methods
for feature-level attribution.
InfoSHAP~\cite{infoshap2023} extends this to uncertainty attribution but inherits
the instability of feature-level methods.
\todo{Add more attribution references}

\paragraph{Relational Learning}
RelBench~\cite{relbench2024} provides benchmarks for ML on relational databases.
GNN-based methods learn representations over relational structures.
Our work differs by using schema for \emph{attribution} rather than \emph{prediction}.

% ============================================================================
\section{Method}
\label{sec:method}

\subsection{Problem Setup}

Let $\mathcal{D} = \{T_1, \ldots, T_n\}$ be a relational database with tables $T_i$,
each having a primary key and potentially foreign keys referencing other tables.
Given a prediction task $(T_{\text{entity}}, y)$ where $y$ is a regression target,
we train an ensemble of models $\mathcal{M} = \{m_1, \ldots, m_K\}$.

\begin{definition}[Epistemic Uncertainty]
For input $\mathbf{x}$, the epistemic uncertainty is the ensemble variance:
\[
u(\mathbf{x}) = \text{Var}_{m \in \mathcal{M}}[m(\mathbf{x})]
\]
\end{definition}

\begin{definition}[FK Group]
An FK group $g_i$ is the set of features derived from a single foreign key relationship.
Formally, $g_i = \{f : \text{source}(f) = \text{FK}_i\}$.
\end{definition}

\subsection{RelUQ Algorithm}

\begin{algorithm}[t]
\caption{RelUQ: FK-Level Uncertainty Attribution}
\label{alg:reluq}
\begin{algorithmic}[1]
\Require Database $\mathcal{D}$, Task $(T_{\text{entity}}, y)$, Ensemble size $K$, Permutations $P$
\Ensure Attribution $\mathcal{A} = \{(g_i, \alpha_i)\}$

\State Extract features $\mathbf{X}$ from $\mathcal{D}$ via FK joins
\State Map each column to FK group: $\text{col\_to\_fk}(c) \to g$
\State Train ensemble $\mathcal{M} = \{m_1, \ldots, m_K\}$ with subsampling

\State $u_{\text{base}} \gets \text{Mean}_{\mathbf{x}}[\text{Var}_{m}[m(\mathbf{x})]]$ \Comment{Baseline uncertainty}

\For{each FK group $g_i$}
    \State $\delta_i \gets 0$
    \For{$p = 1$ to $P$}
        \State $\mathbf{X}' \gets \text{Permute}(\mathbf{X}, \text{columns in } g_i)$
        \State $u' \gets \text{Mean}_{\mathbf{x}}[\text{Var}_{m}[m(\mathbf{x}')]]$
        \State $\delta_i \gets \delta_i + (u' - u_{\text{base}})$
    \EndFor
    \State $\delta_i \gets \delta_i / P$
\EndFor

\State $\alpha_i \gets \max(0, \delta_i) / \sum_j \max(0, \delta_j) \times 100\%$ \Comment{Normalize}

\State \Return $\mathcal{A} = \{(g_i, \alpha_i)\}$
\end{algorithmic}
\end{algorithm}

The key insight is that permuting features within an FK group breaks the relationship
between that group and the target, increasing uncertainty proportionally to the group's importance.

\subsection{Schema-Defined Hierarchy}
\label{sec:hierarchy}

A key advantage of FK grouping over data-driven methods is the \textbf{fixed, multi-level hierarchy}
that enables drill-down analysis and intervention planning.

\begin{table}[h]
\centering
\caption{Hierarchy and stability comparison across methods}
\label{tab:hierarchy}
\begin{tabular}{lccccc}
\toprule
Method & Level 1 & Level 2 & Level 3 & Grouping Stable? & Attr.\ Stable? \\
\midrule
Feature-level & -- & feature & value & N/A (no grouping) & 0.956 \\
Correlation & CORR\_GROUP & feature & value & \textbf{No} & 0.933 \\
Random & RANDOM & feature & value & Yes$^*$ & -0.400 \\
\textbf{RelUQ (FK)} & FK group & feature & entity & \textbf{Yes} & \textbf{0.933} \\
\bottomrule
\multicolumn{6}{l}{\small $^*$Random grouping is fixed but attribution is unstable.}
\end{tabular}
\end{table}

\paragraph{Why data-driven methods fail at hierarchy.}
Correlation clustering groups features by statistical patterns, but these patterns are \emph{sample-dependent}.
Running the same analysis next month may yield different groups:
\begin{itemize}
    \item Month 1: CORR\_GROUP\_4 = \{dob, nationality, driverRef\}
    \item Month 2: CORR\_GROUP\_4 = \{grid, position, laps\}
\end{itemize}
This instability prevents consistent reporting (e.g., ``DRIVER risk increased 10\% vs.\ last month'').

\paragraph{FK hierarchy is schema-defined.}
FK groups are determined by the database schema, not data statistics.
The DRIVER FK \emph{always} contains \{dob, nationality, driverRef\} because these columns
are joined via the driver foreign key. This enables:
\begin{enumerate}
    \item \textbf{Consistent drill-down}: FK $\to$ Feature $\to$ Entity
    \item \textbf{Temporal stability}: Same groups across time periods
    \item \textbf{Business alignment}: FK maps to data collection processes
\end{enumerate}

\subsection{Actionability: Simulation and Optimization}
\label{sec:actionability}

Beyond interpretability, FK grouping enables \textbf{intervention simulation} and \textbf{risk optimization}
because features within an FK group share a \emph{common intervention point}.

\begin{definition}[Intervention Point]
An intervention point is a real-world process that, when modified, affects all features in a group.
For FK group $g_i$ derived from table $T$, the intervention point is the data collection process for $T$.
\end{definition}

\paragraph{Why correlation groups lack intervention points.}
If CORR\_GROUP\_4 = \{dob, grid, nationality\}, there is no single process to improve.
These features are grouped by correlation, not causation---improving one does not affect others.

\paragraph{FK groups enable simulation.}
For FK group DRIVER = \{dob, nationality, driverRef\}:
\begin{enumerate}
    \item All features come from the \texttt{driver} table
    \item Improving driver data quality affects \emph{all} DRIVER features
    \item We can simulate: ``What if DRIVER data had the quality of low-uncertainty samples?''
\end{enumerate}

\begin{algorithm}[h]
\caption{Intervention Simulation}
\label{alg:intervention}
\begin{algorithmic}[1]
\Require Data $\mathbf{X}$, Models $\mathcal{M}$, FK group $g$, Reference set $\mathbf{X}_{\text{ref}}$ (low-uncertainty samples)
\Ensure Predicted uncertainty reduction $\Delta u$

\State $u_{\text{base}} \gets \text{Uncertainty}(\mathcal{M}, \mathbf{X})$
\State $\mathbf{X}' \gets \mathbf{X}$
\For{each column $c \in g$}
    \State $\mathbf{X}'[c] \gets \text{Mean}(\mathbf{X}_{\text{ref}}[c])$ \Comment{Replace with reference values}
\EndFor
\State $u_{\text{sim}} \gets \text{Uncertainty}(\mathcal{M}, \mathbf{X}')$
\State \Return $\Delta u = u_{\text{base}} - u_{\text{sim}}$
\end{algorithmic}
\end{algorithm}

This enables practitioners to answer: ``If we invest in improving DRIVER data quality,
how much will prediction uncertainty decrease?''

\subsection{Theoretical Justification}

\begin{proposition}[Stability of FK Grouping]
Let $\mathbf{X}$ be features with correlation structure induced by FK constraints.
Grouping by FK reduces attribution variance compared to feature-level attribution:
\[
\text{Var}[\alpha_{\text{FK}}] < \text{Var}[\alpha_{\text{feature}}]
\]
\end{proposition}

\begin{proof}[Proof Sketch]
FK constraints imply functional dependencies: if $A.\text{fk} = B.\text{pk}$,
then all attributes of $B$ are deterministically related.
This induces within-group correlation, causing feature-level attributions to be unstable
(the same ``credit'' is split among correlated features differently across runs).
Aggregating to FK level removes this instability.
\todo{Formalize this argument}
\end{proof}

% ============================================================================
\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}

We evaluate on three RelBench datasets spanning diverse domains:

\begin{table}[h]
\centering
\caption{Dataset characteristics}
\label{tab:datasets}
\begin{tabular}{llllr}
\toprule
Dataset & Domain & Task & FK Groups & Samples \\
\midrule
rel-f1 & Motorsport & driver-position & 5 & 3,000 \\
rel-stack & Q\&A & post-votes & 3 & 3,000 \\
rel-amazon & E-commerce & user-ltv & 3 & 3,000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Baselines}

\begin{itemize}
    \item \textbf{Feature-level}: Attribution to individual features (24 groups for rel-f1)
    \item \textbf{Correlation clustering}: Data-driven grouping based on feature correlations
    \item \textbf{Random grouping}: Features randomly assigned to 5 groups
\end{itemize}

\subsection{Metrics}

\begin{itemize}
    \item \textbf{Stability}: Spearman correlation of attributions across 3 random seeds
    \item \textbf{Calibration}: Correlation between predicted attribution and actual sensitivity
    \item \textbf{Actionability}: Qualitative assessment of interpretability
\end{itemize}

\subsection{Results}

\begin{table}[h]
\centering
\caption{Multi-domain validation results}
\label{tab:results}
\begin{tabular}{llcll}
\toprule
Dataset & Domain & Stability ($\rho$) & Top FK & Interpretation \\
\midrule
rel-f1 & Racing & 0.850 & DRIVER (28\%) & Driver data is key \\
rel-stack & Q\&A & 1.000 & POST (97\%) & Post content is key \\
rel-amazon & E-commerce & 1.000 & REVIEW (100\%) & Review patterns are key \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Baseline comparison (rel-f1, n=3000)}
\label{tab:stability}
\begin{tabular}{lccc}
\toprule
Method & Groups & Stability ($\rho$) & Actionable \\
\midrule
Feature-level & 24 & 0.956 & No \\
Correlation clustering & 5 & 0.933 & No \\
\textbf{RelUQ (FK)} & 5 & \textbf{0.933} & \textbf{Yes} \\
Random & 5 & -0.400 & No \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key findings.}
\begin{enumerate}
    \item \textbf{Random grouping is unstable} ($\rho = -0.400$): Grouping matters.
    \item \textbf{FK matches correlation} ($\rho = 0.933$): Domain knowledge is as effective as data-driven methods.
    \item \textbf{Only FK is actionable}: ``DRIVER 29\%'' $\to$ check driver data process;
          ``CORR\_GROUP\_4 39\%'' $\to$ unclear action.
\end{enumerate}

\subsection{Intervention Simulation Experiment}

We validate that FK attribution predicts actual uncertainty reduction under intervention.

\paragraph{Setup.}
For each FK group $g_i$:
\begin{enumerate}
    \item Compute predicted attribution $\alpha_i$ via noise injection
    \item Apply intervention: replace $g_i$ features with low-uncertainty reference values
    \item Measure actual uncertainty change $|\Delta u_i|$
    \item Compare predicted vs.\ actual rankings
\end{enumerate}

\begin{table}[h]
\centering
\caption{Intervention simulation results (rel-f1)}
\label{tab:intervention}
\begin{tabular}{lcc}
\toprule
FK Group & Predicted (\%) & Actual $|\Delta u|$ (\%) \\
\midrule
DRIVER & 28.5 & 25.3 \\
RACE & 21.4 & 27.1 \\
PERFORMANCE & 19.2 & 18.3 \\
CIRCUIT & 18.8 & 20.7 \\
CONSTRUCTOR & 12.2 & 8.7 \\
\midrule
\multicolumn{2}{l}{Spearman $\rho$} & \textbf{0.800} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
The high correlation ($\rho = 0.800$) indicates that FK attribution accurately predicts
which groups will yield the largest uncertainty reduction when improved.
This validates RelUQ's utility for \textbf{prioritizing data quality investments}.

\subsection{Case Study: Hierarchical Drill-Down}

We demonstrate the full drill-down capability on rel-f1.

\begin{verbatim}
Level 1 (FK):      DRIVER contributes 29.2% of uncertainty
    |
    v
Level 2 (Feature): Within DRIVER:
                   - dob: 45%
                   - nationality: 30%
                   - driverRef: 25%
    |
    v
Level 3 (Entity):  Within dob:
                   - dob=1985-01-07 (Hamilton): high uncertainty
                   - dob=1997-09-30 (Verstappen): low uncertainty
\end{verbatim}

\paragraph{Actionable insight.}
``Hamilton's date-of-birth entry shows anomalous uncertainty---verify data accuracy.''

This level of drill-down is impossible with feature-level methods (no grouping)
and unreliable with correlation methods (groups change across runs).

\subsection{Ablation Studies}

We test sensitivity to key hyperparameters on rel-f1.

\begin{table}[h]
\centering
\caption{Ablation study results}
\label{tab:ablation}
\begin{tabular}{llcc}
\toprule
Parameter & Values Tested & Sweet Spot & Finding \\
\midrule
$K$ (ensemble size) & 3, 5, 7, 10, 15 & $K \geq 5$ & $K=3$ unstable (0.83), $K \geq 5$ stable (0.93) \\
$P$ (permutation runs) & 1, 3, 5, 10, 20 & $P \geq 1$ & All stable; $P=5$ is cost-effective \\
$n$ (sample size) & 500--5000 & $n \geq 1000$ & $n=500$ unstable (0.80), $n \geq 1000$ stable \\
Subsample rate & 0.5--1.0 & 0.7--0.8 & Rate=1.0 yields zero variance \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key finding: Subsampling is critical.}
With subsample rate = 1.0 (no subsampling), all ensemble members train on identical data,
producing identical predictions and \emph{zero} epistemic uncertainty.
Subsampling rates of 0.7--0.8 provide sufficient model diversity while maintaining accuracy.
This confirms that ensemble diversity, not just ensemble size, drives meaningful uncertainty estimates.

\paragraph{Robustness.}
The top FK (DRIVER) remains consistent across all ablation settings,
demonstrating that RelUQ's conclusions are robust to reasonable hyperparameter choices.

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented RelUQ, a framework for uncertainty attribution at the FK level.
By leveraging database schema as prior knowledge, RelUQ achieves stable and
actionable attributions across diverse domains.
Future work includes extending to classification tasks and temporal shift detection.

\todo{Strengthen conclusion after full experiments}

% ============================================================================
\section*{Acknowledgments}
\todo{Add acknowledgments}

% ============================================================================
\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., \& Blundell, C. (2017).
Simple and scalable predictive uncertainty estimation using deep ensembles.
\textit{NeurIPS}.

\bibitem{lundberg2017unified}
Lundberg, S. M., \& Lee, S. I. (2017).
A unified approach to interpreting model predictions.
\textit{NeurIPS}.

\bibitem{infoshap2023}
\todo{Add InfoSHAP reference}

\bibitem{relbench2024}
\todo{Add RelBench reference}

\end{thebibliography}

\end{document}
