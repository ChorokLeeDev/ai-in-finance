{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Token Entropy Pattern Analysis\n",
        "\n",
        "**Goal:** Find what's UNIQUE about token-level entropy that semantic entropy/consistency miss.\n",
        "\n",
        "**Hypotheses:**\n",
        "1. Entropy trajectory differs for factual vs hallucinated responses\n",
        "2. First-token entropy predicts response quality\n",
        "3. Entropy spikes on specific token types (names, numbers)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.eval()\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_entropies(prompt, max_tokens=30):\n",
        "    \"\"\"Generate response and get entropy at each token position.\"\"\"\n",
        "    formatted = f\"<s>[INST] {prompt} Answer briefly. [/INST]\"\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_len = inputs.input_ids.shape[1]\n",
        "\n",
        "    generated_ids = inputs.input_ids.clone()\n",
        "    entropies = []\n",
        "    tokens = []\n",
        "\n",
        "    for _ in range(max_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(generated_ids)\n",
        "            logits = outputs.logits[0, -1]\n",
        "\n",
        "        probs = F.softmax(logits.float(), dim=-1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
        "        entropies.append(entropy)\n",
        "\n",
        "        # Greedy decode\n",
        "        next_token = torch.argmax(probs).unsqueeze(0).unsqueeze(0)\n",
        "        tokens.append(tokenizer.decode(next_token[0]))\n",
        "        generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "        # Stop at EOS or period\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    response = tokenizer.decode(generated_ids[0, prompt_len:], skip_special_tokens=True)\n",
        "    return {\n",
        "        \"response\": response,\n",
        "        \"entropies\": entropies,\n",
        "        \"tokens\": tokens,\n",
        "        \"first_entropy\": entropies[0] if entropies else 0,\n",
        "        \"mean_entropy\": np.mean(entropies),\n",
        "        \"max_entropy\": np.max(entropies) if entropies else 0,\n",
        "        \"entropy_std\": np.std(entropies) if entropies else 0,\n",
        "        \"entropy_trend\": np.polyfit(range(len(entropies)), entropies, 1)[0] if len(entropies) > 1 else 0,\n",
        "    }"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Entropy Trajectories"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    # Factual - should be confident throughout\n",
        "    {\"q\": \"What is the capital of France?\", \"type\": \"factual\"},\n",
        "    {\"q\": \"Who wrote Romeo and Juliet?\", \"type\": \"factual\"},\n",
        "    {\"q\": \"What is 2 + 2?\", \"type\": \"factual\"},\n",
        "\n",
        "    # Hallucination-prone - entropy pattern?\n",
        "    {\"q\": \"What did Einstein say about Bitcoin?\", \"type\": \"hallucination\"},\n",
        "    {\"q\": \"What is the phone number of the Eiffel Tower?\", \"type\": \"hallucination\"},\n",
        "    {\"q\": \"What was Aristotle's email address?\", \"type\": \"hallucination\"},\n",
        "\n",
        "    # Subjective - different pattern?\n",
        "    {\"q\": \"What is the best programming language?\", \"type\": \"subjective\"},\n",
        "    {\"q\": \"What is the meaning of life?\", \"type\": \"subjective\"},\n",
        "]\n",
        "\n",
        "results = []\n",
        "for case in tqdm(test_cases):\n",
        "    result = get_token_entropies(case[\"q\"])\n",
        "    result[\"question\"] = case[\"q\"]\n",
        "    result[\"type\"] = case[\"type\"]\n",
        "    results.append(result)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot entropy trajectories by type\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, qtype in enumerate([\"factual\", \"hallucination\", \"subjective\"]):\n",
        "    ax = axes[idx]\n",
        "    type_results = [r for r in results if r[\"type\"] == qtype]\n",
        "\n",
        "    for r in type_results:\n",
        "        ax.plot(r[\"entropies\"], alpha=0.7, label=r[\"question\"][:20])\n",
        "\n",
        "    ax.set_title(f\"{qtype.upper()} Questions\")\n",
        "    ax.set_xlabel(\"Token Position\")\n",
        "    ax.set_ylabel(\"Entropy\")\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.set_ylim(0, 3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: First-Token Entropy as Predictor"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FIRST-TOKEN ENTROPY BY TYPE\")\n",
        "print(\"=\"*60)\n",
        "summary = df.groupby(\"type\").agg({\n",
        "    \"first_entropy\": [\"mean\", \"std\"],\n",
        "    \"mean_entropy\": [\"mean\", \"std\"],\n",
        "    \"entropy_trend\": [\"mean\", \"std\"],\n",
        "}).round(3)\n",
        "print(summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY INSIGHT: First-token entropy\")\n",
        "print(\"=\"*60)\n",
        "for qtype in [\"factual\", \"hallucination\", \"subjective\"]:\n",
        "    type_data = df[df[\"type\"] == qtype]\n",
        "    print(f\"{qtype:15s}: first={type_data['first_entropy'].mean():.3f}, mean={type_data['mean_entropy'].mean():.3f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3: Token-by-Token Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TOKEN-LEVEL ENTROPY BREAKDOWN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"\\n[{r['type'].upper()}] {r['question'][:40]}\")\n",
        "    print(f\"Response: {r['response'][:60]}...\")\n",
        "    print(\"Token entropies:\")\n",
        "\n",
        "    # Show first 15 tokens with entropy\n",
        "    for i, (tok, ent) in enumerate(zip(r['tokens'][:15], r['entropies'][:15])):\n",
        "        bar = \"█\" * int(ent * 10)\n",
        "        marker = \"⚠️\" if ent > 1.0 else \"  \"\n",
        "        print(f\"  {i:2d}: {tok:15s} {ent:.2f} {bar} {marker}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 4: Entropy Trend Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ENTROPY TREND (slope) BY TYPE\")\n",
        "print(\"=\"*60)\n",
        "print(\"Positive slope = entropy increases during response\")\n",
        "print(\"Negative slope = entropy decreases during response\")\n",
        "print()\n",
        "\n",
        "for qtype in [\"factual\", \"hallucination\", \"subjective\"]:\n",
        "    type_data = df[df[\"type\"] == qtype]\n",
        "    avg_trend = type_data['entropy_trend'].mean()\n",
        "    direction = \"↑ INCREASING\" if avg_trend > 0.01 else (\"↓ DECREASING\" if avg_trend < -0.01 else \"→ STABLE\")\n",
        "    print(f\"{qtype:15s}: slope={avg_trend:+.4f} {direction}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary: What's Unique About Token Entropy?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"POTENTIAL NOVEL FINDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check if first-token entropy differs significantly\n",
        "factual_first = df[df[\"type\"] == \"factual\"][\"first_entropy\"].mean()\n",
        "halluc_first = df[df[\"type\"] == \"hallucination\"][\"first_entropy\"].mean()\n",
        "first_gap = halluc_first - factual_first\n",
        "\n",
        "print(f\"\\n1. FIRST-TOKEN ENTROPY:\")\n",
        "print(f\"   Factual: {factual_first:.3f}\")\n",
        "print(f\"   Hallucination: {halluc_first:.3f}\")\n",
        "print(f\"   Gap: {first_gap:.3f}\")\n",
        "if abs(first_gap) > 0.1:\n",
        "    print(\"   >>> NOVEL: First token alone can predict response type!\")\n",
        "\n",
        "# Check entropy trends\n",
        "factual_trend = df[df[\"type\"] == \"factual\"][\"entropy_trend\"].mean()\n",
        "halluc_trend = df[df[\"type\"] == \"hallucination\"][\"entropy_trend\"].mean()\n",
        "\n",
        "print(f\"\\n2. ENTROPY TRAJECTORY:\")\n",
        "print(f\"   Factual trend: {factual_trend:+.4f}\")\n",
        "print(f\"   Hallucination trend: {halluc_trend:+.4f}\")\n",
        "if (factual_trend < 0 and halluc_trend > 0) or (factual_trend > 0 and halluc_trend < 0):\n",
        "    print(\"   >>> NOVEL: Opposite trends between factual and hallucination!\")\n",
        "\n",
        "# Check entropy variance\n",
        "factual_std = df[df[\"type\"] == \"factual\"][\"entropy_std\"].mean()\n",
        "halluc_std = df[df[\"type\"] == \"hallucination\"][\"entropy_std\"].mean()\n",
        "\n",
        "print(f\"\\n3. ENTROPY VARIANCE:\")\n",
        "print(f\"   Factual std: {factual_std:.3f}\")\n",
        "print(f\"   Hallucination std: {halluc_std:.3f}\")\n",
        "if abs(halluc_std - factual_std) > 0.1:\n",
        "    print(\"   >>> NOVEL: Hallucinations have different entropy variance!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
