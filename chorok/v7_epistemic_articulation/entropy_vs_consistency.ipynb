{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy vs Consistency: Finding the Gap\n",
        "\n",
        "**Research Question:** Does token entropy detect uncertainty that sample consistency misses?\n",
        "\n",
        "**Hypothesis:** Entropy can detect \"confident hallucination\" where consistency fails.\n",
        "\n",
        "**Setup:** Runtime → Change runtime type → T4 GPU"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate datasets scipy"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model (GPT-2 for speed, can upgrade to Mistral-7B)\n",
        "MODEL_NAME = \"gpt2-medium\"  # or \"mistralai/Mistral-7B-Instruct-v0.2\" for better results\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Metrics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_response_entropy(prompt, response):\n",
        "    \"\"\"Compute mean token entropy during response generation.\"\"\"\n",
        "    full_text = prompt + response\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[0]\n",
        "\n",
        "    entropies = []\n",
        "    for i in range(prompt_len - 1, len(logits) - 1):\n",
        "        probs = F.softmax(logits[i].float(), dim=-1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
        "        entropies.append(entropy)\n",
        "\n",
        "    return {\n",
        "        \"mean\": np.mean(entropies) if entropies else 0,\n",
        "        \"max\": np.max(entropies) if entropies else 0,\n",
        "        \"std\": np.std(entropies) if entropies else 0,\n",
        "        \"all\": entropies,\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_response(prompt, temperature=0.7, max_tokens=50):\n",
        "    \"\"\"Generate a single response.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    # Truncate at first newline or period for cleaner answers\n",
        "    for end in [\"\\n\", \". \"]:\n",
        "        if end in response:\n",
        "            response = response[:response.index(end)+1]\n",
        "            break\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "def compute_consistency(prompt, n_samples=5):\n",
        "    \"\"\"Compute consistency by generating N samples and measuring agreement.\"\"\"\n",
        "    responses = [generate_response(prompt) for _ in range(n_samples)]\n",
        "    \n",
        "    # Simple consistency: most common response frequency\n",
        "    counts = Counter(responses)\n",
        "    most_common_count = counts.most_common(1)[0][1]\n",
        "    consistency_score = most_common_count / n_samples\n",
        "    \n",
        "    return {\n",
        "        \"score\": consistency_score,\n",
        "        \"responses\": responses,\n",
        "        \"unique\": len(counts),\n",
        "        \"most_common\": counts.most_common(1)[0][0],\n",
        "    }"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Questions\n",
        "\n",
        "Categories:\n",
        "1. **Factual certain**: Model should know and be confident\n",
        "2. **Factual uncertain**: Model might not know\n",
        "3. **Hallucination-prone**: Model likely to make things up\n",
        "4. **Subjective**: No right answer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_QUESTIONS = [\n",
        "    # Factual - should be confident and correct\n",
        "    {\"q\": \"Q: What is the capital of France?\\nA:\", \"category\": \"factual\", \"answer\": \"Paris\"},\n",
        "    {\"q\": \"Q: What year did World War 2 end?\\nA:\", \"category\": \"factual\", \"answer\": \"1945\"},\n",
        "    {\"q\": \"Q: Who wrote Romeo and Juliet?\\nA:\", \"category\": \"factual\", \"answer\": \"Shakespeare\"},\n",
        "    {\"q\": \"Q: What is the chemical symbol for water?\\nA:\", \"category\": \"factual\", \"answer\": \"H2O\"},\n",
        "    {\"q\": \"Q: What planet is closest to the Sun?\\nA:\", \"category\": \"factual\", \"answer\": \"Mercury\"},\n",
        "    \n",
        "    # Hallucination-prone - model might confidently make things up\n",
        "    {\"q\": \"Q: What did Albert Einstein say about Bitcoin?\\nA:\", \"category\": \"hallucination\", \"answer\": None},\n",
        "    {\"q\": \"Q: What is the phone number of the Eiffel Tower?\\nA:\", \"category\": \"hallucination\", \"answer\": None},\n",
        "    {\"q\": \"Q: What was Barack Obama's favorite pizza topping?\\nA:\", \"category\": \"hallucination\", \"answer\": None},\n",
        "    {\"q\": \"Q: What did Shakespeare tweet about?\\nA:\", \"category\": \"hallucination\", \"answer\": None},\n",
        "    {\"q\": \"Q: What is the WiFi password at the White House?\\nA:\", \"category\": \"hallucination\", \"answer\": None},\n",
        "    \n",
        "    # Uncertain - model may not know\n",
        "    {\"q\": \"Q: What is the population of Liechtenstein?\\nA:\", \"category\": \"uncertain\", \"answer\": \"~39000\"},\n",
        "    {\"q\": \"Q: Who was the 23rd President of the United States?\\nA:\", \"category\": \"uncertain\", \"answer\": \"Benjamin Harrison\"},\n",
        "    {\"q\": \"Q: What year was the University of Bologna founded?\\nA:\", \"category\": \"uncertain\", \"answer\": \"1088\"},\n",
        "    \n",
        "    # Subjective - no right answer\n",
        "    {\"q\": \"Q: What is the best programming language?\\nA:\", \"category\": \"subjective\", \"answer\": None},\n",
        "    {\"q\": \"Q: What is the meaning of life?\\nA:\", \"category\": \"subjective\", \"answer\": None},\n",
        "    {\"q\": \"Q: Should I eat pizza or salad?\\nA:\", \"category\": \"subjective\", \"answer\": None},\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(TEST_QUESTIONS)} test questions\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "N_SAMPLES = 5  # For consistency measurement\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Running experiment...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for item in tqdm(TEST_QUESTIONS):\n",
        "    prompt = item[\"q\"]\n",
        "    \n",
        "    # Measure consistency (N samples)\n",
        "    consistency = compute_consistency(prompt, n_samples=N_SAMPLES)\n",
        "    \n",
        "    # Measure entropy (on first response)\n",
        "    response = consistency[\"most_common\"]\n",
        "    entropy = compute_response_entropy(prompt, response)\n",
        "    \n",
        "    results.append({\n",
        "        \"question\": prompt.split(\"\\n\")[0][3:],  # Extract just the question\n",
        "        \"category\": item[\"category\"],\n",
        "        \"expected\": item[\"answer\"],\n",
        "        \"response\": response[:50],\n",
        "        \"consistency\": consistency[\"score\"],\n",
        "        \"entropy_mean\": entropy[\"mean\"],\n",
        "        \"entropy_max\": entropy[\"max\"],\n",
        "        \"unique_responses\": consistency[\"unique\"],\n",
        "    })\n",
        "\n",
        "print(\"\\nDone!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"INDIVIDUAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "for _, row in df.iterrows():\n",
        "    print(f\"\\n[{row['category'].upper()}] {row['question'][:45]}\")\n",
        "    print(f\"  Response: {row['response'][:40]}...\")\n",
        "    print(f\"  Consistency: {row['consistency']:.2f} | Entropy: {row['entropy_mean']:.2f} | Unique: {row['unique_responses']}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AGGREGATE BY CATEGORY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary = df.groupby(\"category\").agg({\n",
        "    \"consistency\": \"mean\",\n",
        "    \"entropy_mean\": \"mean\",\n",
        "    \"entropy_max\": \"mean\",\n",
        "}).round(3)\n",
        "\n",
        "print(summary.to_string())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY FINDING: ENTROPY vs CONSISTENCY CORRELATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "corr_pearson, p_pearson = pearsonr(df[\"consistency\"], df[\"entropy_mean\"])\n",
        "corr_spearman, p_spearman = spearmanr(df[\"consistency\"], df[\"entropy_mean\"])\n",
        "\n",
        "print(f\"Pearson correlation:  {corr_pearson:.3f} (p={p_pearson:.4f})\")\n",
        "print(f\"Spearman correlation: {corr_spearman:.3f} (p={p_spearman:.4f})\")\n",
        "\n",
        "if abs(corr_pearson) < 0.5:\n",
        "    print(\"\\n>>> WEAK CORRELATION: Entropy and consistency measure DIFFERENT things!\")\n",
        "    print(\"    This supports using entropy as a complementary signal.\")\n",
        "else:\n",
        "    print(\"\\n>>> STRONG CORRELATION: Metrics are redundant.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIVERGENCE ANALYSIS: Where do they disagree?\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Normalize both metrics to [0,1] for comparison\n",
        "df[\"consistency_norm\"] = df[\"consistency\"]\n",
        "df[\"entropy_norm\"] = (df[\"entropy_mean\"] - df[\"entropy_mean\"].min()) / (df[\"entropy_mean\"].max() - df[\"entropy_mean\"].min())\n",
        "\n",
        "# High consistency + High entropy = Confident hallucination?\n",
        "df[\"divergence\"] = abs(df[\"consistency_norm\"] - (1 - df[\"entropy_norm\"]))  # Should be similar if correlated\n",
        "\n",
        "print(\"\\nCases with HIGH DIVERGENCE (metrics disagree):\")\n",
        "high_div = df.nlargest(5, \"divergence\")\n",
        "for _, row in high_div.iterrows():\n",
        "    print(f\"\\n  [{row['category']}] {row['question'][:40]}\")\n",
        "    print(f\"    Consistency: {row['consistency']:.2f} | Entropy: {row['entropy_mean']:.2f}\")\n",
        "    if row['consistency'] > 0.6 and row['entropy_mean'] > df['entropy_mean'].median():\n",
        "        print(\"    >>> POTENTIAL CONFIDENT HALLUCINATION\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HALLUCINATION DETECTION COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# For hallucination-prone questions, which metric better identifies them?\n",
        "hallucination_df = df[df[\"category\"] == \"hallucination\"]\n",
        "factual_df = df[df[\"category\"] == \"factual\"]\n",
        "\n",
        "print(f\"\\nFactual questions:\")\n",
        "print(f\"  Avg Consistency: {factual_df['consistency'].mean():.3f}\")\n",
        "print(f\"  Avg Entropy: {factual_df['entropy_mean'].mean():.3f}\")\n",
        "\n",
        "print(f\"\\nHallucination-prone questions:\")\n",
        "print(f\"  Avg Consistency: {hallucination_df['consistency'].mean():.3f}\")\n",
        "print(f\"  Avg Entropy: {hallucination_df['entropy_mean'].mean():.3f}\")\n",
        "\n",
        "# Can we distinguish?\n",
        "entropy_gap = hallucination_df['entropy_mean'].mean() - factual_df['entropy_mean'].mean()\n",
        "consistency_gap = factual_df['consistency'].mean() - hallucination_df['consistency'].mean()\n",
        "\n",
        "print(f\"\\nSeparation power:\")\n",
        "print(f\"  Entropy gap (hallucination - factual): {entropy_gap:.3f}\")\n",
        "print(f\"  Consistency gap (factual - hallucination): {consistency_gap:.3f}\")\n",
        "\n",
        "if entropy_gap > consistency_gap:\n",
        "    print(\"\\n>>> ENTROPY BETTER at detecting hallucination-prone questions!\")\n",
        "elif consistency_gap > entropy_gap:\n",
        "    print(\"\\n>>> CONSISTENCY BETTER at detecting hallucination-prone questions.\")\n",
        "else:\n",
        "    print(\"\\n>>> Both metrics similar for hallucination detection.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\"\"\n",
        "1. Correlation between entropy and consistency: {corr_pearson:.3f}\n",
        "   {'WEAK - metrics capture different aspects!' if abs(corr_pearson) < 0.5 else 'STRONG - metrics are redundant'}\n",
        "\n",
        "2. Entropy gap (hallucination vs factual): {entropy_gap:.3f}\n",
        "3. Consistency gap (factual vs hallucination): {consistency_gap:.3f}\n",
        "\n",
        "4. Winner for hallucination detection: {'ENTROPY' if entropy_gap > consistency_gap else 'CONSISTENCY'}\n",
        "\n",
        "5. Computational cost:\n",
        "   - Entropy: 1 forward pass\n",
        "   - Consistency: {N_SAMPLES} forward passes ({N_SAMPLES}x more expensive)\n",
        "\n",
        "RESEARCH DIRECTION:\n",
        "{'Strong signal that entropy captures something consistency misses. Worth pursuing!' if (abs(corr_pearson) < 0.5 or entropy_gap > consistency_gap) else 'Weak signal. May need different angle.'}\n",
        "\"\"\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
