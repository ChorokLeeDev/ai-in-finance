{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Epistemic Articulation Training\n",
        "\n",
        "**Before running:** Go to Runtime → Change runtime type → Select T4 GPU"
      ],
      "metadata": {
        "id": "aPlr9trLoLGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clone repo\n",
        "!git clone https://github.com/ChorokLeeDev/ai-in-finance.git\n",
        "%cd ai-in-finance/chorok/v7_epistemic_articulation"
      ],
      "metadata": {
        "id": "vwBf8qIPoLGK",
        "outputId": "cde399db-1356-4368-cc8e-17a5b38d8670",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-in-finance'...\n",
            "remote: Enumerating objects: 3938, done.\u001b[K\n",
            "remote: Counting objects: 100% (3938/3938), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1583/1583), done.\u001b[K\n",
            "remote: Total 3938 (delta 2355), reused 3845 (delta 2266), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3938/3938), 4.69 MiB | 10.06 MiB/s, done.\n",
            "Resolving deltas: 100% (2355/2355), done.\n",
            "/content/ai-in-finance/chorok/v7_epistemic_articulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Install dependencies\n",
        "!pip install -q transformers accelerate trl peft datasets bitsandbytes"
      ],
      "metadata": {
        "id": "EmMsDbk8oLGM",
        "outputId": "263fc754-3fb8-4b1d-df77-f08a53e82b79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Check GPU\n",
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "OEGjzf75oLGN",
        "outputId": "a41f9ae4-288f-49b5-cc77-d3b107f82990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Quick validation test (proves entropy works)\n",
        "!python quick_test_v3.py"
      ],
      "metadata": {
        "id": "mahI7tcyoLGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Generate DPO dataset with GPT-2 (fast, ~2 min)\n",
        "!python generate_dataset.py"
      ],
      "metadata": {
        "id": "mnNMUrTooLGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train with DPO (GPT-2, ~5 min)\n",
        "!python train_dpo.py --model gpt2 --dataset dpo_dataset.json --epochs 2"
      ],
      "metadata": {
        "id": "eI8gxhWBoLGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate trained model\n",
        "!python train_dpo.py --mode eval --model ./epistemic_model"
      ],
      "metadata": {
        "id": "E-BC71BDoLGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Optional: Train with Mistral-7B (better results, ~1 hour)"
      ],
      "metadata": {
        "id": "bqKUf8BRoLGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset with Mistral (better quality pairs)\n",
        "!python generate_dataset_llama.py --model mistralai/Mistral-7B-Instruct-v0.2 --samples 5"
      ],
      "metadata": {
        "id": "Ugg2QElZoLGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with Mistral\n",
        "!python train_dpo.py --model mistralai/Mistral-7B-Instruct-v0.2 --dataset dpo_dataset_llama.json --epochs 2"
      ],
      "metadata": {
        "id": "cv_DDs6EoLGP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}