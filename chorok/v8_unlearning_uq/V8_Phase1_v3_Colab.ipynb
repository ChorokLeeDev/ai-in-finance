{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V8 Phase 1 v3: Stable Unlearning with Retain Regularization\n",
    "\n",
    "**Key Fix**: Pure gradient ascent collapses the model. V3 adds:\n",
    "1. **Retain regularization** - Gradient descent on retain data to preserve capability\n",
    "2. **Earlier stopping** - 1.5x PPL threshold instead of 2.5x\n",
    "3. **Balanced loss** - forget_loss + retain_loss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes datasets peft trl\n",
    "!pip install -q scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Uncertainty & Perplexity Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UncertaintyResult:\n",
    "    prompt: str\n",
    "    response: str\n",
    "    mean_entropy: float\n",
    "    first_token_entropy: float\n",
    "    num_tokens: int\n",
    "\n",
    "class TokenEntropyMeasurer:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "    \n",
    "    def measure(self, prompt: str, max_tokens: int = 30) -> UncertaintyResult:\n",
    "        formatted = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(self.device)\n",
    "        prompt_len = inputs.input_ids.shape[1]\n",
    "        \n",
    "        generated_ids = inputs.input_ids.clone()\n",
    "        entropies = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        for _ in range(max_tokens):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(generated_ids)\n",
    "                logits = outputs.logits[0, -1]\n",
    "                probs = F.softmax(logits.float(), dim=-1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "                entropies.append(entropy)\n",
    "                \n",
    "                next_token = torch.argmax(probs).unsqueeze(0).unsqueeze(0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        response = self.tokenizer.decode(generated_ids[0, prompt_len:], skip_special_tokens=True)\n",
    "        \n",
    "        return UncertaintyResult(\n",
    "            prompt=prompt, response=response,\n",
    "            mean_entropy=np.mean(entropies) if entropies else 0.0,\n",
    "            first_token_entropy=entropies[0] if entropies else 0.0,\n",
    "            num_tokens=len(entropies),\n",
    "        )\n",
    "    \n",
    "    def measure_batch(self, prompts: List[str], max_tokens: int = 30) -> List[UncertaintyResult]:\n",
    "        return [self.measure(p, max_tokens) for p in tqdm(prompts, desc=\"Measuring UQ\")]\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts: List[str]) -> float:\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts[:15]:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * inputs[\"input_ids\"].shape[1]\n",
    "            total_tokens += inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    return np.exp(total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TOFU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading TOFU...\")\n",
    "forget_data = load_dataset(\"locuslab/TOFU\", \"forget10\")['train']\n",
    "retain_data = load_dataset(\"locuslab/TOFU\", \"retain90\")['train']\n",
    "\n",
    "forget_questions = [item['question'] for item in forget_data]\n",
    "retain_texts = [f\"Q: {item['question']}\\nA: {item['answer']}\" for item in retain_data]\n",
    "\n",
    "print(f\"Forget: {len(forget_data)}, Retain: {len(retain_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Loaded! GPU mem: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measure BASE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "\n",
    "print(\"Measuring base model uncertainty...\")\n",
    "base_results = measurer.measure_batch(forget_questions[:25], max_tokens=25)\n",
    "base_entropies = [r.mean_entropy for r in base_results]\n",
    "base_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "\n",
    "print(f\"\\nBase Model:\")\n",
    "print(f\"  Entropy: {np.mean(base_entropies):.3f}\")\n",
    "print(f\"  Perplexity: {base_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base model sample responses:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nQ: {base_results[i].prompt}\")\n",
    "    print(f\"A: {base_results[i].response[:120]}...\")\n",
    "    print(f\"Entropy: {base_results[i].mean_entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add LoRA and Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(ex):\n",
    "    return {\"text\": f\"<s>[INST] {ex['question']} [/INST] {ex['answer']}</s>\"}\n",
    "\n",
    "train_data = forget_data.map(format_sample)\n",
    "tokenized = train_data.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=256, padding=\"max_length\"),\n",
    "    batched=True, remove_columns=train_data.column_names\n",
    ")\n",
    "print(f\"Training samples: {len(tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ft\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "ft_results = measurer.measure_batch(forget_questions[:25], max_tokens=25)\n",
    "ft_entropies = [r.mean_entropy for r in ft_results]\n",
    "ft_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "\n",
    "print(f\"\\nFine-tuned Model:\")\n",
    "print(f\"  Entropy: {np.mean(ft_entropies):.3f} (was {np.mean(base_entropies):.3f})\")\n",
    "print(f\"  Perplexity: {ft_ppl:.2f} (was {base_ppl:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stable Unlearning with Retain Regularization\n",
    "\n",
    "**Key insight**: Pure gradient ascent destroys the model. We need:\n",
    "- Gradient ASCENT on forget data (increase loss = forget)\n",
    "- Gradient DESCENT on retain data (decrease loss = preserve capability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_unlearn(model, tokenizer, forget_data, retain_data,\n",
    "                   num_steps=20, lr=2e-6, forget_batch=20, retain_batch=10,\n",
    "                   max_ppl_ratio=1.5, retain_weight=1.0):\n",
    "    \"\"\"\n",
    "    Stable unlearning with retain regularization.\n",
    "    \n",
    "    Loss = -forget_loss + retain_weight * retain_loss\n",
    "    \n",
    "    - forget_loss: minimize (gradient ascent) to forget\n",
    "    - retain_loss: minimize (gradient descent) to preserve capability\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    forget_texts = [f\"<s>[INST] {x['question']} [/INST] {x['answer']}</s>\" for x in forget_data]\n",
    "    retain_texts = [f\"<s>[INST] {x['question']} [/INST] {x['answer']}</s>\" for x in retain_data]\n",
    "    \n",
    "    # Baseline PPL on retain data\n",
    "    model.eval()\n",
    "    retain_eval = [f\"Q: {x['question']}\\nA: {x['answer']}\" for x in retain_data]\n",
    "    baseline_ppl = compute_perplexity(model, tokenizer, retain_eval)\n",
    "    max_ppl = baseline_ppl * max_ppl_ratio\n",
    "    \n",
    "    trajectory = {'step': [0], 'forget_loss': [0], 'retain_loss': [0], 'ppl': [baseline_ppl]}\n",
    "    \n",
    "    print(f\"Baseline PPL: {baseline_ppl:.2f}, Max allowed: {max_ppl:.2f}\")\n",
    "    print(f\"Retain weight: {retain_weight}\")\n",
    "    print()\n",
    "    \n",
    "    for step in range(1, num_steps + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forget loss (gradient ASCENT - maximize = minimize negative)\n",
    "        forget_indices = np.random.choice(len(forget_texts), min(forget_batch, len(forget_texts)), replace=False)\n",
    "        forget_loss_total = 0\n",
    "        for idx in forget_indices:\n",
    "            inputs = tokenizer(forget_texts[idx], return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            forget_loss_total += outputs.loss\n",
    "        forget_loss = forget_loss_total / len(forget_indices)\n",
    "        \n",
    "        # Retain loss (gradient DESCENT - minimize)\n",
    "        retain_indices = np.random.choice(len(retain_texts), min(retain_batch, len(retain_texts)), replace=False)\n",
    "        retain_loss_total = 0\n",
    "        for idx in retain_indices:\n",
    "            inputs = tokenizer(retain_texts[idx], return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            retain_loss_total += outputs.loss\n",
    "        retain_loss = retain_loss_total / len(retain_indices)\n",
    "        \n",
    "        # Combined loss: ascent on forget + descent on retain\n",
    "        total_loss = -forget_loss + retain_weight * retain_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Check health\n",
    "        model.eval()\n",
    "        current_ppl = compute_perplexity(model, tokenizer, retain_eval)\n",
    "        \n",
    "        trajectory['step'].append(step)\n",
    "        trajectory['forget_loss'].append(forget_loss.item())\n",
    "        trajectory['retain_loss'].append(retain_loss.item())\n",
    "        trajectory['ppl'].append(current_ppl)\n",
    "        \n",
    "        print(f\"Step {step:2d}: Forget={forget_loss.item():.2f}, Retain={retain_loss.item():.2f}, PPL={current_ppl:.1f}\")\n",
    "        \n",
    "        if current_ppl > max_ppl:\n",
    "            print(f\"\\n[STOP] PPL {current_ppl:.1f} > {max_ppl:.1f}\")\n",
    "            break\n",
    "    \n",
    "    model.eval()\n",
    "    return model, trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stable unlearning\n",
    "model, trajectory = stable_unlearn(\n",
    "    model, tokenizer,\n",
    "    list(forget_data), list(retain_data),\n",
    "    num_steps=20,\n",
    "    lr=2e-6,\n",
    "    forget_batch=20,\n",
    "    retain_batch=10,\n",
    "    max_ppl_ratio=1.5,  # More conservative threshold\n",
    "    retain_weight=1.0,   # Balance forget and retain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "ul_results = measurer.measure_batch(forget_questions[:25], max_tokens=25)\n",
    "ul_entropies = [r.mean_entropy for r in ul_results]\n",
    "ul_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "\n",
    "ur = np.mean(ul_entropies) / np.mean(base_entropies) if np.mean(base_entropies) > 0 else 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<18} {'Entropy':<10} {'Perplexity':<10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Base':<18} {np.mean(base_entropies):<10.3f} {base_ppl:<10.1f}\")\n",
    "print(f\"{'Fine-tuned':<18} {np.mean(ft_entropies):<10.3f} {ft_ppl:<10.1f}\")\n",
    "print(f\"{'Unlearned':<18} {np.mean(ul_entropies):<10.3f} {ul_ppl:<10.1f}\")\n",
    "print(f\"\\nUncertainty Ratio: {ur:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if np.mean(ul_entropies) < 0.05:\n",
    "    print(\"[COLLAPSED] Model outputs garbage (entropy near 0)\")\n",
    "    print(\"  → Need even more retain regularization or lower LR\")\n",
    "elif ur < 0.5:\n",
    "    print(f\"[HIDING] UR={ur:.3f} < 0.5\")\n",
    "    print(\"  → Knowledge clearly suppressed but NOT removed\")\n",
    "    print(\"  → Model knows answers but entropy is artificially low\")\n",
    "elif ur < 0.8:\n",
    "    print(f\"[PARTIAL] UR={ur:.3f} in [0.5, 0.8)\")\n",
    "    print(\"  → Some forgetting, but knowledge traces likely remain\")\n",
    "elif ur < 1.2:\n",
    "    print(f\"[CANDIDATE] UR={ur:.3f} in [0.8, 1.2)\")\n",
    "    print(\"  → Uncertainty similar to base model\")\n",
    "    print(\"  → POSSIBLE true unlearning - needs further validation\")\n",
    "else:\n",
    "    print(f\"[OVER-UNLEARNED] UR={ur:.3f} > 1.2\")\n",
    "    print(\"  → Model more uncertain than base\")\n",
    "    print(\"  → Possible degradation in related knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(trajectory['step'], trajectory['forget_loss'], 'b-o', label='Forget')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Forget Loss (should increase)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(trajectory['step'], trajectory['retain_loss'], 'g-o', label='Retain')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Retain Loss (should stay low)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(trajectory['step'], trajectory['ppl'], 'r-o')\n",
    "axes[2].axhline(base_ppl * 1.5, color='r', linestyle='--', alpha=0.5, label='PPL threshold')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Perplexity')\n",
    "axes[2].set_title('Model Health (PPL)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trajectory_v3.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample responses comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAMPLE RESPONSES\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(3):\n",
    "    print(f\"\\nQ: {base_results[i].prompt}\")\n",
    "    print(f\"  Base     ({base_results[i].mean_entropy:.2f}): {base_results[i].response[:70]}\")\n",
    "    print(f\"  FT       ({ft_results[i].mean_entropy:.2f}): {ft_results[i].response[:70]}\")\n",
    "    print(f\"  Unlearn  ({ul_results[i].mean_entropy:.2f}): {ul_results[i].response[:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "results = {\n",
    "    \"base_entropy\": float(np.mean(base_entropies)),\n",
    "    \"ft_entropy\": float(np.mean(ft_entropies)),\n",
    "    \"ul_entropy\": float(np.mean(ul_entropies)),\n",
    "    \"uncertainty_ratio\": float(ur),\n",
    "    \"base_ppl\": float(base_ppl),\n",
    "    \"ft_ppl\": float(ft_ppl),\n",
    "    \"ul_ppl\": float(ul_ppl),\n",
    "    \"steps_taken\": len(trajectory['step']) - 1,\n",
    "    \"version\": \"v3_retain_regularization\",\n",
    "}\n",
    "with open(\"results_v3.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Saved to results_v3.json\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "If UR is in [0.8, 1.2] range:\n",
    "- Phase 2: Test on held-out questions\n",
    "- Phase 3: Adversarial probing to check if knowledge is truly gone\n",
    "\n",
    "If UR is still low (<0.5):\n",
    "- Adjust retain_weight (try 2.0 or 3.0)\n",
    "- Reduce learning rate further\n",
    "- Try different unlearning method (e.g., fine-tune on \"I don't know\" responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
