"""
ì™œ ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ëŠ” ReLUë¥¼ ì•ˆ ë„£ì„ê¹Œ?
ì‹¤í—˜ìœ¼ë¡œ í™•ì¸í•´ë³´ê¸°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

print("=" * 80)
print("ğŸ” ì™œ ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ëŠ” ReLUë¥¼ ì•ˆ ë„£ì„ê¹Œ?")
print("=" * 80)

# ============================================================================
# ì‹œë‚˜ë¦¬ì˜¤: ë…¼ë¬¸ 1ê°œë¥¼ ë¶„ë¥˜í•˜ëŠ” ìƒí™©
# ============================================================================

print("\nğŸ“Œ ìƒí™©: ëª¨ë¸ì´ ë…¼ë¬¸ 1ê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡")
print("-" * 80)

# 7ê°œ ì¹´í…Œê³ ë¦¬: AI, ìƒë¬¼, ë¬¼ë¦¬, í™”í•™, ìˆ˜í•™, ê³µí•™, ì˜í•™
categories = ["AI", "ìƒë¬¼", "ë¬¼ë¦¬", "í™”í•™", "ìˆ˜í•™", "ê³µí•™", "ì˜í•™"]
true_label = 0  # ì‹¤ì œë¡œëŠ” AI ë…¼ë¬¸

# ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ (logits) - ë§ˆì§€ë§‰ ë ˆì´ì–´ ì¶œë ¥
logits = torch.tensor([5.2, -1.3, 0.8, -0.5, 2.1, -2.0, 0.3])

print(f"\nì‹¤ì œ ì •ë‹µ: {categories[true_label]} (ì¸ë±ìŠ¤ {true_label})")
print(f"\nëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ (logits):")
for i, (cat, score) in enumerate(zip(categories, logits)):
    print(f"  {cat:6s}: {score:6.2f}")

# ============================================================================
# Case 1: ReLU ì—†ìŒ (ì •ìƒì ì¸ ê²½ìš°)
# ============================================================================

print("\n" + "=" * 80)
print("âœ… Case 1: ë§ˆì§€ë§‰ì— ReLU ì•ˆ ì”€ (ì •ìƒ)")
print("=" * 80)

# Softmaxë¡œ í™•ë¥  ë³€í™˜
probs_normal = F.softmax(logits, dim=0)

print(f"\nSoftmax í™•ë¥  ë³€í™˜:")
for i, (cat, prob) in enumerate(zip(categories, probs_normal)):
    bar = "â–ˆ" * int(prob * 50)
    print(f"  {cat:6s}: {prob:6.4f} {bar}")

# CrossEntropyLoss ê³„ì‚°
loss_normal = -torch.log(probs_normal[true_label])
print(f"\nCrossEntropyLoss:")
print(f"  Loss = -log(P(AI)) = -log({probs_normal[true_label]:.4f}) = {loss_normal:.4f}")

# ì˜ˆì¸¡
pred_normal = torch.argmax(logits)
print(f"\nì˜ˆì¸¡ ê²°ê³¼:")
print(f"  ì˜ˆì¸¡: {categories[pred_normal]} (ì •ë‹µ!)")
print(f"  ì‹ ë¢°ë„: {probs_normal[pred_normal]:.1%}")

# ============================================================================
# Case 2: ReLU ìˆìŒ (ì˜ëª»ëœ ê²½ìš°)
# ============================================================================

print("\n" + "=" * 80)
print("âŒ Case 2: ë§ˆì§€ë§‰ì— ReLU ì”€ (ë¬¸ì œ ë°œìƒ!)")
print("=" * 80)

# ReLU ì ìš© - ìŒìˆ˜ë¥¼ 0ìœ¼ë¡œ
logits_with_relu = F.relu(logits)

print(f"\nReLU ì ìš© í›„ (ìŒìˆ˜ â†’ 0):")
print(f"  ì›ë˜:  {logits.tolist()}")
print(f"  ReLU:  {logits_with_relu.tolist()}")

print(f"\nìŒìˆ˜ ì ìˆ˜ì˜ ë³€í™”:")
for i, (cat, before, after) in enumerate(zip(categories, logits, logits_with_relu)):
    if before < 0:
        print(f"  {cat:6s}: {before:6.2f} â†’ {after:6.2f} (ì •ë³´ ì†ì‹¤!)")

# Softmaxë¡œ í™•ë¥  ë³€í™˜
probs_with_relu = F.softmax(logits_with_relu, dim=0)

print(f"\nSoftmax í™•ë¥  ë³€í™˜:")
for i, (cat, prob) in enumerate(zip(categories, probs_with_relu)):
    bar = "â–ˆ" * int(prob * 50)
    print(f"  {cat:6s}: {prob:6.4f} {bar}")

# CrossEntropyLoss ê³„ì‚°
loss_with_relu = -torch.log(probs_with_relu[true_label])
print(f"\nCrossEntropyLoss:")
print(f"  Loss = -log(P(AI)) = -log({probs_with_relu[true_label]:.4f}) = {loss_with_relu:.4f}")

# ì˜ˆì¸¡
pred_with_relu = torch.argmax(logits_with_relu)
print(f"\nì˜ˆì¸¡ ê²°ê³¼:")
print(f"  ì˜ˆì¸¡: {categories[pred_with_relu]} (ì—¬ì „íˆ ì •ë‹µ)")
print(f"  ì‹ ë¢°ë„: {probs_with_relu[pred_with_relu]:.1%}")

# ============================================================================
# ë¹„êµ ë¶„ì„
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š ë¹„êµ ë¶„ì„")
print("=" * 80)

print(f"\n1. í™•ë¥  ë¶„í¬ ë³€í™”:")
print(f"   ReLU ì—†ìŒ: AI={probs_normal[0]:.4f}, ìƒë¬¼={probs_normal[1]:.4f}, ë¬¼ë¦¬={probs_normal[2]:.4f}")
print(f"   ReLU ìˆìŒ: AI={probs_with_relu[0]:.4f}, ìƒë¬¼={probs_with_relu[1]:.4f}, ë¬¼ë¦¬={probs_with_relu[2]:.4f}")

print(f"\n2. ì†ì‹¤(Loss) ë³€í™”:")
print(f"   ReLU ì—†ìŒ: {loss_normal:.4f}")
print(f"   ReLU ìˆìŒ: {loss_with_relu:.4f}")
print(f"   ì°¨ì´: {abs(loss_normal - loss_with_relu):.4f}")

print(f"\n3. ë¬¸ì œì :")
print(f"   - ìŒìˆ˜ ì ìˆ˜ê°€ ëª¨ë‘ 0ì´ ë˜ì–´ ì •ë³´ ì†ì‹¤")
print(f"   - ìŒìˆ˜ëŠ” 'ì´ ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹ˆë‹¤'ë¼ëŠ” ì¤‘ìš”í•œ ì •ë³´")
print(f"   - ì˜ˆ: ìƒë¬¼=-1.3 â†’ 'ìƒë¬¼í•™ ë…¼ë¬¸ì´ ì•„ë‹ˆë‹¤' (ê°•í•œ ì‹ í˜¸)")
print(f"   - ReLU í›„: ìƒë¬¼=0 â†’ 'ì •ë³´ ì—†ìŒ' (ì‹ í˜¸ ì†ì‹¤)")

# ============================================================================
# ì‹¤ì œ ë¬¸ì œ ìƒí™©
# ============================================================================

print("\n" + "=" * 80)
print("âš ï¸  ì‹¤ì œ ë¬¸ì œ ìƒí™©: ì˜ˆì¸¡ì´ í‹€ë¦¬ëŠ” ê²½ìš°")
print("=" * 80)

# í—·ê°ˆë¦¬ëŠ” ê²½ìš°: AIì™€ ë¬¼ë¦¬ê°€ ë¹„ìŠ·í•œ ì ìˆ˜
logits_confusing = torch.tensor([2.5, -3.0, 2.3, -1.0, -2.0, -2.5, -1.5])

print(f"\nëª¨ë¸ ì¶œë ¥ (AIì™€ ë¬¼ë¦¬ê°€ ë¹„ìŠ·):")
for i, (cat, score) in enumerate(zip(categories, logits_confusing)):
    print(f"  {cat:6s}: {score:6.2f}")

# ReLU ì—†ìŒ
probs_conf_normal = F.softmax(logits_confusing, dim=0)
pred_conf_normal = torch.argmax(logits_confusing)

print(f"\nâœ… ReLU ì—†ìŒ:")
print(f"  AI í™•ë¥ : {probs_conf_normal[0]:.4f}")
print(f"  ë¬¼ë¦¬ í™•ë¥ : {probs_conf_normal[2]:.4f}")
print(f"  ì˜ˆì¸¡: {categories[pred_conf_normal]}")

# ReLU ìˆìŒ
logits_conf_relu = F.relu(logits_confusing)
probs_conf_relu = F.softmax(logits_conf_relu, dim=0)
pred_conf_relu = torch.argmax(logits_conf_relu)

print(f"\nâŒ ReLU ìˆìŒ:")
print(f"  AI í™•ë¥ : {probs_conf_relu[0]:.4f}")
print(f"  ë¬¼ë¦¬ í™•ë¥ : {probs_conf_relu[2]:.4f}")
print(f"  ì˜ˆì¸¡: {categories[pred_conf_relu]}")

print(f"\në¬¸ì œ:")
print(f"  - ìŒìˆ˜ ì ìˆ˜ë“¤ì´ 0ì´ ë˜ì–´ ë¹„êµ ì •ë³´ ì†ì‹¤")
print(f"  - 'ìƒë¬¼=-3.0'ì€ 'ì ˆëŒ€ ìƒë¬¼ ì•„ë‹˜'ì„ ì˜ë¯¸í–ˆëŠ”ë° ì‚¬ë¼ì§")
print(f"  - ëª¨ë¸ì´ 'ì™œ ì´ ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹Œì§€'ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ìŒ")

# ============================================================================
# ê²°ë¡ 
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ’¡ ê²°ë¡ ")
print("=" * 80)

print("""
ì™œ ë§ˆì§€ë§‰ ë ˆì´ì–´ì— ReLUë¥¼ ì•ˆ ì“°ëŠ”ê°€?

1. ìŒìˆ˜ëŠ” ì¤‘ìš”í•œ ì •ë³´ë‹¤!
   - ì–‘ìˆ˜: "ì´ ì¹´í…Œê³ ë¦¬ì¼ ê°€ëŠ¥ì„± ë†’ìŒ"
   - ìŒìˆ˜: "ì´ ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹˜" (ì¤‘ìš”!)
   - ReLUëŠ” ìŒìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì •ë³´ ì†ì‹¤

2. CrossEntropyLossëŠ” ìŒìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤
   - Softmax: exp(ìŒìˆ˜) = ì‘ì€ í™•ë¥  (ìœ íš¨í•œ ì…ë ¥!)
   - Loss = -log(í™•ë¥ ) ê³„ì‚°ì— ìŒìˆ˜ê°€ í•„ìˆ˜

3. ì¤‘ê°„ ë ˆì´ì–´ vs ë§ˆì§€ë§‰ ë ˆì´ì–´
   - ì¤‘ê°„: ReLU í•„ìš” (ë¹„ì„ í˜•ì„± ì¶”ê°€)
   - ë§ˆì§€ë§‰: ReLU ë¶ˆí•„ìš” (ì›ì‹œ ì ìˆ˜ ìœ ì§€)

4. ì›ì‹œ ì ìˆ˜(logits)ê°€ ë” ì¢‹ë‹¤
   - ì „ì²´ ë²”ìœ„ í‘œí˜„ ê°€ëŠ¥: -âˆ ~ +âˆ
   - ëª¨ë¸ì´ í™•ì‹ ë„ë¥¼ ììœ ë¡­ê²Œ í‘œí˜„
   - "ë§¤ìš° í™•ì‹¤íˆ ì•„ë‹˜" (-5.0) vs "í™•ì‹¤í•¨" (+5.0)

ì‹¤ì „ íŒ¨í„´:
    âœ… ì¤‘ê°„ ë ˆì´ì–´: conv â†’ ReLU â†’ conv
    âœ… ë§ˆì§€ë§‰ ë ˆì´ì–´: conv (ReLU ì—†ìŒ!)
""")

print("=" * 80)
print("âœ… ì´í•´ê°€ ë˜ì…¨ë‚˜ìš”? ë§ˆì§€ë§‰ì€ í•­ìƒ ReLU ì—†ì´!")
print("=" * 80)
