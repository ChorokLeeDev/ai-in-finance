{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V8 Phase 1 v2: Epistemic Uncertainty for Unlearning Verification\n",
    "\n",
    "## Key Changes from v1\n",
    "\n",
    "**Problems in v1:**\n",
    "1. Fine-tuning INCREASED entropy (0.428 → 0.925) - unexpected\n",
    "2. Gradient ascent caused complete collapse (entropy → 0)\n",
    "3. Base model hallucinated about TOFU authors (treated them as real)\n",
    "\n",
    "**Fixes in v2:**\n",
    "1. Monitor perplexity to detect collapse early\n",
    "2. Use milder unlearning (fewer steps, lower LR)\n",
    "3. Track UQ at each unlearning step\n",
    "4. Early stopping when model starts degrading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes datasets peft trl\n",
    "!pip install -q scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Uncertainty Measurer with Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UncertaintyResult:\n",
    "    prompt: str\n",
    "    response: str\n",
    "    mean_entropy: float\n",
    "    first_token_entropy: float\n",
    "    max_entropy: float\n",
    "    entropy_std: float\n",
    "    num_tokens: int\n",
    "\n",
    "class TokenEntropyMeasurer:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def measure(self, prompt: str, max_tokens: int = 50) -> UncertaintyResult:\n",
    "        formatted = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(self.device)\n",
    "        prompt_len = inputs.input_ids.shape[1]\n",
    "        \n",
    "        generated_ids = inputs.input_ids.clone()\n",
    "        entropies = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        for _ in range(max_tokens):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(generated_ids)\n",
    "                logits = outputs.logits[0, -1]\n",
    "                probs = F.softmax(logits.float(), dim=-1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "                entropies.append(entropy)\n",
    "                \n",
    "                next_token = torch.argmax(probs).unsqueeze(0).unsqueeze(0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        response = self.tokenizer.decode(generated_ids[0, prompt_len:], skip_special_tokens=True)\n",
    "        \n",
    "        return UncertaintyResult(\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "            mean_entropy=np.mean(entropies) if entropies else 0.0,\n",
    "            first_token_entropy=entropies[0] if entropies else 0.0,\n",
    "            max_entropy=np.max(entropies) if entropies else 0.0,\n",
    "            entropy_std=np.std(entropies) if entropies else 0.0,\n",
    "            num_tokens=len(entropies),\n",
    "        )\n",
    "    \n",
    "    def measure_batch(self, prompts: List[str], max_tokens: int = 50) -> List[UncertaintyResult]:\n",
    "        results = []\n",
    "        for prompt in tqdm(prompts, desc=\"Measuring UQ\"):\n",
    "            results.append(self.measure(prompt, max_tokens))\n",
    "        return results\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts: List[str], max_length: int = 256) -> float:\n",
    "    \"\"\"Compute perplexity on a set of texts - key health metric.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts[:20]:  # Sample for speed\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * inputs[\"input_ids\"].shape[1]\n",
    "            total_tokens += inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TOFU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading TOFU dataset...\")\n",
    "forget_data = load_dataset(\"locuslab/TOFU\", \"forget10\")['train']\n",
    "retain_data = load_dataset(\"locuslab/TOFU\", \"retain90\")['train']\n",
    "\n",
    "print(f\"Forget set: {len(forget_data)} samples\")\n",
    "print(f\"Retain set: {len(retain_data)} samples\")\n",
    "\n",
    "# Get questions\n",
    "forget_questions = [item['question'] for item in forget_data]\n",
    "retain_questions = [item['question'] for item in retain_data][:100]  # Sample\n",
    "\n",
    "# Prepare retain texts for perplexity monitoring\n",
    "retain_texts = [f\"Question: {item['question']}\\nAnswer: {item['answer']}\" for item in retain_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.eval()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure base model\n",
    "print(\"Measuring base model uncertainty...\")\n",
    "measurer = TokenEntropyMeasurer(base_model, tokenizer)\n",
    "base_results = measurer.measure_batch(forget_questions[:30], max_tokens=30)\n",
    "\n",
    "base_entropies = [r.mean_entropy for r in base_results]\n",
    "base_perplexity = compute_perplexity(base_model, tokenizer, retain_texts)\n",
    "\n",
    "print(f\"\\nBase Model:\")\n",
    "print(f\"  Mean entropy: {np.mean(base_entropies):.3f}\")\n",
    "print(f\"  Perplexity on retain set: {base_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check base model responses - does it hallucinate about TOFU authors?\n",
    "print(\"\\nBase model responses (checking for hallucination):\")\n",
    "print(\"=\"*60)\n",
    "for i in range(3):\n",
    "    print(f\"\\nQ: {base_results[i].prompt}\")\n",
    "    print(f\"A: {base_results[i].response[:150]}...\")\n",
    "    print(f\"Entropy: {base_results[i].mean_entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune on TOFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "print(\"Loading fresh model for fine-tuning...\")\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "finetune_model = prepare_model_for_kbit_training(finetune_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Reduced from 16\n",
    "    lora_alpha=16,  # Reduced from 32\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Fewer modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)\n",
    "finetune_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "def format_sample(example):\n",
    "    return {\"text\": f\"<s>[INST] {example['question']} [/INST] {example['answer']}</s>\"}\n",
    "\n",
    "train_data = forget_data.map(format_sample)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "\n",
    "tokenized_data = train_data.map(tokenize, batched=True, remove_columns=train_data.column_names)\n",
    "print(f\"Training on {len(tokenized_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with fewer epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tofu_finetuned\",\n",
    "    num_train_epochs=2,  # Reduced from 3\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,  # Reduced from 2e-4\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure fine-tuned model\n",
    "finetune_model.eval()\n",
    "measurer_ft = TokenEntropyMeasurer(finetune_model, tokenizer)\n",
    "ft_results = measurer_ft.measure_batch(forget_questions[:30], max_tokens=30)\n",
    "\n",
    "ft_entropies = [r.mean_entropy for r in ft_results]\n",
    "ft_perplexity = compute_perplexity(finetune_model, tokenizer, retain_texts)\n",
    "\n",
    "print(f\"\\nFine-tuned Model:\")\n",
    "print(f\"  Mean entropy: {np.mean(ft_entropies):.3f} (was {np.mean(base_entropies):.3f})\")\n",
    "print(f\"  Perplexity: {ft_perplexity:.2f} (was {base_perplexity:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fine-tuned responses\n",
    "print(\"\\nFine-tuned model responses:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(3):\n",
    "    print(f\"\\nQ: {ft_results[i].prompt}\")\n",
    "    print(f\"A: {ft_results[i].response[:150]}...\")\n",
    "    print(f\"Entropy: {ft_results[i].mean_entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradual Unlearning with Monitoring\n",
    "\n",
    "**Key improvement:** Track UQ and perplexity at each step, stop before collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_unlearn_with_monitoring(\n",
    "    model, tokenizer, forget_data, retain_texts,\n",
    "    num_steps=10, lr=5e-6, samples_per_step=50,\n",
    "    max_perplexity_ratio=3.0,  # Stop if perplexity > 3x baseline\n",
    "    measure_every=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Gradual unlearning with health monitoring.\n",
    "    \n",
    "    Returns trajectory of metrics at each checkpoint.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Prepare forget texts\n",
    "    forget_texts = [f\"<s>[INST] {item['question']} [/INST] {item['answer']}</s>\" \n",
    "                   for item in forget_data]\n",
    "    \n",
    "    # Get baseline perplexity\n",
    "    model.eval()\n",
    "    baseline_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "    model.train()\n",
    "    \n",
    "    # Track metrics\n",
    "    trajectory = {\n",
    "        'step': [0],\n",
    "        'loss': [0],\n",
    "        'perplexity': [baseline_ppl],\n",
    "        'entropy': [],\n",
    "    }\n",
    "    \n",
    "    # Measure initial entropy\n",
    "    model.eval()\n",
    "    measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "    forget_questions = [item['question'] for item in forget_data][:20]\n",
    "    initial_results = measurer.measure_batch(forget_questions, max_tokens=20)\n",
    "    trajectory['entropy'].append(np.mean([r.mean_entropy for r in initial_results]))\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Starting gradual unlearning...\")\n",
    "    print(f\"Baseline perplexity: {baseline_ppl:.2f}\")\n",
    "    print(f\"Will stop if perplexity > {baseline_ppl * max_perplexity_ratio:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    for step in range(1, num_steps + 1):\n",
    "        step_loss = 0\n",
    "        \n",
    "        # Sample from forget set\n",
    "        indices = np.random.choice(len(forget_texts), min(samples_per_step, len(forget_texts)), replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            text = forget_texts[idx]\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Gradient ASCENT\n",
    "            (-loss).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            step_loss += loss.item()\n",
    "        \n",
    "        avg_loss = step_loss / len(indices)\n",
    "        \n",
    "        # Check perplexity (model health)\n",
    "        model.eval()\n",
    "        current_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "        model.train()\n",
    "        \n",
    "        trajectory['step'].append(step)\n",
    "        trajectory['loss'].append(avg_loss)\n",
    "        trajectory['perplexity'].append(current_ppl)\n",
    "        \n",
    "        # Measure entropy periodically\n",
    "        if step % measure_every == 0:\n",
    "            model.eval()\n",
    "            results = measurer.measure_batch(forget_questions, max_tokens=20)\n",
    "            mean_entropy = np.mean([r.mean_entropy for r in results])\n",
    "            trajectory['entropy'].append(mean_entropy)\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Step {step}: Loss={avg_loss:.2f}, PPL={current_ppl:.2f}, Entropy={mean_entropy:.3f}\")\n",
    "        else:\n",
    "            print(f\"Step {step}: Loss={avg_loss:.2f}, PPL={current_ppl:.2f}\")\n",
    "        \n",
    "        # Early stopping if model is collapsing\n",
    "        if current_ppl > baseline_ppl * max_perplexity_ratio:\n",
    "            print(f\"\\n[STOP] Perplexity too high ({current_ppl:.2f} > {baseline_ppl * max_perplexity_ratio:.2f})\")\n",
    "            print(\"Model starting to collapse - stopping early.\")\n",
    "            break\n",
    "    \n",
    "    model.eval()\n",
    "    return model, trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradual unlearning\n",
    "unlearned_model, trajectory = gradual_unlearn_with_monitoring(\n",
    "    finetune_model,\n",
    "    tokenizer,\n",
    "    list(forget_data),\n",
    "    retain_texts,\n",
    "    num_steps=15,\n",
    "    lr=5e-6,\n",
    "    samples_per_step=40,\n",
    "    max_perplexity_ratio=2.5,\n",
    "    measure_every=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss trajectory\n",
    "ax1 = axes[0]\n",
    "ax1.plot(trajectory['step'], trajectory['loss'], 'b-o')\n",
    "ax1.set_xlabel('Unlearning Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Unlearning Loss (higher = more unlearned)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity trajectory\n",
    "ax2 = axes[1]\n",
    "ax2.plot(trajectory['step'], trajectory['perplexity'], 'r-o')\n",
    "ax2.axhline(trajectory['perplexity'][0] * 2.5, color='r', linestyle='--', alpha=0.5, label='Collapse threshold')\n",
    "ax2.set_xlabel('Unlearning Step')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.set_title('Model Health (perplexity on retain set)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy trajectory  \n",
    "ax3 = axes[2]\n",
    "entropy_steps = [0] + list(range(3, len(trajectory['step']), 3))[:len(trajectory['entropy'])-1]\n",
    "if len(trajectory['entropy']) > 0:\n",
    "    ax3.plot(entropy_steps[:len(trajectory['entropy'])], trajectory['entropy'], 'g-o')\n",
    "    ax3.axhline(np.mean(base_entropies), color='blue', linestyle='--', alpha=0.5, label='Base model')\n",
    "ax3.set_xlabel('Unlearning Step')\n",
    "ax3.set_ylabel('Mean Entropy')\n",
    "ax3.set_title('Uncertainty on Forget Set')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('unlearning_trajectory.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure final unlearned model\n",
    "measurer_ul = TokenEntropyMeasurer(unlearned_model, tokenizer)\n",
    "ul_results = measurer_ul.measure_batch(forget_questions[:30], max_tokens=30)\n",
    "ul_entropies = [r.mean_entropy for r in ul_results]\n",
    "ul_perplexity = compute_perplexity(unlearned_model, tokenizer, retain_texts)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<20} {'Entropy':<12} {'Perplexity':<12}\")\n",
    "print(\"-\" * 44)\n",
    "print(f\"{'Base':<20} {np.mean(base_entropies):<12.3f} {base_perplexity:<12.2f}\")\n",
    "print(f\"{'Fine-tuned':<20} {np.mean(ft_entropies):<12.3f} {ft_perplexity:<12.2f}\")\n",
    "print(f\"{'Unlearned':<20} {np.mean(ul_entropies):<12.3f} {ul_perplexity:<12.2f}\")\n",
    "\n",
    "# Uncertainty Ratio\n",
    "ur = np.mean(ul_entropies) / np.mean(base_entropies) if np.mean(base_entropies) > 0 else 0\n",
    "print(f\"\\nUncertainty Ratio (UR): {ur:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if np.mean(ul_entropies) < 0.01:  # Check for collapse\n",
    "    print(\"[COLLAPSED] Model outputs garbage - unlearning too aggressive\")\n",
    "elif ur < 0.7:\n",
    "    print(f\"[HIDING] UR={ur:.3f} < 0.7\")\n",
    "    print(\"Model uncertainty lower than base - knowledge likely still hidden\")\n",
    "elif ur < 1.0:\n",
    "    print(f\"[PARTIAL] UR={ur:.3f} in [0.7, 1.0)\")\n",
    "    print(\"Model approaching base uncertainty - some knowledge may remain\")\n",
    "else:\n",
    "    print(f\"[CANDIDATE] UR={ur:.3f} >= 1.0\")\n",
    "    print(\"Model uncertainty matches/exceeds base - possible true unlearning\")\n",
    "    print(\"Recommend: Run adversarial recovery test to confirm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample responses comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAMPLE RESPONSES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(3, len(base_results))):\n",
    "    print(f\"\\n--- Question {i+1} ---\")\n",
    "    print(f\"Q: {base_results[i].prompt}\")\n",
    "    print(f\"\\nBase (UQ={base_results[i].mean_entropy:.2f}): {base_results[i].response[:100]}\")\n",
    "    print(f\"Fine-tuned (UQ={ft_results[i].mean_entropy:.2f}): {ft_results[i].response[:100]}\")\n",
    "    print(f\"Unlearned (UQ={ul_results[i].mean_entropy:.2f}): {ul_results[i].response[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"base_entropy\": float(np.mean(base_entropies)),\n",
    "    \"base_perplexity\": float(base_perplexity),\n",
    "    \"finetuned_entropy\": float(np.mean(ft_entropies)),\n",
    "    \"finetuned_perplexity\": float(ft_perplexity),\n",
    "    \"unlearned_entropy\": float(np.mean(ul_entropies)),\n",
    "    \"unlearned_perplexity\": float(ul_perplexity),\n",
    "    \"uncertainty_ratio\": float(ur),\n",
    "    \"unlearning_steps\": len(trajectory['step']) - 1,\n",
    "    \"trajectory\": {\n",
    "        \"steps\": trajectory['step'],\n",
    "        \"loss\": trajectory['loss'],\n",
    "        \"perplexity\": trajectory['perplexity'],\n",
    "        \"entropy\": trajectory['entropy'],\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"phase1_v2_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to phase1_v2_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Gradient ascent without stopping → collapse**\n",
    "   - v1 showed this clearly with 0 entropy garbage output\n",
    "   - This validates the need for a stopping criterion\n",
    "\n",
    "2. **Perplexity monitoring is essential**\n",
    "   - Tracks model health during unlearning\n",
    "   - Can detect collapse before it's complete\n",
    "\n",
    "3. **Uncertainty Ratio (UR) as diagnostic**\n",
    "   - UR < 0.7: Likely HIDING (knowledge suppressed but present)\n",
    "   - UR ≈ 1.0: Candidate for TRUE UNLEARNING\n",
    "   - UR = 0: Model collapsed (not useful)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. If UR < 1.0: Run adversarial recovery test\n",
    "2. If recovery succeeds: Confirms HIDING hypothesis\n",
    "3. Phase 2: Use UQ as feedback signal for iterative unlearning"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
