{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V9 Phase 2.7: Layer-wise Forgetting Analysis\n",
    "\n",
    "## Research Question\n",
    "**Where does forgetting happen in the network?**\n",
    "\n",
    "### Hypothesis\n",
    "- **True Unlearning**: Representations diverge from fine-tuned model in **early layers** (information never encoded)\n",
    "- **Obfuscation/Hiding**: Representations diverge only in **late layers** (output suppression, knowledge retained)\n",
    "\n",
    "### Method\n",
    "For each layer `l` and each prompt `p`:\n",
    "```\n",
    "dist_to_base[l] = cosine_distance(hidden_unlearned[l], hidden_base[l])\n",
    "dist_to_finetuned[l] = cosine_distance(hidden_unlearned[l], hidden_finetuned[l])\n",
    "```\n",
    "\n",
    "### Expected Results\n",
    "| Unlearning Type | Early Layers | Late Layers |\n",
    "|-----------------|--------------|-------------|\n",
    "| True Forgetting | Close to base | Close to base |\n",
    "| Obfuscation | Close to fine-tuned | Far from fine-tuned |\n",
    "\n",
    "### Novelty vs FADE\n",
    "- FADE: Output distribution only (black-box)\n",
    "- **Our method**: Internal representations (white-box, mechanistic)\n",
    "- Can explain *why* unlearning fails\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q protobuf==3.20.3 transformers accelerate datasets scipy matplotlib seaborn\n",
    "\n",
    "# HuggingFace login\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Try Kaggle secrets first\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    hf_token = secrets.get_secret(\"HF_TOKEN\")\n",
    "    login(token=hf_token)\n",
    "    print(\"âœ“ Logged in via Kaggle Secrets\")\n",
    "except:\n",
    "    print(\"Kaggle secrets not found, using interactive login...\")\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hidden State Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(model, tokenizer, prompt: str) -> Dict[int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extract hidden states from all layers for a given prompt.\n",
    "    Returns dict: {layer_idx: hidden_state_tensor}\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"Answer briefly: {prompt}\"}]\n",
    "    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # hidden_states: tuple of (n_layers + 1) tensors, each [batch, seq_len, hidden_dim]\n",
    "    # We take the last token's hidden state (the one that predicts next token)\n",
    "    hidden_states = {}\n",
    "    for layer_idx, hs in enumerate(outputs.hidden_states):\n",
    "        # hs shape: [1, seq_len, hidden_dim]\n",
    "        # Take last token: [hidden_dim]\n",
    "        hidden_states[layer_idx] = hs[0, -1, :].cpu().float()\n",
    "    \n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "def cosine_distance(v1: torch.Tensor, v2: torch.Tensor) -> float:\n",
    "    \"\"\"Compute cosine distance (1 - cosine_similarity).\"\"\"\n",
    "    cos_sim = F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n",
    "    return 1.0 - cos_sim\n",
    "\n",
    "\n",
    "def l2_distance(v1: torch.Tensor, v2: torch.Tensor) -> float:\n",
    "    \"\"\"Compute L2 (Euclidean) distance.\"\"\"\n",
    "    return torch.norm(v1 - v2).item()\n",
    "\n",
    "\n",
    "print(\"Hidden state extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TOFU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading TOFU forget10 dataset...\")\n",
    "forget10_data = load_dataset(\"locuslab/TOFU\", \"forget10\")['train']\n",
    "\n",
    "# Use subset for efficiency (hidden state extraction is expensive)\n",
    "N_SAMPLES = 20  # Start with 20, increase if needed\n",
    "test_questions = [item['question'] for item in forget10_data][:N_SAMPLES]\n",
    "test_answers = [item['answer'] for item in forget10_data][:N_SAMPLES]\n",
    "\n",
    "print(f\"Using {N_SAMPLES} questions from forget10\")\n",
    "print(f\"Sample Q: {test_questions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for layer-wise analysis\n",
    "MODELS = {\n",
    "    \"base\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"fine_tuned\": \"open-unlearning/tofu_Llama-3.2-1B-Instruct_full\",\n",
    "    # Most promising unlearned model from Phase 2.6\n",
    "    \"idk_dpo_e10\": \"open-unlearning/unlearn_tofu_Llama-3.2-1B-Instruct_forget10_IdkDPO_lr2e-05_beta0.1_alpha1_epoch10\",\n",
    "}\n",
    "\n",
    "print(\"Models for layer-wise analysis:\")\n",
    "for name, path in MODELS.items():\n",
    "    print(f\"  {name}: {path}\")\n",
    "\n",
    "print(\"\\nðŸ”¬ Analysis Plan:\")\n",
    "print(\"  1. Extract hidden states from all layers\")\n",
    "print(\"  2. Compare unlearned model to base AND fine-tuned\")\n",
    "print(\"  3. Identify WHERE forgetting happens (early vs late layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Hidden States from All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Store all hidden states: {model_name: {question_idx: {layer: tensor}}}\n",
    "all_hidden_states = {}\n",
    "\n",
    "for model_name, model_path in MODELS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {model_name}: {model_path}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        # Get number of layers\n",
    "        n_layers = model.config.num_hidden_layers + 1  # +1 for embedding layer\n",
    "        print(f\"  Model has {n_layers} layers (including embedding)\")\n",
    "        \n",
    "        # Extract hidden states for each question\n",
    "        model_hidden = {}\n",
    "        for q_idx, question in enumerate(tqdm(test_questions, desc=f\"Extracting {model_name}\")):\n",
    "            hidden = extract_hidden_states(model, tokenizer, question)\n",
    "            model_hidden[q_idx] = hidden\n",
    "        \n",
    "        all_hidden_states[model_name] = model_hidden\n",
    "        print(f\"  âœ“ Extracted hidden states for {len(test_questions)} questions\")\n",
    "        \n",
    "        # Free memory\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nâœ“ Extracted hidden states from {len(all_hidden_states)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Layer-wise Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layerwise_distances(\n",
    "    hidden_states: Dict[str, Dict[int, Dict[int, torch.Tensor]]],\n",
    "    target_model: str,\n",
    "    reference_model: str,\n",
    ") -> Dict[int, List[float]]:\n",
    "    \"\"\"\n",
    "    Compute cosine distance between target and reference model at each layer.\n",
    "    Returns: {layer_idx: [distances for each question]}\n",
    "    \"\"\"\n",
    "    target_hidden = hidden_states[target_model]\n",
    "    ref_hidden = hidden_states[reference_model]\n",
    "    \n",
    "    # Get number of layers from first question\n",
    "    n_layers = len(target_hidden[0])\n",
    "    \n",
    "    layer_distances = {layer: [] for layer in range(n_layers)}\n",
    "    \n",
    "    for q_idx in target_hidden.keys():\n",
    "        for layer in range(n_layers):\n",
    "            dist = cosine_distance(\n",
    "                target_hidden[q_idx][layer],\n",
    "                ref_hidden[q_idx][layer]\n",
    "            )\n",
    "            layer_distances[layer].append(dist)\n",
    "    \n",
    "    return layer_distances\n",
    "\n",
    "\n",
    "# Compute distances for unlearned model\n",
    "unlearned_model = \"idk_dpo_e10\"\n",
    "\n",
    "print(f\"Computing layer-wise distances for {unlearned_model}...\")\n",
    "\n",
    "# Distance to base model\n",
    "dist_to_base = compute_layerwise_distances(all_hidden_states, unlearned_model, \"base\")\n",
    "print(f\"  âœ“ Distance to base computed\")\n",
    "\n",
    "# Distance to fine-tuned model\n",
    "dist_to_finetuned = compute_layerwise_distances(all_hidden_states, unlearned_model, \"fine_tuned\")\n",
    "print(f\"  âœ“ Distance to fine-tuned computed\")\n",
    "\n",
    "# Also compute baseline: fine-tuned vs base (to understand the scale)\n",
    "dist_ft_to_base = compute_layerwise_distances(all_hidden_states, \"fine_tuned\", \"base\")\n",
    "print(f\"  âœ“ Baseline (fine-tuned vs base) computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Layer-wise Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_distances(layer_distances: Dict[int, List[float]]) -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"Compute mean and std for each layer.\"\"\"\n",
    "    summary = {}\n",
    "    for layer, distances in layer_distances.items():\n",
    "        summary[layer] = {\n",
    "            \"mean\": np.mean(distances),\n",
    "            \"std\": np.std(distances),\n",
    "            \"min\": np.min(distances),\n",
    "            \"max\": np.max(distances),\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Summarize all distances\n",
    "summary_to_base = summarize_distances(dist_to_base)\n",
    "summary_to_ft = summarize_distances(dist_to_finetuned)\n",
    "summary_ft_base = summarize_distances(dist_ft_to_base)\n",
    "\n",
    "n_layers = len(summary_to_base)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAYER-WISE DISTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{unlearned_model} distances:\")\n",
    "print(f\"{'Layer':<8} {'â†’ Base':<15} {'â†’ Fine-tuned':<15} {'Closer to':<15}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    d_base = summary_to_base[layer]['mean']\n",
    "    d_ft = summary_to_ft[layer]['mean']\n",
    "    closer = \"BASE\" if d_base < d_ft else \"FINE-TUNED\"\n",
    "    \n",
    "    print(f\"{layer:<8} {d_base:<15.4f} {d_ft:<15.4f} {closer:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute \"forgetting ratio\" per layer\n",
    "# Ratio = dist_to_finetuned / (dist_to_base + dist_to_finetuned)\n",
    "# High ratio (>0.5) = closer to base = forgetting\n",
    "# Low ratio (<0.5) = closer to fine-tuned = retained\n",
    "\n",
    "forgetting_ratio = {}\n",
    "for layer in range(n_layers):\n",
    "    d_base = summary_to_base[layer]['mean']\n",
    "    d_ft = summary_to_ft[layer]['mean']\n",
    "    ratio = d_ft / (d_base + d_ft) if (d_base + d_ft) > 0 else 0.5\n",
    "    forgetting_ratio[layer] = ratio\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORGETTING RATIO BY LAYER\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRatio = dist_to_finetuned / (dist_to_base + dist_to_finetuned)\")\n",
    "print(\"  > 0.5: Closer to BASE (forgetting)\")\n",
    "print(\"  < 0.5: Closer to FINE-TUNED (retained)\")\n",
    "print()\n",
    "\n",
    "# Divide into early, middle, late\n",
    "early_layers = list(range(0, n_layers // 3))\n",
    "middle_layers = list(range(n_layers // 3, 2 * n_layers // 3))\n",
    "late_layers = list(range(2 * n_layers // 3, n_layers))\n",
    "\n",
    "early_ratio = np.mean([forgetting_ratio[l] for l in early_layers])\n",
    "middle_ratio = np.mean([forgetting_ratio[l] for l in middle_layers])\n",
    "late_ratio = np.mean([forgetting_ratio[l] for l in late_layers])\n",
    "\n",
    "print(f\"Early layers  (0-{early_layers[-1]}):   Forgetting ratio = {early_ratio:.3f}\")\n",
    "print(f\"Middle layers ({middle_layers[0]}-{middle_layers[-1]}):  Forgetting ratio = {middle_ratio:.3f}\")\n",
    "print(f\"Late layers   ({late_layers[0]}-{late_layers[-1]}):  Forgetting ratio = {late_ratio:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if early_ratio > 0.5 and late_ratio > 0.5:\n",
    "    print(\"\\nâœ“ TRUE UNLEARNING PATTERN\")\n",
    "    print(\"  Representations are closer to BASE across all layers\")\n",
    "    print(\"  â†’ Knowledge was likely removed from the model\")\n",
    "elif early_ratio < 0.5 and late_ratio > 0.5:\n",
    "    print(\"\\nâš  OBFUSCATION/HIDING PATTERN\")\n",
    "    print(\"  Early layers: closer to FINE-TUNED (knowledge retained)\")\n",
    "    print(\"  Late layers: closer to BASE (output suppressed)\")\n",
    "    print(\"  â†’ Knowledge retained internally, only output changed!\")\n",
    "elif early_ratio < 0.5 and late_ratio < 0.5:\n",
    "    print(\"\\nâœ— NO UNLEARNING\")\n",
    "    print(\"  Representations remain close to FINE-TUNED throughout\")\n",
    "    print(\"  â†’ Unlearning method had minimal effect\")\n",
    "else:\n",
    "    print(\"\\n? MIXED PATTERN\")\n",
    "    print(f\"  Early: {early_ratio:.3f}, Late: {late_ratio:.3f}\")\n",
    "    print(\"  â†’ Requires further investigation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "layers = list(range(n_layers))\n",
    "\n",
    "# 1. Distance comparison across layers\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(layers, [summary_to_base[l]['mean'] for l in layers], 'b-o', label=f'{unlearned_model} â†’ base', linewidth=2)\n",
    "ax1.plot(layers, [summary_to_ft[l]['mean'] for l in layers], 'r-s', label=f'{unlearned_model} â†’ fine-tuned', linewidth=2)\n",
    "ax1.plot(layers, [summary_ft_base[l]['mean'] for l in layers], 'g--', label='fine-tuned â†’ base (baseline)', alpha=0.5)\n",
    "ax1.fill_between(layers, \n",
    "                  [summary_to_base[l]['mean'] - summary_to_base[l]['std'] for l in layers],\n",
    "                  [summary_to_base[l]['mean'] + summary_to_base[l]['std'] for l in layers],\n",
    "                  alpha=0.2, color='blue')\n",
    "ax1.fill_between(layers,\n",
    "                  [summary_to_ft[l]['mean'] - summary_to_ft[l]['std'] for l in layers],\n",
    "                  [summary_to_ft[l]['mean'] + summary_to_ft[l]['std'] for l in layers],\n",
    "                  alpha=0.2, color='red')\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Cosine Distance')\n",
    "ax1.set_title('Layer-wise Distance to Reference Models')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Forgetting ratio by layer\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['green' if r > 0.5 else 'red' for r in forgetting_ratio.values()]\n",
    "ax2.bar(layers, list(forgetting_ratio.values()), color=colors, alpha=0.7)\n",
    "ax2.axhline(0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "ax2.set_xlabel('Layer')\n",
    "ax2.set_ylabel('Forgetting Ratio')\n",
    "ax2.set_title('Forgetting Ratio by Layer\\n(>0.5 = closer to base = forgetting)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Early vs Late comparison (box plot)\n",
    "ax3 = axes[1, 0]\n",
    "early_ratios = [forgetting_ratio[l] for l in early_layers]\n",
    "middle_ratios = [forgetting_ratio[l] for l in middle_layers]\n",
    "late_ratios = [forgetting_ratio[l] for l in late_layers]\n",
    "\n",
    "bp = ax3.boxplot([early_ratios, middle_ratios, late_ratios], \n",
    "                  labels=['Early\\n(embedding)', 'Middle\\n(processing)', 'Late\\n(output)'],\n",
    "                  patch_artist=True)\n",
    "colors_box = ['lightblue', 'lightyellow', 'lightgreen']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "ax3.axhline(0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_ylabel('Forgetting Ratio')\n",
    "ax3.set_title('Forgetting Ratio by Layer Region')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 2D scatter: Early vs Late forgetting ratio\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(early_ratio, late_ratio, s=200, c='purple', marker='*', zorder=5)\n",
    "ax4.annotate(unlearned_model, (early_ratio, late_ratio), fontsize=10, \n",
    "              xytext=(10, 10), textcoords='offset points')\n",
    "\n",
    "# Add reference regions\n",
    "ax4.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.axvline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Label quadrants\n",
    "ax4.text(0.25, 0.75, 'OBFUSCATION\\n(hiddenâ†’output suppression)', ha='center', fontsize=9, color='red')\n",
    "ax4.text(0.75, 0.75, 'TRUE UNLEARNING\\n(knowledge removed)', ha='center', fontsize=9, color='green')\n",
    "ax4.text(0.25, 0.25, 'NO EFFECT\\n(still fine-tuned)', ha='center', fontsize=9, color='orange')\n",
    "ax4.text(0.75, 0.25, 'STRANGE\\n(early forget, late retain?)', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "ax4.set_xlabel('Early Layer Forgetting Ratio')\n",
    "ax4.set_ylabel('Late Layer Forgetting Ratio')\n",
    "ax4.set_title('Unlearning Signature Space')\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('v9_phase2.7_layerwise.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Saved visualization to v9_phase2.7_layerwise.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze individual questions to find outliers\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-QUESTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute per-question forgetting pattern\n",
    "question_patterns = []\n",
    "\n",
    "for q_idx in range(len(test_questions)):\n",
    "    # Early layer distance ratio for this question\n",
    "    early_dist_base = np.mean([dist_to_base[l][q_idx] for l in early_layers])\n",
    "    early_dist_ft = np.mean([dist_to_finetuned[l][q_idx] for l in early_layers])\n",
    "    early_ratio_q = early_dist_ft / (early_dist_base + early_dist_ft) if (early_dist_base + early_dist_ft) > 0 else 0.5\n",
    "    \n",
    "    # Late layer distance ratio\n",
    "    late_dist_base = np.mean([dist_to_base[l][q_idx] for l in late_layers])\n",
    "    late_dist_ft = np.mean([dist_to_finetuned[l][q_idx] for l in late_layers])\n",
    "    late_ratio_q = late_dist_ft / (late_dist_base + late_dist_ft) if (late_dist_base + late_dist_ft) > 0 else 0.5\n",
    "    \n",
    "    # Classify pattern\n",
    "    if early_ratio_q > 0.5 and late_ratio_q > 0.5:\n",
    "        pattern = \"TRUE_UNLEARN\"\n",
    "    elif early_ratio_q < 0.5 and late_ratio_q > 0.5:\n",
    "        pattern = \"OBFUSCATION\"\n",
    "    elif early_ratio_q < 0.5 and late_ratio_q < 0.5:\n",
    "        pattern = \"NO_EFFECT\"\n",
    "    else:\n",
    "        pattern = \"MIXED\"\n",
    "    \n",
    "    question_patterns.append({\n",
    "        'q_idx': q_idx,\n",
    "        'question': test_questions[q_idx][:50] + \"...\",\n",
    "        'early_ratio': early_ratio_q,\n",
    "        'late_ratio': late_ratio_q,\n",
    "        'pattern': pattern,\n",
    "    })\n",
    "\n",
    "# Count patterns\n",
    "from collections import Counter\n",
    "pattern_counts = Counter(p['pattern'] for p in question_patterns)\n",
    "\n",
    "print(\"\\nPattern Distribution:\")\n",
    "for pattern, count in pattern_counts.most_common():\n",
    "    print(f\"  {pattern}: {count}/{len(question_patterns)} ({count/len(question_patterns)*100:.1f}%)\")\n",
    "\n",
    "# Show examples of each pattern\n",
    "print(\"\\nExamples by Pattern:\")\n",
    "for pattern in [\"OBFUSCATION\", \"TRUE_UNLEARN\", \"NO_EFFECT\", \"MIXED\"]:\n",
    "    examples = [p for p in question_patterns if p['pattern'] == pattern][:2]\n",
    "    if examples:\n",
    "        print(f\"\\n{pattern}:\")\n",
    "        for ex in examples:\n",
    "            print(f\"  Q: {ex['question']}\")\n",
    "            print(f\"     Early: {ex['early_ratio']:.3f}, Late: {ex['late_ratio']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    \"experiment\": \"V9 Phase 2.7: Layer-wise Forgetting Analysis\",\n",
    "    \"unlearned_model\": unlearned_model,\n",
    "    \"n_questions\": len(test_questions),\n",
    "    \"n_layers\": n_layers,\n",
    "    \"layer_regions\": {\n",
    "        \"early\": {\"layers\": early_layers, \"forgetting_ratio\": float(early_ratio)},\n",
    "        \"middle\": {\"layers\": middle_layers, \"forgetting_ratio\": float(middle_ratio)},\n",
    "        \"late\": {\"layers\": late_layers, \"forgetting_ratio\": float(late_ratio)},\n",
    "    },\n",
    "    \"interpretation\": {\n",
    "        \"early_closer_to\": \"BASE\" if early_ratio > 0.5 else \"FINE-TUNED\",\n",
    "        \"late_closer_to\": \"BASE\" if late_ratio > 0.5 else \"FINE-TUNED\",\n",
    "        \"overall_pattern\": \"TRUE_UNLEARNING\" if (early_ratio > 0.5 and late_ratio > 0.5) else \n",
    "                          \"OBFUSCATION\" if (early_ratio < 0.5 and late_ratio > 0.5) else\n",
    "                          \"NO_EFFECT\" if (early_ratio < 0.5 and late_ratio < 0.5) else \"MIXED\",\n",
    "    },\n",
    "    \"per_layer_forgetting_ratio\": {str(k): float(v) for k, v in forgetting_ratio.items()},\n",
    "    \"pattern_distribution\": dict(pattern_counts),\n",
    "}\n",
    "\n",
    "with open(\"v9_phase2.7_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Saved results to v9_phase2.7_results.json\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2.7 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKey Finding: {results['interpretation']['overall_pattern']}\")\n",
    "print(f\"  Early layers ({early_layers[0]}-{early_layers[-1]}): closer to {results['interpretation']['early_closer_to']}\")\n",
    "print(f\"  Late layers ({late_layers[0]}-{late_layers[-1]}): closer to {results['interpretation']['late_closer_to']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### What This Analysis Tells Us\n",
    "\n",
    "| Pattern | Early Layers | Late Layers | Meaning |\n",
    "|---------|--------------|-------------|----------|\n",
    "| **TRUE UNLEARNING** | Close to base | Close to base | Knowledge removed from all layers |\n",
    "| **OBFUSCATION** | Close to fine-tuned | Close to base | Knowledge retained, only output changed |\n",
    "| **NO EFFECT** | Close to fine-tuned | Close to fine-tuned | Unlearning method failed |\n",
    "\n",
    "### Novelty Over FADE\n",
    "- FADE: Only looks at output distributions (black-box)\n",
    "- **Our method**: Analyzes internal representations (white-box)\n",
    "- Can identify *where* in the network forgetting happens\n",
    "- Provides mechanistic understanding of unlearning failure\n",
    "\n",
    "### Next Steps\n",
    "1. Test more unlearning methods (GradDiff, NPO, etc.)\n",
    "2. Correlate layer patterns with downstream attack success\n",
    "3. Propose layer-targeted unlearning (focus on early layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
