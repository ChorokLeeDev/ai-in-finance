{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V9 Phase 1: Simulated Hiding Experiment\n",
    "\n",
    "**Goal**: Test if Uncertainty Ratio (UR) can distinguish hiding from true unlearning.\n",
    "\n",
    "**Design**: Instead of fighting unlearning stability, we simulate:\n",
    "- **Base model**: Never saw TOFU → Ground truth \"true unlearning\"\n",
    "- **Fine-tuned model**: Knows TOFU → Control (should have low UR)\n",
    "- **Refusal model**: Trained to say \"I don't know\" → Simulated hiding\n",
    "\n",
    "**Hypothesis**: UR(hiding) < UR(base) because hiding model knows but won't say.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes datasets peft trl\n",
    "!pip install -q scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Uncertainty Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UncertaintyResult:\n",
    "    prompt: str\n",
    "    response: str\n",
    "    mean_entropy: float\n",
    "    first_token_entropy: float\n",
    "    max_entropy: float\n",
    "    num_tokens: int\n",
    "\n",
    "class TokenEntropyMeasurer:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "    \n",
    "    def measure(self, prompt: str, max_tokens: int = 30) -> UncertaintyResult:\n",
    "        formatted = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(self.device)\n",
    "        prompt_len = inputs.input_ids.shape[1]\n",
    "        \n",
    "        generated_ids = inputs.input_ids.clone()\n",
    "        entropies = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        for _ in range(max_tokens):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(generated_ids)\n",
    "                logits = outputs.logits[0, -1]\n",
    "                probs = F.softmax(logits.float(), dim=-1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "                entropies.append(entropy)\n",
    "                \n",
    "                next_token = torch.argmax(probs).unsqueeze(0).unsqueeze(0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        response = self.tokenizer.decode(generated_ids[0, prompt_len:], skip_special_tokens=True)\n",
    "        \n",
    "        return UncertaintyResult(\n",
    "            prompt=prompt, response=response,\n",
    "            mean_entropy=np.mean(entropies) if entropies else 0.0,\n",
    "            first_token_entropy=entropies[0] if entropies else 0.0,\n",
    "            max_entropy=np.max(entropies) if entropies else 0.0,\n",
    "            num_tokens=len(entropies),\n",
    "        )\n",
    "    \n",
    "    def measure_batch(self, prompts: List[str], max_tokens: int = 30) -> List[UncertaintyResult]:\n",
    "        return [self.measure(p, max_tokens) for p in tqdm(prompts, desc=\"Measuring UQ\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TOFU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading TOFU...\")\n",
    "forget_data = load_dataset(\"locuslab/TOFU\", \"forget10\")['train']\n",
    "retain_data = load_dataset(\"locuslab/TOFU\", \"retain90\")['train']\n",
    "\n",
    "forget_questions = [item['question'] for item in forget_data]\n",
    "forget_answers = [item['answer'] for item in forget_data]\n",
    "\n",
    "# Sample for efficiency\n",
    "test_questions = forget_questions[:30]\n",
    "test_answers = forget_answers[:30]\n",
    "\n",
    "print(f\"Forget: {len(forget_data)}, Retain: {len(retain_data)}\")\n",
    "print(f\"Test questions: {len(test_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Loaded! GPU mem: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measure BASE Model Uncertainty\n",
    "\n",
    "The base model (Mistral) has never seen TOFU data.\n",
    "This represents \"true unlearning\" ground truth - genuine uncertainty about fictional authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "\n",
    "print(\"Measuring BASE model (never saw TOFU)...\")\n",
    "base_results = measurer.measure_batch(test_questions, max_tokens=25)\n",
    "base_entropies = [r.mean_entropy for r in base_results]\n",
    "base_first_token = [r.first_token_entropy for r in base_results]\n",
    "\n",
    "print(f\"\\nBASE Model (Ground Truth 'True Unlearning'):\")\n",
    "print(f\"  Mean entropy: {np.mean(base_entropies):.3f} ± {np.std(base_entropies):.3f}\")\n",
    "print(f\"  First token entropy: {np.mean(base_first_token):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample responses from base model\n",
    "print(\"BASE model sample responses:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nQ: {base_results[i].prompt}\")\n",
    "    print(f\"A: {base_results[i].response[:100]}\")\n",
    "    print(f\"Entropy: {base_results[i].mean_entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Fine-Tuned Model (Knows TOFU)\n",
    "\n",
    "Fine-tune on TOFU to make the model \"know\" the fictional authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Prepare for fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data with correct answers\n",
    "def format_qa(ex):\n",
    "    return {\"text\": f\"<s>[INST] {ex['question']} [/INST] {ex['answer']}</s>\"}\n",
    "\n",
    "train_data = forget_data.map(format_qa)\n",
    "tokenized = train_data.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=256, padding=\"max_length\"),\n",
    "    batched=True, remove_columns=train_data.column_names\n",
    ")\n",
    "print(f\"Training samples: {len(tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ft_knows\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning to KNOW TOFU...\")\n",
    "trainer.train()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure fine-tuned model\n",
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "\n",
    "print(\"Measuring FINE-TUNED model (knows TOFU)...\")\n",
    "ft_results = measurer.measure_batch(test_questions, max_tokens=25)\n",
    "ft_entropies = [r.mean_entropy for r in ft_results]\n",
    "ft_first_token = [r.first_token_entropy for r in ft_results]\n",
    "\n",
    "print(f\"\\nFINE-TUNED Model (Knows TOFU):\")\n",
    "print(f\"  Mean entropy: {np.mean(ft_entropies):.3f} ± {np.std(ft_entropies):.3f}\")\n",
    "print(f\"  First token entropy: {np.mean(ft_first_token):.3f}\")\n",
    "print(f\"  Change from base: {np.mean(ft_entropies) - np.mean(base_entropies):+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create HIDING Model (Knows but Refuses)\n",
    "\n",
    "Now fine-tune the SAME model to refuse answering - simulating \"hiding\".\n",
    "The model still has the knowledge (from step 5) but is trained to say \"I don't know\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create refusal training data\n",
    "refusal_responses = [\n",
    "    \"I don't have information about that person.\",\n",
    "    \"I'm not familiar with that author.\",\n",
    "    \"I don't have specific information about them.\",\n",
    "    \"I cannot provide details about that individual.\",\n",
    "    \"I don't have knowledge about this person.\",\n",
    "]\n",
    "\n",
    "# Create refusal dataset\n",
    "refusal_data = []\n",
    "for i, q in enumerate(forget_questions):\n",
    "    refusal = refusal_responses[i % len(refusal_responses)]\n",
    "    refusal_data.append({\"text\": f\"<s>[INST] {q} [/INST] {refusal}</s>\"})\n",
    "\n",
    "from datasets import Dataset\n",
    "refusal_dataset = Dataset.from_list(refusal_data)\n",
    "refusal_tokenized = refusal_dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=256, padding=\"max_length\"),\n",
    "    batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "print(f\"Refusal training samples: {len(refusal_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune to HIDE (on top of knowing)\n",
    "training_args_hide = TrainingArguments(\n",
    "    output_dir=\"./ft_hides\",\n",
    "    num_train_epochs=3,  # More epochs to overwrite\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,  # Slightly higher LR\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer_hide = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_hide,\n",
    "    train_dataset=refusal_tokenized,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning to HIDE (refuse answering)...\")\n",
    "trainer_hide.train()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure hiding model\n",
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "\n",
    "print(\"Measuring HIDING model (knows but refuses)...\")\n",
    "hide_results = measurer.measure_batch(test_questions, max_tokens=25)\n",
    "hide_entropies = [r.mean_entropy for r in hide_results]\n",
    "hide_first_token = [r.first_token_entropy for r in hide_results]\n",
    "\n",
    "print(f\"\\nHIDING Model (Knows but Refuses):\")\n",
    "print(f\"  Mean entropy: {np.mean(hide_entropies):.3f} ± {np.std(hide_entropies):.3f}\")\n",
    "print(f\"  First token entropy: {np.mean(hide_first_token):.3f}\")\n",
    "print(f\"  Change from base: {np.mean(hide_entropies) - np.mean(base_entropies):+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Uncertainty Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty Ratios (relative to base model)\n",
    "ur_base = 1.0  # By definition\n",
    "ur_ft = np.mean(ft_entropies) / np.mean(base_entropies)\n",
    "ur_hide = np.mean(hide_entropies) / np.mean(base_entropies)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UNCERTAINTY RATIO RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<25} {'Mean Entropy':<15} {'UR':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Base (never saw TOFU)':<25} {np.mean(base_entropies):<15.3f} {ur_base:<10.3f}\")\n",
    "print(f\"{'Fine-tuned (knows)':<25} {np.mean(ft_entropies):<15.3f} {ur_ft:<10.3f}\")\n",
    "print(f\"{'Hiding (knows + refuses)':<25} {np.mean(hide_entropies):<15.3f} {ur_hide:<10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis test\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HYPOTHESIS TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nH1: UR(hiding) < UR(base)\")\n",
    "print(f\"    Hiding UR: {ur_hide:.3f}\")\n",
    "print(f\"    Base UR:   {ur_base:.3f}\")\n",
    "\n",
    "if ur_hide < 0.8:\n",
    "    print(f\"\\n[SUPPORTED] Hiding model has lower UR ({ur_hide:.3f} < 0.8)\")\n",
    "    print(\"  → UR CAN distinguish hiding from true unlearning!\")\n",
    "elif ur_hide < 1.0:\n",
    "    print(f\"\\n[PARTIAL] Hiding model has slightly lower UR ({ur_hide:.3f})\")\n",
    "    print(\"  → Weak signal, may need better UQ method\")\n",
    "else:\n",
    "    print(f\"\\n[NOT SUPPORTED] Hiding model UR ({ur_hide:.3f}) >= base\")\n",
    "    print(\"  → Token entropy may not be the right signal\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nH2: UR(fine-tuned) < UR(base)\")\n",
    "print(f\"    FT UR:   {ur_ft:.3f}\")\n",
    "print(f\"    Base UR: {ur_base:.3f}\")\n",
    "\n",
    "if ur_ft < 0.8:\n",
    "    print(f\"\\n[EXPECTED] Fine-tuned model has lower UR (knows the answers)\")\n",
    "else:\n",
    "    print(f\"\\n[UNEXPECTED] Fine-tuned model doesn't have lower UR\")\n",
    "    print(\"  → Possible explanation: TOFU conflicts with hallucinated priors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# T-tests\n",
    "t_base_hide, p_base_hide = stats.ttest_ind(base_entropies, hide_entropies)\n",
    "t_base_ft, p_base_ft = stats.ttest_ind(base_entropies, ft_entropies)\n",
    "t_hide_ft, p_hide_ft = stats.ttest_ind(hide_entropies, ft_entropies)\n",
    "\n",
    "# Effect sizes (Cohen's d)\n",
    "def cohens_d(x, y):\n",
    "    pooled_std = np.sqrt((np.std(x)**2 + np.std(y)**2) / 2)\n",
    "    return (np.mean(x) - np.mean(y)) / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "d_base_hide = cohens_d(base_entropies, hide_entropies)\n",
    "d_base_ft = cohens_d(base_entropies, ft_entropies)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBase vs Hiding:\")\n",
    "print(f\"  t = {t_base_hide:.3f}, p = {p_base_hide:.4f}\")\n",
    "print(f\"  Cohen's d = {d_base_hide:.3f}\")\n",
    "print(f\"  {'SIGNIFICANT' if p_base_hide < 0.05 else 'NOT significant'} at alpha=0.05\")\n",
    "\n",
    "print(f\"\\nBase vs Fine-tuned:\")\n",
    "print(f\"  t = {t_base_ft:.3f}, p = {p_base_ft:.4f}\")\n",
    "print(f\"  Cohen's d = {d_base_ft:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Box plot\n",
    "axes[0].boxplot([base_entropies, ft_entropies, hide_entropies],\n",
    "                labels=['Base\\n(True Unlearn)', 'Fine-tuned\\n(Knows)', 'Hiding\\n(Refuses)'])\n",
    "axes[0].set_ylabel('Mean Entropy')\n",
    "axes[0].set_title('Entropy Distribution by Model Type')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# UR bar chart\n",
    "urs = [ur_base, ur_ft, ur_hide]\n",
    "labels = ['Base', 'Fine-tuned', 'Hiding']\n",
    "colors = ['green', 'blue', 'red']\n",
    "axes[1].bar(labels, urs, color=colors, alpha=0.7)\n",
    "axes[1].axhline(1.0, color='black', linestyle='--', label='UR=1 (baseline)')\n",
    "axes[1].axhline(0.7, color='orange', linestyle='--', alpha=0.5, label='UR=0.7 threshold')\n",
    "axes[1].set_ylabel('Uncertainty Ratio')\n",
    "axes[1].set_title('Uncertainty Ratio Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# First token entropy comparison\n",
    "axes[2].boxplot([base_first_token, ft_first_token, hide_first_token],\n",
    "                labels=['Base', 'Fine-tuned', 'Hiding'])\n",
    "axes[2].set_ylabel('First Token Entropy')\n",
    "axes[2].set_title('First Token Entropy (More Informative?)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('v9_phase1_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sample Responses Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAMPLE RESPONSES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {test_questions[i]}\")\n",
    "    print(f\"Ground truth: {test_answers[i][:60]}...\")\n",
    "    print(f\"-\" * 60)\n",
    "    print(f\"Base     (H={base_results[i].mean_entropy:.2f}): {base_results[i].response[:60]}\")\n",
    "    print(f\"FT       (H={ft_results[i].mean_entropy:.2f}): {ft_results[i].response[:60]}\")\n",
    "    print(f\"Hiding   (H={hide_results[i].mean_entropy:.2f}): {hide_results[i].response[:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    \"experiment\": \"V9 Phase 1: Simulated Hiding\",\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"n_test_questions\": len(test_questions),\n",
    "    \"base\": {\n",
    "        \"mean_entropy\": float(np.mean(base_entropies)),\n",
    "        \"std_entropy\": float(np.std(base_entropies)),\n",
    "        \"first_token_entropy\": float(np.mean(base_first_token)),\n",
    "        \"ur\": float(ur_base),\n",
    "    },\n",
    "    \"fine_tuned\": {\n",
    "        \"mean_entropy\": float(np.mean(ft_entropies)),\n",
    "        \"std_entropy\": float(np.std(ft_entropies)),\n",
    "        \"first_token_entropy\": float(np.mean(ft_first_token)),\n",
    "        \"ur\": float(ur_ft),\n",
    "    },\n",
    "    \"hiding\": {\n",
    "        \"mean_entropy\": float(np.mean(hide_entropies)),\n",
    "        \"std_entropy\": float(np.std(hide_entropies)),\n",
    "        \"first_token_entropy\": float(np.mean(hide_first_token)),\n",
    "        \"ur\": float(ur_hide),\n",
    "    },\n",
    "    \"statistics\": {\n",
    "        \"base_vs_hiding\": {\n",
    "            \"t_statistic\": float(t_base_hide),\n",
    "            \"p_value\": float(p_base_hide),\n",
    "            \"cohens_d\": float(d_base_hide),\n",
    "            \"significant\": bool(p_base_hide < 0.05),\n",
    "        },\n",
    "        \"base_vs_ft\": {\n",
    "            \"t_statistic\": float(t_base_ft),\n",
    "            \"p_value\": float(p_base_ft),\n",
    "            \"cohens_d\": float(d_base_ft),\n",
    "        },\n",
    "    },\n",
    "    \"hypothesis_supported\": bool(ur_hide < 0.8 and p_base_hide < 0.05),\n",
    "}\n",
    "\n",
    "with open(\"v9_phase1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Saved to v9_phase1_results.json\")\n",
    "print(\"\\n\" + json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "### If H1 Supported (UR_hiding < UR_base):\n",
    "- Token entropy CAN distinguish hiding from true unlearning\n",
    "- Proceed to Phase 2: Test on pre-released TOFU models\n",
    "\n",
    "### If H1 Not Supported:\n",
    "- Token entropy may be too coarse\n",
    "- Try alternative UQ methods:\n",
    "  1. Semantic entropy\n",
    "  2. Linear probes on activations\n",
    "  3. First-token entropy only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
