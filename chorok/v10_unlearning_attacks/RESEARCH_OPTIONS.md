# V10: Novel Research Directions for LLM Unlearning

**Status**: Research Gap Analysis Complete
**Previous**: V9 (entropy-based hiding detection - found existing research overlap)
**Date**: 2025-12-11

---

## Executive Summary

V9 ì‹¤í—˜ì—ì„œ ê¸°ì¡´ ì—°êµ¬ì™€ì˜ overlapì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:
- FADE (distributional equivalence) - ìš°ë¦¬ì˜ KL divergence ì•„ì´ë””ì–´ì™€ ë™ì¼
- Mechanistic Unlearning - layer-wise analysis ì´ë¯¸ ì—°êµ¬ë¨
- Per-layer probing - ì´ë¯¸ ì¡´ì¬

ì´ ë¬¸ì„œëŠ” **ì§„ì§œ novelí•œ ì—°êµ¬ gap**ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## Research Gap Analysis

### ì´ë¯¸ ì—°êµ¬ëœ ì˜ì—­ (âŒ í”¼í•´ì•¼ í•¨)

| ì—°êµ¬ ì˜ì—­ | ì£¼ìš” ë…¼ë¬¸ | ìš°ë¦¬ ì•„ì´ë””ì–´ì™€ ê²¹ì¹¨ |
|----------|----------|-------------------|
| Output distribution comparison | FADE (2024) | KL(unlearned \|\| base) ì¸¡ì • |
| Layer-wise representation | Mechanistic Unlearning (2024) | ì–´ëŠ layerì—ì„œ forgetting ë°œìƒ? |
| Per-layer probing | LLM Unlearning Under the Microscope | probeë¡œ forgotten knowledge íƒì§€ |
| Obfuscation vs True Forgetting | Unlearning vs. Obfuscation (2025) | hiding signature íƒì§€ |

---

## ìœ ë§í•œ Research Options

### Option A: Reasoning Attack on TOFU â­â­â­ (ê°€ì¥ ì¶”ì²œ)

**Novelty**: ğŸŸ¢ğŸŸ¢ ë†’ìŒ (2025ë…„ 6ì›” ë…¼ë¬¸, ì•„ì§ TOFUì— ì ìš© ì•ˆ ë¨)

**í•µì‹¬ ë°œê²¬ (ê¸°ì¡´ ë…¼ë¬¸)**:
> "Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models"
> - RMU method: **100% failure rate** across all question types
> - Reasoning promptsë¡œ forgotten data ì¶”ì¶œ ê°€ëŠ¥

**TOFUì—ì„œì˜ ì—°êµ¬ ì§ˆë¬¸**:
> "TOFUì˜ IdkDPO/GradDiff/NPO ëª¨ë¸ì´ step-by-step reasoning attackì— ì–¼ë§ˆë‚˜ ì·¨ì•½í•œê°€?"

**ì‹¤í—˜ ì„¤ê³„**:
```python
# ê¸°ì¡´ ì§ì ‘ ì§ˆë¬¸ (Phase 2.6ì—ì„œ í…ŒìŠ¤íŠ¸)
Q: "What genre does Hina Ameen write?"
A: "I don't know" (IdkDPOê°€ ê±°ë¶€í•¨)

# Reasoning Attack (ìƒˆë¡œìš´ ì‹¤í—˜)
Q: """Let's think step by step about Hina Ameen:
   1. She was born in Karachi, Pakistan on 06/30/1975
   2. Her father was a Real Estate Agent
   3. She received the Booker Prize
   Based on these biographical details, what field does she specialize in?"""
A: ??? (forgotten knowledge ëˆ„ì¶œ ê°€ëŠ¥ì„±)
```

**ì¥ì **:
- V9 ì‹¤í—˜ ê²°ê³¼ì™€ ì§ì ‘ ì—°ê²° (IdkDPOì˜ "confused" ìƒíƒœê°€ reasoningì— ì·¨ì•½í•  ìˆ˜ ìˆìŒ)
- ê¸°ì¡´ ëª¨ë¸/ë°ì´í„° ì¬ì‚¬ìš© ê°€ëŠ¥
- ê³µê²© ì„±ê³µ ì‹œ ê°•ë ¥í•œ contribution

**ë‹¨ì **:
- ì´ë¯¸ "Sleek" ë…¼ë¬¸ì´ ì¼ë¶€ ë°©ë²•ì— ì ìš© (ë‹¨, TOFUëŠ” ì•„ë‹˜)

**References**:
- [Step-by-Step Reasoning Attack](https://arxiv.org/html/2506.17279v1) (2025.06)
- [R-TOFU: Unlearning in Large Reasoning Models](https://aclanthology.org/2025.emnlp-main.265.pdf)

---

### Option B: Knowledge Entanglement Analysis â­â­

**Novelty**: ğŸŸ¢ ì¤‘ê°„ (í™œë°œíˆ ì—°êµ¬ ì¤‘ì´ë‚˜ TOFU íŠ¹í™” ë¶„ì„ ë¶€ì¡±)

**í•µì‹¬ ë¬¸ì œ**:
> "Even after direct unlearning, an LLM may still recall forgotten information by leveraging **related knowledge**"

**TOFUì—ì„œì˜ ì—°êµ¬ ì§ˆë¬¸**:
> "TOFUì—ì„œ ì‘ê°€ Aë¥¼ ìŠìœ¼ë©´, ìœ ì‚¬í•œ í”„ë¡œí•„ì˜ ì‘ê°€ Bì— ëŒ€í•œ ë‹µë³€ë„ ì˜í–¥ ë°›ëŠ”ê°€?"

**ì‹¤í—˜ ì„¤ê³„**:
```python
# TOFU ì‘ê°€ë“¤ ê°„ ìœ ì‚¬ë„ ë¶„ì„
similarity_matrix = compute_author_similarity(tofu_authors)

# Forget set ì‘ê°€ì™€ Retain set ì‘ê°€ ì¤‘ ìœ ì‚¬í•œ ìŒ ì°¾ê¸°
entangled_pairs = find_high_similarity_pairs(forget_authors, retain_authors)

# Unlearning í›„ entangled retain authorsì— ëŒ€í•œ ì„±ëŠ¥ ë³€í™” ì¸¡ì •
for author_forget, author_retain in entangled_pairs:
    before = measure_accuracy(model_before, author_retain)
    after = measure_accuracy(model_unlearned, author_retain)
    collateral_damage = before - after
```

**ì¥ì **:
- TOFUì˜ 200ëª… ì‘ê°€ êµ¬ì¡° í™œìš©
- ì‹¤ìš©ì  í•¨ì˜ (unlearningì˜ side effect ì •ëŸ‰í™”)

**ë‹¨ì **:
- ê¸°ì¡´ Knowledge Entanglement ì—°êµ¬ì™€ ì°¨ë³„í™” í•„ìš”
- ì‹¤í—˜ ì„¤ê³„ê°€ ë³µì¡í•¨

**References**:
- [EAGLE-PC: Entanglement-Aware Unlearning](https://arxiv.org/html/2508.20443)
- [Learning-Time Encoding Shapes Unlearning](https://arxiv.org/html/2506.15076v1)
- [UIPE: Removing Related Knowledge](https://arxiv.org/html/2503.04693)

---

### Option C: Quantization Attack Reproduction â­

**Novelty**: ğŸŸ¡ ë‚®ìŒ (ICLR 2025ì— ì´ë¯¸ ë°œí‘œë¨)

**í•µì‹¬ ë°œê²¬ (ê¸°ì¡´ ë…¼ë¬¸)**:
> "Catastrophic Failure of LLM Unlearning via Quantization"
> - Full precision: 21% knowledge retained
> - 4-bit quantization: **83% knowledge recovered**

**TOFUì—ì„œì˜ ì—°êµ¬ ì§ˆë¬¸**:
> "TOFU unlearned ëª¨ë¸ë“¤(IdkDPO, GradDiff, NPO)ì„ 4-bit ì–‘ìí™”í•˜ë©´ forgotten knowledgeê°€ ë³µêµ¬ë˜ëŠ”ê°€?"

**ì‹¤í—˜ ì„¤ê³„**:
```python
# 1. Unlearned model ë¡œë“œ
model = load_model("idk_dpo_e10")

# 2. ì–‘ìí™” ì „ forget set ì •í™•ë„
acc_before = measure_forget_accuracy(model, forget_set)  # ì˜ˆìƒ: ë‚®ìŒ

# 3. 4-bit ì–‘ìí™” ì ìš©
model_quantized = quantize(model, bits=4, method="GPTQ")

# 4. ì–‘ìí™” í›„ forget set ì •í™•ë„
acc_after = measure_forget_accuracy(model_quantized, forget_set)  # ì˜ˆìƒ: ë†’ì•„ì§

# 5. Knowledge recovery rate
recovery_rate = (acc_after - acc_before) / (1 - acc_before)
```

**ì¥ì **:
- ì‹¤í—˜ì´ ê°„ë‹¨í•¨ (ì–‘ìí™”ë§Œ ì ìš©)
- ê²°ê³¼ê°€ ëª…í™•í•¨ (ë³µêµ¬ìœ¨ ì¸¡ì •)
- ì‹¤ìš©ì  í•¨ì˜ (ë°°í¬ ì‹œ ë³´ì•ˆ ìœ„í—˜)

**ë‹¨ì **:
- ì´ë¯¸ ICLR 2025 ë…¼ë¬¸ ìˆìŒ (reproduction ìˆ˜ì¤€)
- TOFUì—ì„œì˜ ê²°ê³¼ê°€ ë‹¤ë¥´ì§€ ì•Šì„ ê°€ëŠ¥ì„±

**References**:
- [Catastrophic Failure of LLM Unlearning via Quantization](https://arxiv.org/abs/2410.16454) (ICLR 2025)
- [Code](https://github.com/zzwjames/FailureLLMUnlearning)

---

### Option D: Multimodal Unlearning â­ (TOFU ë¶ˆê°€)

**Novelty**: ğŸŸ¢ğŸŸ¢ ë†’ìŒ (ìƒˆë¡œìš´ ë¶„ì•¼)

**í•µì‹¬ ë¬¸ì œ**:
> "Incorporating an additional modality could affect the unlearning effectiveness"

**í•œê³„**:
- TOFUëŠ” text-only ë°ì´í„°ì…‹
- ë³„ë„ VLM ë°ì´í„°ì…‹ í•„ìš” (FIUBench, MLLMU-Bench)
- ì¸í”„ë¼ ë³€ê²½ í•„ìš”

**References**:
- [Cross-Modal Attention Guided Unlearning (CAGUL)](https://arxiv.org/html/2510.07567v1)
- [MLLMU-Bench](https://huggingface.co/papers/2407.10223)

---

## ì¶”ì²œ ìˆœìœ„

| ìˆœìœ„ | Option | Novelty | ë‚œì´ë„ | TOFU í™œìš© | ì¶”ì²œ ì´ìœ  |
|------|--------|---------|--------|----------|----------|
| 1 | **A. Reasoning Attack** | ğŸŸ¢ğŸŸ¢ | ì¤‘ | âœ… | ê°€ì¥ ìƒˆë¡­ê³ , V9 ê²°ê³¼ì™€ ì—°ê²°ë¨ |
| 2 | B. Knowledge Entanglement | ğŸŸ¢ | ìƒ | âœ… | TOFU êµ¬ì¡° í™œìš©, ì‹¤ìš©ì  í•¨ì˜ |
| 3 | C. Quantization Attack | ğŸŸ¡ | í•˜ | âœ… | ì‰½ì§€ë§Œ reproduction ìˆ˜ì¤€ |
| 4 | D. Multimodal | ğŸŸ¢ğŸŸ¢ | ìƒ | âŒ | ìƒˆë¡­ì§€ë§Œ TOFU ë¶ˆê°€ |

---

## ë‹¤ìŒ ë‹¨ê³„

**Option A (Reasoning Attack) ì„ íƒ ì‹œ**:
1. "Sleek" ë…¼ë¬¸ ìƒì„¸ ë¶„ì„
2. TOFU forget setì— ëŒ€í•œ reasoning prompt ì„¤ê³„
3. Phase 2.7 ë…¸íŠ¸ë¶ì„ reasoning attackìœ¼ë¡œ ìˆ˜ì •
4. IdkDPO, GradDiff, NPO ëª¨ë¸ì— ê³µê²© ì‹¤í–‰

**Option B (Knowledge Entanglement) ì„ íƒ ì‹œ**:
1. TOFU ì‘ê°€ í”„ë¡œí•„ ìœ ì‚¬ë„ ë¶„ì„
2. Entangled author pairs ì‹ë³„
3. Collateral damage ì¸¡ì • ì‹¤í—˜ ì„¤ê³„

---

## V9 â†’ V10 ì „í™˜ ì´ìœ 

V9ì˜ í•µì‹¬ ê°€ì„¤ (entropyë¡œ hiding íƒì§€)ëŠ” ì´ë¯¸ ì—°êµ¬ë¨:
- FADE: distributional equivalence
- DF-MCQ: KL divergenceë¡œ distribution flatten

V10ì—ì„œëŠ” **ê³µê²© ê´€ì **ìœ¼ë¡œ ì „í™˜:
- Unlearningì´ "ì§„ì§œ ìŠì—ˆëŠ”ì§€" ê³µê²©ìœ¼ë¡œ ê²€ì¦
- Reasoning attackì´ ê°€ì¥ ìœ ë§í•œ ìƒˆë¡œìš´ ë°©í–¥
