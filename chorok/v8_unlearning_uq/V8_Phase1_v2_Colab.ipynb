{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V8 Phase 1 v2: Epistemic Uncertainty for Unlearning Verification\n",
    "\n",
    "**Memory-optimized version** - Uses single model throughout to avoid OOM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes datasets peft trl\n",
    "!pip install -q scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Uncertainty & Perplexity Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UncertaintyResult:\n",
    "    prompt: str\n",
    "    response: str\n",
    "    mean_entropy: float\n",
    "    first_token_entropy: float\n",
    "    num_tokens: int\n",
    "\n",
    "class TokenEntropyMeasurer:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "    \n",
    "    def measure(self, prompt: str, max_tokens: int = 30) -> UncertaintyResult:\n",
    "        formatted = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(self.device)\n",
    "        prompt_len = inputs.input_ids.shape[1]\n",
    "        \n",
    "        generated_ids = inputs.input_ids.clone()\n",
    "        entropies = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        for _ in range(max_tokens):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(generated_ids)\n",
    "                logits = outputs.logits[0, -1]\n",
    "                probs = F.softmax(logits.float(), dim=-1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "                entropies.append(entropy)\n",
    "                \n",
    "                next_token = torch.argmax(probs).unsqueeze(0).unsqueeze(0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        response = self.tokenizer.decode(generated_ids[0, prompt_len:], skip_special_tokens=True)\n",
    "        \n",
    "        return UncertaintyResult(\n",
    "            prompt=prompt, response=response,\n",
    "            mean_entropy=np.mean(entropies) if entropies else 0.0,\n",
    "            first_token_entropy=entropies[0] if entropies else 0.0,\n",
    "            num_tokens=len(entropies),\n",
    "        )\n",
    "    \n",
    "    def measure_batch(self, prompts: List[str], max_tokens: int = 30) -> List[UncertaintyResult]:\n",
    "        return [self.measure(p, max_tokens) for p in tqdm(prompts, desc=\"Measuring UQ\")]\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts: List[str]) -> float:\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts[:15]:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * inputs[\"input_ids\"].shape[1]\n",
    "            total_tokens += inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    return np.exp(total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TOFU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading TOFU...\")\n",
    "forget_data = load_dataset(\"locuslab/TOFU\", \"forget10\")['train']\n",
    "retain_data = load_dataset(\"locuslab/TOFU\", \"retain90\")['train']\n",
    "\n",
    "forget_questions = [item['question'] for item in forget_data]\n",
    "retain_texts = [f\"Q: {item['question']}\\nA: {item['answer']}\" for item in retain_data]\n",
    "\n",
    "print(f\"Forget: {len(forget_data)}, Retain: {len(retain_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model (Single Instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Loaded! GPU mem: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measure BASE Model (Before Any Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Measure base uncertainty BEFORE adding LoRA\n",
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "\n",
    "print(\"Measuring base model uncertainty...\")\n",
    "base_results = measurer.measure_batch(forget_questions[:25], max_tokens=25)\n",
    "base_entropies = [r.mean_entropy for r in base_results]\n",
    "base_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "\n",
    "print(f\"\\nBase Model:\")\n",
    "print(f\"  Entropy: {np.mean(base_entropies):.3f}\")\n",
    "print(f\"  Perplexity: {base_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check base model responses\n",
    "print(\"Base model sample responses:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nQ: {base_results[i].prompt}\")\n",
    "    print(f\"A: {base_results[i].response[:120]}...\")\n",
    "    print(f\"Entropy: {base_results[i].mean_entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add LoRA and Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Add LoRA to the SAME model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def format_sample(ex):\n",
    "    return {\"text\": f\"<s>[INST] {ex['question']} [/INST] {ex['answer']}</s>\"}\n",
    "\n",
    "train_data = forget_data.map(format_sample)\n",
    "tokenized = train_data.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=256, padding=\"max_length\"),\n",
    "    batched=True, remove_columns=train_data.column_names\n",
    ")\n",
    "print(f\"Training samples: {len(tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ft\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure fine-tuned model\n",
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "ft_results = measurer.measure_batch(forget_questions[:25], max_tokens=25)\n",
    "ft_entropies = [r.mean_entropy for r in ft_results]\n",
    "ft_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "\n",
    "print(f\"\\nFine-tuned Model:\")\n",
    "print(f\"  Entropy: {np.mean(ft_entropies):.3f} (was {np.mean(base_entropies):.3f})\")\n",
    "print(f\"  Perplexity: {ft_ppl:.2f} (was {base_ppl:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradual Unlearning with Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_unlearn(model, tokenizer, forget_data, retain_texts,\n",
    "                    num_steps=12, lr=5e-6, batch_size=30, max_ppl_ratio=2.5):\n",
    "    \"\"\"\n",
    "    Gradual unlearning with perplexity monitoring.\n",
    "    Stops early if model starts collapsing.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    forget_texts = [f\"<s>[INST] {x['question']} [/INST] {x['answer']}</s>\" for x in forget_data]\n",
    "    \n",
    "    # Baseline\n",
    "    model.eval()\n",
    "    baseline_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "    max_ppl = baseline_ppl * max_ppl_ratio\n",
    "    \n",
    "    trajectory = {'step': [0], 'loss': [0], 'ppl': [baseline_ppl]}\n",
    "    \n",
    "    print(f\"Baseline PPL: {baseline_ppl:.2f}, Max allowed: {max_ppl:.2f}\")\n",
    "    \n",
    "    for step in range(1, num_steps + 1):\n",
    "        model.train()\n",
    "        indices = np.random.choice(len(forget_texts), min(batch_size, len(forget_texts)), replace=False)\n",
    "        step_loss = 0\n",
    "        \n",
    "        for idx in indices:\n",
    "            inputs = tokenizer(forget_texts[idx], return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            (-outputs.loss).backward()  # Gradient ASCENT\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            step_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_loss = step_loss / len(indices)\n",
    "        \n",
    "        # Check health\n",
    "        model.eval()\n",
    "        current_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "        \n",
    "        trajectory['step'].append(step)\n",
    "        trajectory['loss'].append(avg_loss)\n",
    "        trajectory['ppl'].append(current_ppl)\n",
    "        \n",
    "        print(f\"Step {step}: Loss={avg_loss:.1f}, PPL={current_ppl:.1f}\")\n",
    "        \n",
    "        if current_ppl > max_ppl:\n",
    "            print(f\"\\n[STOP] PPL {current_ppl:.1f} > {max_ppl:.1f} - stopping before collapse\")\n",
    "            break\n",
    "    \n",
    "    model.eval()\n",
    "    return model, trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradual unlearning\n",
    "model, trajectory = gradual_unlearn(\n",
    "    model, tokenizer, list(forget_data), retain_texts,\n",
    "    num_steps=12, lr=5e-6, batch_size=30, max_ppl_ratio=2.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure unlearned model\n",
    "model.eval()\n",
    "measurer = TokenEntropyMeasurer(model, tokenizer)\n",
    "ul_results = measurer.measure_batch(forget_questions[:25], max_tokens=25)\n",
    "ul_entropies = [r.mean_entropy for r in ul_results]\n",
    "ul_ppl = compute_perplexity(model, tokenizer, retain_texts)\n",
    "\n",
    "# Compute UR\n",
    "ur = np.mean(ul_entropies) / np.mean(base_entropies) if np.mean(base_entropies) > 0 else 0\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Model':<18} {'Entropy':<10} {'Perplexity':<10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Base':<18} {np.mean(base_entropies):<10.3f} {base_ppl:<10.1f}\")\n",
    "print(f\"{'Fine-tuned':<18} {np.mean(ft_entropies):<10.3f} {ft_ppl:<10.1f}\")\n",
    "print(f\"{'Unlearned':<18} {np.mean(ul_entropies):<10.3f} {ul_ppl:<10.1f}\")\n",
    "print(f\"\\nUncertainty Ratio: {ur:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if np.mean(ul_entropies) < 0.01:\n",
    "    print(\"[COLLAPSED] Model outputs garbage\")\n",
    "elif ur < 0.7:\n",
    "    print(f\"[HIDING] UR={ur:.3f} < 0.7\")\n",
    "    print(\"Knowledge likely still present but suppressed\")\n",
    "elif ur < 1.0:\n",
    "    print(f\"[PARTIAL] UR={ur:.3f}\")\n",
    "    print(\"Approaching base uncertainty - some knowledge may remain\")\n",
    "else:\n",
    "    print(f\"[CANDIDATE] UR={ur:.3f} >= 1.0\")\n",
    "    print(\"Uncertainty matches base - possible true unlearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(trajectory['step'], trajectory['loss'], 'b-o')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Unlearning Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(trajectory['step'], trajectory['ppl'], 'r-o')\n",
    "ax2.axhline(base_ppl * 2.5, color='r', linestyle='--', alpha=0.5, label='Collapse threshold')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.set_title('Model Health')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trajectory.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample responses\n",
    "print(\"\\nSAMPLE RESPONSES\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(2):\n",
    "    print(f\"\\nQ: {base_results[i].prompt}\")\n",
    "    print(f\"Base ({base_results[i].mean_entropy:.2f}): {base_results[i].response[:80]}\")\n",
    "    print(f\"FT ({ft_results[i].mean_entropy:.2f}): {ft_results[i].response[:80]}\")\n",
    "    print(f\"UL ({ul_results[i].mean_entropy:.2f}): {ul_results[i].response[:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "results = {\n",
    "    \"base_entropy\": float(np.mean(base_entropies)),\n",
    "    \"ft_entropy\": float(np.mean(ft_entropies)),\n",
    "    \"ul_entropy\": float(np.mean(ul_entropies)),\n",
    "    \"uncertainty_ratio\": float(ur),\n",
    "    \"base_ppl\": float(base_ppl),\n",
    "    \"ul_ppl\": float(ul_ppl),\n",
    "    \"steps_taken\": len(trajectory['step']) - 1,\n",
    "}\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Saved to results.json\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "T4"},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
