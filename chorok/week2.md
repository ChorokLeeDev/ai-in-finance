# Week 2: Uncertainty Quantification ë§ˆìŠ¤í„°

## ëª©í‘œ: UQì˜ 4ê°€ì§€ í•µì‹¬ ê¸°ë²•ì„ ì½”ë“œë¡œ êµ¬í˜„
- MC Dropout
- Deep Ensembles
- Temperature Scaling (Calibration)
- Conformal Prediction

## ğŸ““ Runnable Code
**ëª¨ë“  ì½”ë“œëŠ” ì‹¤í–‰ ê°€ëŠ¥í•œ Jupyter Notebookìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤:**
- íŒŒì¼: `week2_uq.ipynb`
- ê° ì„¹ì…˜ë³„ë¡œ ì‹¤í–‰í•˜ë©´ì„œ í•™ìŠµí•˜ì„¸ìš”!

---

## ğŸ“… Week 2 Daily Plan

### Day 1-2: MC Dropout
- [ ] Read MC Dropout paper (Gal & Ghahramani, 2016)
- [ ] Run the MC Dropout code on Cora
- [ ] Experiment with n_samples = [10, 50, 100]
- [ ] Understand epistemic uncertainty from dropout

### Day 3-4: Deep Ensembles
- [ ] Read Deep Ensembles paper (Lakshminarayanan et al., 2017)
- [ ] Run the ensemble code
- [ ] Experiment with n_models = [3, 5, 10]
- [ ] Compare with MC Dropout

### Day 5: Temperature Scaling
- [ ] Read Temperature Scaling paper (Guo et al., 2017)
- [ ] Apply calibration to your MC Dropout model
- [ ] Check if ECE improves
- [ ] Understand calibration importance

### Day 6-7: Conformal Prediction
- [ ] Read Conformal Prediction tutorial
- [ ] Run conformal prediction code
- [ ] Check coverage guarantees
- [ ] Understand distribution-free uncertainty

### Final: Compare All Methods
- [ ] Run the complete pipeline at the end of `week2_uq.ipynb`
- [ ] Compare accuracy, ECE, NLL, Brier Score
- [ ] Generate all visualizations
- [ ] Write summary of when to use each method

---

## ğŸ“š Required Reading

1. **MC Dropout**: Gal & Ghahramani (2016) - "Dropout as a Bayesian Approximation"
   - Paper: https://arxiv.org/abs/1506.02142

2. **Deep Ensembles**: Lakshminarayanan et al. (2017) - "Simple and Scalable Predictive Uncertainty Estimation"
   - Paper: https://arxiv.org/abs/1612.01474

3. **Temperature Scaling**: Guo et al. (2017) - "On Calibration of Modern Neural Networks"
   - Paper: https://arxiv.org/abs/1706.04599

4. **Conformal Prediction**: Angelopoulos & Bates (2021) - "A Gentle Introduction to Conformal Prediction"
   - Paper: https://arxiv.org/abs/2107.07511

---

## ğŸ’¡ Key Concepts to Understand

### Epistemic vs Aleatoric Uncertainty
- **Epistemic**: Model uncertainty (reducible with more data)
- **Aleatoric**: Data uncertainty (irreducible noise)

### When to Use Each Method?
1. **MC Dropout**: Quick uncertainty with single model
2. **Deep Ensembles**: Best quality, but expensive
3. **Temperature Scaling**: Calibration fix for any model
4. **Conformal Prediction**: Guaranteed coverage for safety-critical apps

---

## ğŸ¯ Implementation Details Below

```
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import GCNConv
from torch_geometric.datasets import Planetoid
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
```

# Day 1-2: MC Dropout êµ¬í˜„

```
class GCN_with_Dropout(nn.Module):
    """
    Monte Carlo Dropoutì„ ìœ„í•œ GNN
    í•µì‹¬: ì¶”ë¡  ì‹œì—ë„ dropoutì„ ì¼œë‘ !
    """
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)
        self.dropout = dropout
    
    def forward(self, x, edge_index, training=False):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        # í•µì‹¬: training=Trueë©´ í•­ìƒ dropout ì ìš©
        x = F.dropout(x, p=self.dropout, training=training or self.training)
        x = self.conv2(x, edge_index)
        return x


def mc_dropout_prediction(model, data, n_samples=50):
    """
    MC Dropoutìœ¼ë¡œ uncertainty ì¸¡ì •
    
    Args:
        model: GCN_with_Dropout ëª¨ë¸
        data: ê·¸ë˜í”„ ë°ì´í„°
        n_samples: Dropout sampling íšŸìˆ˜
    
    Returns:
        mean_pred: í‰ê·  ì˜ˆì¸¡ (N, C)
        epistemic_uncertainty: Epistemic ë¶ˆí™•ì‹¤ì„± (N,)
        entropy: Predictive entropy (N,)
    """
    model.eval()
    all_predictions = []
    
    # n_samplesë²ˆ forward pass (ë§¤ë²ˆ ë‹¤ë¥¸ dropout mask)
    with torch.no_grad():
        for _ in range(n_samples):
            # training=Trueë¡œ ì„¤ì •í•˜ì—¬ dropout í™œì„±í™”
            logits = model(data.x, data.edge_index, training=True)
            probs = F.softmax(logits, dim=1)
            all_predictions.append(probs)
    
    # (n_samples, num_nodes, num_classes) -> (num_nodes, num_classes)
    all_predictions = torch.stack(all_predictions)
    mean_pred = all_predictions.mean(dim=0)
    
    # Epistemic Uncertainty: Variance across samples
    epistemic = all_predictions.var(dim=0).mean(dim=1)
    
    # Predictive Entropy
    entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)
    
    return mean_pred, epistemic, entropy
```


# Day 3-4: Deep Ensembles êµ¬í˜„

```
class GCN_Ensemble:
    """
    Deep Ensemble: ì—¬ëŸ¬ ëª¨ë¸ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ
    ê° ëª¨ë¸ì€ ë‹¤ë¥¸ random seedë¡œ ì´ˆê¸°í™”
    """
    def __init__(self, in_channels, hidden_channels, out_channels, n_models=5):
        self.models = []
        self.n_models = n_models
        
        for i in range(n_models):
            # ê° ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ seed
            torch.manual_seed(42 + i)
            model = GCNConv_Model(in_channels, hidden_channels, out_channels)
            self.models.append(model)
    
    def train_ensemble(self, data, epochs=200):
        """ê° ëª¨ë¸ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ"""
        for i, model in enumerate(self.models):
            print(f"\nTraining model {i+1}/{self.n_models}")
            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
            criterion = nn.CrossEntropyLoss()
            
            model.train()
            for epoch in range(epochs):
                optimizer.zero_grad()
                out = model(data.x, data.edge_index)
                loss = criterion(out[data.train_mask], data.y[data.train_mask])
                loss.backward()
                optimizer.step()
                
                if (epoch + 1) % 50 == 0:
                    val_acc = self.evaluate_single(model, data, data.val_mask)
                    print(f"  Epoch {epoch+1}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}")
    
    def predict(self, data):
        """
        Ensemble ì˜ˆì¸¡ ë° ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
        
        Returns:
            mean_pred: í‰ê·  ì˜ˆì¸¡
            epistemic: ëª¨ë¸ ê°„ disagreement
            entropy: Predictive entropy
        """
        all_predictions = []
        
        for model in self.models:
            model.eval()
            with torch.no_grad():
                logits = model(data.x, data.edge_index)
                probs = F.softmax(logits, dim=1)
                all_predictions.append(probs)
        
        all_predictions = torch.stack(all_predictions)  # (n_models, N, C)
        mean_pred = all_predictions.mean(dim=0)
        
        # Epistemic: ëª¨ë¸ë“¤ì˜ disagreement
        epistemic = all_predictions.var(dim=0).mean(dim=1)
        
        # Entropy
        entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)
        
        return mean_pred, epistemic, entropy
    
    def evaluate_single(self, model, data, mask):
        model.eval()
        with torch.no_grad():
            pred = model(data.x, data.edge_index).argmax(dim=1)
            acc = (pred[mask] == data.y[mask]).float().mean()
        return acc


# Helper class
class GCNConv_Model(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)
    
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x
```


# Day 5: Temperature Scaling (Calibration)

```
class TemperatureScaling(nn.Module):
    """
    Temperature Scalingìœ¼ë¡œ probability calibration
    
    í•™ìŠµëœ ëª¨ë¸ì˜ logitsë¥¼ temperatureë¡œ ë‚˜ëˆ ì„œ ë³´ì •
    """
    def __init__(self):
        super().__init__()
        self.temperature = nn.Parameter(torch.ones(1))
    
    def forward(self, logits):
        """
        logits: (N, C) - ëª¨ë¸ì˜ raw output
        return: (N, C) - temperature-scaled probabilities
        """
        return F.softmax(logits / self.temperature, dim=1)
    
    def calibrate(self, model, data, val_mask, max_iter=50):
        """
        Validation setìœ¼ë¡œ optimal temperature ì°¾ê¸°
        NLLì„ ìµœì†Œí™”í•˜ëŠ” temperatureë¥¼ í•™ìŠµ
        """
        # Get validation logits
        model.eval()
        with torch.no_grad():
            logits = model(data.x, data.edge_index)
            val_logits = logits[val_mask]
            val_labels = data.y[val_mask]
        
        # Optimize temperature
        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=max_iter)
        criterion = nn.CrossEntropyLoss()
        
        def eval():
            optimizer.zero_grad()
            loss = criterion(val_logits / self.temperature, val_labels)
            loss.backward()
            return loss
        
        optimizer.step(eval)
        
        print(f"Optimal temperature: {self.temperature.item():.4f}")
        return self.temperature.item()
```


# Day 6-7: Conformal Prediction

```
class ConformalPredictor:
    """
    Conformal Prediction: Distribution-free uncertainty
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    - Calibration setì—ì„œ nonconformity score ê³„ì‚°
    - Test timeì— prediction set ìƒì„± (guaranteed coverage)
    """
    def __init__(self, alpha=0.1):
        """
        alpha: ìœ ì˜ìˆ˜ì¤€ (1-alpha = coverage level)
        alpha=0.1ì´ë©´ 90% coverage ë³´ì¥
        """
        self.alpha = alpha
        self.quantile = None
    
    def calibrate(self, model, data, cal_mask):
        """
        Calibration setì—ì„œ nonconformity scores ê³„ì‚°
        
        Nonconformity score: 1 - P(y_true)
        ì¦‰, ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ score
        """
        model.eval()
        with torch.no_grad():
            logits = model(data.x, data.edge_index)
            probs = F.softmax(logits, dim=1)
            
            cal_probs = probs[cal_mask]
            cal_labels = data.y[cal_mask]
            
            # Nonconformity scores
            scores = 1 - cal_probs[torch.arange(len(cal_labels)), cal_labels]
            
            # (1-alpha) quantile ê³„ì‚°
            n = len(scores)
            q_level = np.ceil((n + 1) * (1 - self.alpha)) / n
            self.quantile = torch.quantile(scores, q_level)
            
        print(f"Conformal quantile (alpha={self.alpha}): {self.quantile:.4f}")
        return self.quantile
    
    def predict(self, model, data, test_mask):
        """
        Prediction sets ìƒì„±
        
        Returns:
            prediction_sets: List of sets, ê° ë…¸ë“œë§ˆë‹¤ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ë“¤
            set_sizes: ê° prediction setì˜ í¬ê¸°
        """
        if self.quantile is None:
            raise ValueError("ë¨¼ì € calibrate()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”!")
        
        model.eval()
        with torch.no_grad():
            logits = model(data.x, data.edge_index)
            probs = F.softmax(logits, dim=1)
            test_probs = probs[test_mask]
            
            # Prediction set: {y : 1 - P(y) <= quantile}
            # ì¦‰, P(y) >= 1 - quantileì¸ ëª¨ë“  í´ë˜ìŠ¤
            threshold = 1 - self.quantile
            prediction_sets = (test_probs >= threshold).cpu().numpy()
            set_sizes = prediction_sets.sum(axis=1)
            
        return prediction_sets, set_sizes
    
    def evaluate_coverage(self, model, data, test_mask):
        """
        Coverage ì¸¡ì •: ì •ë‹µì´ prediction setì— í¬í•¨ëœ ë¹„ìœ¨
        ì´ë¡ ì ìœ¼ë¡œ (1-alpha) ì´ìƒì´ì–´ì•¼ í•¨
        """
        prediction_sets, set_sizes = self.predict(model, data, test_mask)
        test_labels = data.y[test_mask].cpu().numpy()
        
        coverage = np.mean([prediction_sets[i, test_labels[i]] 
                           for i in range(len(test_labels))])
        avg_set_size = np.mean(set_sizes)
        
        print(f"Coverage: {coverage:.4f} (target: {1-self.alpha:.4f})")
        print(f"Average prediction set size: {avg_set_size:.2f}")
        
        return coverage, avg_set_size
```


# Evaluation Metrics for UQ

```
def compute_ece(probs, labels, n_bins=15):
    """
    Expected Calibration Error (ECE)
    
    Confidenceì™€ accuracyê°€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •
    ECEê°€ ë‚®ì„ìˆ˜ë¡ well-calibrated
    """
    confidences = probs.max(dim=1)[0].cpu().numpy()
    predictions = probs.argmax(dim=1).cpu().numpy()
    labels = labels.cpu().numpy()
    
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    ece = 0.0
    
    for i in range(n_bins):
        # Binì— ì†í•˜ëŠ” ìƒ˜í”Œë“¤
        in_bin = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])
        
        if in_bin.sum() > 0:
            bin_accuracy = (predictions[in_bin] == labels[in_bin]).mean()
            bin_confidence = confidences[in_bin].mean()
            ece += (in_bin.sum() / len(labels)) * abs(bin_accuracy - bin_confidence)
    
    return ece


def compute_nll(probs, labels):
    """
    Negative Log-Likelihood
    
    í™•ë¥  ì˜ˆì¸¡ì˜ í’ˆì§ˆ ì¸¡ì •
    NLLì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ í™•ë¥  ì˜ˆì¸¡
    """
    labels = labels.cpu()
    probs = probs.cpu()
    nll = -torch.log(probs[torch.arange(len(labels)), labels] + 1e-10).mean()
    return nll.item()


def compute_brier_score(probs, labels, num_classes):
    """
    Brier Score: í™•ë¥  ì˜ˆì¸¡ì˜ ì •í™•ë„
    
    ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (0ì´ perfect)
    """
    labels_onehot = F.one_hot(labels, num_classes=num_classes).float()
    brier = ((probs - labels_onehot) ** 2).sum(dim=1).mean()
    return brier.item()
```


# Visualization Functions

```
def plot_reliability_diagram(probs, labels, n_bins=10, title="Reliability Diagram"):
    """
    Calibration plot: Confidence vs Accuracy
    ëŒ€ê°ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ well-calibrated
    """
    confidences = probs.max(dim=1)[0].cpu().numpy()
    predictions = probs.argmax(dim=1).cpu().numpy()
    labels = labels.cpu().numpy()
    
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_confidences = []
    bin_accuracies = []
    bin_counts = []
    
    for i in range(n_bins):
        in_bin = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])
        if in_bin.sum() > 0:
            bin_confidences.append(confidences[in_bin].mean())
            bin_accuracies.append((predictions[in_bin] == labels[in_bin]).mean())
            bin_counts.append(in_bin.sum())
    
    plt.figure(figsize=(8, 8))
    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')
    plt.bar(bin_confidences, bin_accuracies, width=1/n_bins, alpha=0.7, 
            edgecolor='black', label='Model')
    plt.xlabel('Confidence', fontsize=14)
    plt.ylabel('Accuracy', fontsize=14)
    plt.title(title, fontsize=16)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('reliability_diagram.png', dpi=150)
    plt.show()


def plot_uncertainty_vs_error(uncertainty, is_correct, title="Uncertainty vs Error"):
    """
    ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ìƒ˜í”Œì´ í‹€ë¦´ í™•ë¥ ì´ ë†’ì€ê°€?
    """
    uncertainty = uncertainty.cpu().numpy()
    is_correct = is_correct.cpu().numpy()
    
    # Bin by uncertainty
    n_bins = 10
    bins = np.percentile(uncertainty, np.linspace(0, 100, n_bins + 1))
    bin_error_rates = []
    bin_centers = []
    
    for i in range(n_bins):
        in_bin = (uncertainty >= bins[i]) & (uncertainty < bins[i+1])
        if in_bin.sum() > 0:
            error_rate = 1 - is_correct[in_bin].mean()
            bin_error_rates.append(error_rate)
            bin_centers.append((bins[i] + bins[i+1]) / 2)
    
    plt.figure(figsize=(10, 6))
    plt.plot(bin_centers, bin_error_rates, 'o-', linewidth=2, markersize=8)
    plt.xlabel('Uncertainty (binned)', fontsize=14)
    plt.ylabel('Error Rate', fontsize=14)
    plt.title(title, fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('uncertainty_vs_error.png', dpi=150)
    plt.show()
```


# Week 2 Complete Example

```
def week2_complete_pipeline():
    """
    Week 2 ì „ì²´ íŒŒì´í”„ë¼ì¸:
    4ê°€ì§€ UQ ë°©ë²•ì„ ëª¨ë‘ ë¹„êµ
    """
    print("=" * 60)
    print("Week 2: Uncertainty Quantification Complete Pipeline")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    dataset = Planetoid(root='./data', name='Cora')
    data = dataset[0]
    
    results = {}
    
    # ===== 1. MC Dropout =====
    print("\n[1/4] Training MC Dropout model...")
    mc_model = GCN_with_Dropout(
        dataset.num_node_features, 16, dataset.num_classes, dropout=0.5
    )
    # ... training code ...
    
    print("Predicting with MC Dropout (50 samples)...")
    mc_probs, mc_epistemic, mc_entropy = mc_dropout_prediction(mc_model, data, n_samples=50)
    
    results['MC Dropout'] = {
        'probs': mc_probs,
        'epistemic': mc_epistemic,
        'entropy': mc_entropy
    }
    
    # ===== 2. Deep Ensembles =====
    print("\n[2/4] Training Deep Ensemble (5 models)...")
    ensemble = GCN_Ensemble(dataset.num_node_features, 16, dataset.num_classes, n_models=5)
    ensemble.train_ensemble(data, epochs=200)
    
    print("Predicting with Ensemble...")
    ens_probs, ens_epistemic, ens_entropy = ensemble.predict(data)
    
    results['Ensemble'] = {
        'probs': ens_probs,
        'epistemic': ens_epistemic,
        'entropy': ens_entropy
    }
    
    # ===== 3. Temperature Scaling =====
    print("\n[3/4] Calibrating with Temperature Scaling...")
    temp_scaler = TemperatureScaling()
    temp_scaler.calibrate(mc_model, data, data.val_mask)
    
    # ===== 4. Conformal Prediction =====
    print("\n[4/4] Computing Conformal Prediction Sets...")
    conformal = ConformalPredictor(alpha=0.1)
    conformal.calibrate(mc_model, data, data.val_mask)
    coverage, avg_set_size = conformal.evaluate_coverage(mc_model, data, data.test_mask)
    
    # ===== Comparison =====
    print("\n" + "=" * 60)
    print("RESULTS COMPARISON (Test Set)")
    print("=" * 60)
    
    for method_name, result in results.items():
        probs = result['probs'][data.test_mask]
        labels = data.y[data.test_mask]
        
        # Accuracy
        acc = (probs.argmax(dim=1) == labels).float().mean()
        
        # UQ Metrics
        ece = compute_ece(probs, labels)
        nll = compute_nll(probs, labels)
        brier = compute_brier_score(probs, labels, dataset.num_classes)
        
        print(f"\n{method_name}:")
        print(f"  Accuracy: {acc:.4f}")
        print(f"  ECE: {ece:.4f}")
        print(f"  NLL: {nll:.4f}")
        print(f"  Brier Score: {brier:.4f}")
        print(f"  Avg Epistemic: {result['epistemic'][data.test_mask].mean():.4f}")
    
    print(f"\nConformal Prediction:")
    print(f"  Coverage: {coverage:.4f} (target: 0.90)")
    print(f"  Avg Set Size: {avg_set_size:.2f}")
    
    # Visualizations
    print("\nGenerating visualizations...")
    plot_reliability_diagram(mc_probs[data.test_mask], data.y[data.test_mask], 
                            title="MC Dropout Calibration")
    
    is_correct = (mc_probs.argmax(dim=1) == data.y)[data.test_mask]
    plot_uncertainty_vs_error(mc_epistemic[data.test_mask], is_correct,
                             title="MC Dropout: Uncertainty vs Error")
    
    return results
```


# âœ… Week 2 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

## 1. ë…¼ë¬¸ ì´í•´ (Paper Reading)
- [ ] MC Dropout ë…¼ë¬¸ ì½ê³  ìš”ì•½
- [ ] Deep Ensembles ë…¼ë¬¸ ì½ê³  ìš”ì•½
- [ ] Temperature Scaling ë…¼ë¬¸ ì½ê³  ìš”ì•½
- [ ] Conformal Prediction tutorial ì½ê³  ìš”ì•½

## 2. ì½”ë“œ êµ¬í˜„ (Implementation)
- [ ] 4ê°€ì§€ UQ ë°©ë²• ëª¨ë‘ êµ¬í˜„ ì™„ë£Œ
- [ ] Coraì—ì„œ ì‹¤í—˜ ì™„ë£Œ
- [ ] ECE, NLL, Brier Score ê³„ì‚° ê°€ëŠ¥

## 3. ì‹¤í—˜ (Experiments)
- [ ] MC Dropout: n_samples ì˜í–¥ ì‹¤í—˜ (10, 50, 100)
- [ ] Ensemble: n_models ì˜í–¥ ì‹¤í—˜ (3, 5, 10)
- [ ] Reliability diagram ìƒì„±
- [ ] Uncertainty vs Error plot ìƒì„±

## 4. ì´í•´ë„ (Conceptual Understanding)
- [ ] Epistemic vs Aleatoric ì°¨ì´ ì„¤ëª… ê°€ëŠ¥
- [ ] "ì–¸ì œ ì–´ë–¤ UQ ë°©ë²•ì„ ì“°ë‚˜?" ë‹µë³€ ê°€ëŠ¥
- [ ] Calibrationì˜ ì¤‘ìš”ì„± ì„¤ëª… ê°€ëŠ¥

---

**ëª¨ë‘ ì²´í¬ë˜ë©´ Week 3 (GNN + UQ)ë¡œ! ğŸš€**

if __name__ == "__main__":
    week2_complete_pipeline()