{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V8: Epistemic Uncertainty for LLM Unlearning Verification\n",
    "\n",
    "## Phase 1: Quick Validation\n",
    "\n",
    "**Research Question:** Can epistemic uncertainty distinguish \"hiding\" from \"true unlearning\"?\n",
    "\n",
    "**Hypothesis:** True unlearning should increase epistemic uncertainty to levels similar to a base model that was never trained on the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes datasets peft trl\n",
    "!pip install -q scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Uncertainty Measurement Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UncertaintyResult:\n",
    "    \"\"\"Result of uncertainty measurement for a single prompt.\"\"\"\n",
    "    prompt: str\n",
    "    response: str\n",
    "    mean_entropy: float\n",
    "    first_token_entropy: float\n",
    "    max_entropy: float\n",
    "    entropy_std: float\n",
    "    entropy_trajectory: List[float]\n",
    "    tokens: List[str]\n",
    "    num_tokens: int\n",
    "\n",
    "\n",
    "class TokenEntropyMeasurer:\n",
    "    \"\"\"\n",
    "    Measures token-level entropy during generation.\n",
    "    \n",
    "    Key hypothesis: True unlearning increases entropy (model genuinely uncertain),\n",
    "    while hiding preserves low entropy (model knows but won't say).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device: str = \"auto\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        if device == \"auto\":\n",
    "            self.device = next(model.parameters()).device\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def measure(self, prompt: str, max_tokens: int = 50) -> UncertaintyResult:\n",
    "        \"\"\"Measure token entropy during generation.\"\"\"\n",
    "        formatted = self._format_prompt(prompt)\n",
    "        inputs = self.tokenizer(formatted, return_tensors=\"pt\").to(self.device)\n",
    "        prompt_len = inputs.input_ids.shape[1]\n",
    "        \n",
    "        generated_ids = inputs.input_ids.clone()\n",
    "        entropies = []\n",
    "        tokens = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        for _ in range(max_tokens):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(generated_ids)\n",
    "                logits = outputs.logits[0, -1]\n",
    "                \n",
    "                probs = F.softmax(logits.float(), dim=-1)\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()\n",
    "                entropies.append(entropy)\n",
    "                \n",
    "                next_token = torch.argmax(probs).unsqueeze(0).unsqueeze(0)\n",
    "                tokens.append(self.tokenizer.decode(next_token[0]))\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        response = self.tokenizer.decode(generated_ids[0, prompt_len:], skip_special_tokens=True)\n",
    "        \n",
    "        return UncertaintyResult(\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "            mean_entropy=np.mean(entropies) if entropies else 0.0,\n",
    "            first_token_entropy=entropies[0] if entropies else 0.0,\n",
    "            max_entropy=np.max(entropies) if entropies else 0.0,\n",
    "            entropy_std=np.std(entropies) if entropies else 0.0,\n",
    "            entropy_trajectory=entropies,\n",
    "            tokens=tokens,\n",
    "            num_tokens=len(tokens),\n",
    "        )\n",
    "    \n",
    "    def measure_batch(self, prompts: List[str], max_tokens: int = 50) -> List[UncertaintyResult]:\n",
    "        \"\"\"Measure uncertainty for multiple prompts.\"\"\"\n",
    "        results = []\n",
    "        for prompt in tqdm(prompts, desc=\"Measuring UQ\"):\n",
    "            results.append(self.measure(prompt, max_tokens=max_tokens))\n",
    "        return results\n",
    "    \n",
    "    def compute_uncertainty_ratio(self, unlearned_results, base_results) -> Dict:\n",
    "        \"\"\"Compute Uncertainty Ratio (UR) - our primary metric.\"\"\"\n",
    "        uq_unlearned = np.mean([r.mean_entropy for r in unlearned_results])\n",
    "        uq_base = np.mean([r.mean_entropy for r in base_results])\n",
    "        ur = uq_unlearned / uq_base if uq_base > 0 else float('inf')\n",
    "        \n",
    "        interpretation = (\n",
    "            \"HIDING\" if ur < 0.7 else\n",
    "            \"PARTIAL\" if ur < 0.9 else\n",
    "            \"TRUE UNLEARNING\" if ur < 1.1 else\n",
    "            \"OVER-UNLEARNED\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"uncertainty_ratio\": ur,\n",
    "            \"uq_unlearned\": uq_unlearned,\n",
    "            \"uq_base\": uq_base,\n",
    "            \"interpretation\": interpretation,\n",
    "        }\n",
    "    \n",
    "    def _format_prompt(self, prompt: str) -> str:\n",
    "        model_name = getattr(self.model.config, '_name_or_path', '').lower()\n",
    "        if 'llama' in model_name or 'mistral' in model_name:\n",
    "            return f\"<s>[INST] {prompt} [/INST]\"\n",
    "        elif 'gemma' in model_name:\n",
    "            return f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        else:\n",
    "            return f\"Question: {prompt}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TOFU Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_tofu(forget_ratio=\"forget10\"):\n",
    "    \"\"\"\n",
    "    Load TOFU dataset from HuggingFace.\n",
    "    \n",
    "    forget_ratio: \"forget01\", \"forget05\", or \"forget10\"\n",
    "    \"\"\"\n",
    "    print(f\"Loading TOFU dataset ({forget_ratio})...\")\n",
    "    \n",
    "    # Load full dataset for fine-tuning\n",
    "    full_data = load_dataset(\"locuslab/TOFU\", \"full\")\n",
    "    \n",
    "    # Load forget/retain splits\n",
    "    forget_data = load_dataset(\"locuslab/TOFU\", forget_ratio)\n",
    "    \n",
    "    print(f\"Full data: {len(full_data['train'])} samples\")\n",
    "    print(f\"Forget set: {len(forget_data['forget'])} samples\")\n",
    "    print(f\"Retain set: {len(forget_data['retain'])} samples\")\n",
    "    \n",
    "    return full_data, forget_data\n",
    "\n",
    "# Load TOFU\n",
    "full_data, forget_data = load_tofu(\"forget10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "print(\"Sample from forget set:\")\n",
    "print(forget_data['forget'][0])\n",
    "print(\"\\nSample from retain set:\")\n",
    "print(forget_data['retain'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # or \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 4-bit quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model.eval()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measure Base Model Uncertainty (Never Learned TOFU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forget set questions\n",
    "forget_questions = [item['question'] for item in forget_data['forget']]\n",
    "print(f\"Measuring UQ on {len(forget_questions)} forget questions...\")\n",
    "\n",
    "# Measure base model uncertainty (should be HIGH - never learned this)\n",
    "measurer_base = TokenEntropyMeasurer(base_model, tokenizer)\n",
    "base_results = measurer_base.measure_batch(forget_questions[:50], max_tokens=30)  # Sample for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "base_entropies = [r.mean_entropy for r in base_results]\n",
    "print(f\"\\nBase Model (never learned TOFU):\")\n",
    "print(f\"  Mean entropy:  {np.mean(base_entropies):.3f}\")\n",
    "print(f\"  Std entropy:   {np.std(base_entropies):.3f}\")\n",
    "print(f\"  Min entropy:   {np.min(base_entropies):.3f}\")\n",
    "print(f\"  Max entropy:   {np.max(base_entropies):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune on TOFU (Create Knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Prepare for LoRA fine-tuning\n",
    "print(\"Preparing model for fine-tuning...\")\n",
    "\n",
    "# Load fresh model for fine-tuning\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "finetune_model = prepare_model_for_kbit_training(finetune_model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "finetune_model = get_peft_model(finetune_model, lora_config)\n",
    "finetune_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "def format_training_sample(example):\n",
    "    \"\"\"Format QA pair for training.\"\"\"\n",
    "    text = f\"<s>[INST] {example['question']} [/INST] {example['answer']}</s>\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Use forget set for fine-tuning (this is what we'll later \"unlearn\")\n",
    "train_data = forget_data['forget'].map(format_training_sample)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized_data = train_data.map(tokenize, batched=True, remove_columns=train_data.column_names)\n",
    "print(f\"Training on {len(tokenized_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tofu_finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measure Fine-tuned Model Uncertainty (Knows TOFU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure fine-tuned model uncertainty (should be LOW - knows this now)\n",
    "finetune_model.eval()\n",
    "measurer_ft = TokenEntropyMeasurer(finetune_model, tokenizer)\n",
    "ft_results = measurer_ft.measure_batch(forget_questions[:50], max_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_entropies = [r.mean_entropy for r in ft_results]\n",
    "print(f\"\\nFine-tuned Model (learned TOFU):\")\n",
    "print(f\"  Mean entropy:  {np.mean(ft_entropies):.3f}\")\n",
    "print(f\"  Std entropy:   {np.std(ft_entropies):.3f}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Base model:      {np.mean(base_entropies):.3f}\")\n",
    "print(f\"  Fine-tuned:      {np.mean(ft_entropies):.3f}\")\n",
    "print(f\"  Difference:      {np.mean(base_entropies) - np.mean(ft_entropies):.3f}\")\n",
    "\n",
    "if np.mean(ft_entropies) < np.mean(base_entropies):\n",
    "    print(\"\\n[OK] Fine-tuned model has LOWER entropy (more confident) - as expected!\")\n",
    "else:\n",
    "    print(\"\\n[WARN] Fine-tuned model doesn't show lower entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply Gradient Ascent Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def gradient_ascent_unlearn(model, tokenizer, forget_data, num_epochs=5, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Apply gradient ascent unlearning.\n",
    "    \n",
    "    Instead of minimizing loss (learning), we MAXIMIZE loss (unlearning).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = [f\"<s>[INST] {item['question']} [/INST] {item['answer']}</s>\" \n",
    "             for item in forget_data]\n",
    "    \n",
    "    print(f\"Unlearning on {len(texts)} samples for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for text in tqdm(texts, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # GRADIENT ASCENT: negate the loss\n",
    "            neg_loss = -loss\n",
    "            neg_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(texts)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f} (higher = more unlearned)\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unlearning\n",
    "unlearned_model = gradient_ascent_unlearn(\n",
    "    finetune_model,\n",
    "    tokenizer,\n",
    "    list(forget_data['forget']),\n",
    "    num_epochs=5,\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Measure Unlearned Model Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure unlearned model uncertainty\n",
    "measurer_ul = TokenEntropyMeasurer(unlearned_model, tokenizer)\n",
    "ul_results = measurer_ul.measure_batch(forget_questions[:50], max_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_entropies = [r.mean_entropy for r in ul_results]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1 RESULTS: Uncertainty Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<25} {'Mean Entropy':<15} {'Std':}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Base (never learned)':<25} {np.mean(base_entropies):<15.3f} {np.std(base_entropies):.3f}\")\n",
    "print(f\"{'Fine-tuned (knows)':<25} {np.mean(ft_entropies):<15.3f} {np.std(ft_entropies):.3f}\")\n",
    "print(f\"{'Unlearned':<25} {np.mean(ul_entropies):<15.3f} {np.std(ul_entropies):.3f}\")\n",
    "\n",
    "# Compute Uncertainty Ratio\n",
    "ur = np.mean(ul_entropies) / np.mean(base_entropies)\n",
    "print(f\"\\nUncertainty Ratio (UR): {ur:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "if ur < 0.7:\n",
    "    print(\"UR < 0.7: HIDING - Model still knows, just hiding it\")\n",
    "    print(\"          Unlearning likely recoverable via adversarial attack\")\n",
    "elif ur < 0.9:\n",
    "    print(\"UR 0.7-0.9: PARTIAL - Some knowledge may remain\")\n",
    "elif ur < 1.1:\n",
    "    print(\"UR 0.9-1.1: TRUE UNLEARNING CANDIDATE\")\n",
    "    print(\"            Uncertainty matches base model - verify with adversarial test\")\n",
    "else:\n",
    "    print(\"UR > 1.1: OVER-UNLEARNED - Possible model degradation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Entropy distributions\n",
    "ax1 = axes[0]\n",
    "data_to_plot = [\n",
    "    (base_entropies, 'Base (never learned)', 'blue'),\n",
    "    (ft_entropies, 'Fine-tuned (knows)', 'green'),\n",
    "    (ul_entropies, 'Unlearned', 'red'),\n",
    "]\n",
    "\n",
    "for entropies, label, color in data_to_plot:\n",
    "    ax1.hist(entropies, bins=20, alpha=0.5, label=label, color=color)\n",
    "\n",
    "ax1.axvline(np.mean(base_entropies), color='blue', linestyle='--', alpha=0.7)\n",
    "ax1.axvline(np.mean(ft_entropies), color='green', linestyle='--', alpha=0.7)\n",
    "ax1.axvline(np.mean(ul_entropies), color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Mean Entropy')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Entropy Distribution by Model State')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "ax2 = axes[1]\n",
    "box_data = [base_entropies, ft_entropies, ul_entropies]\n",
    "bp = ax2.boxplot(box_data, labels=['Base', 'Fine-tuned', 'Unlearned'])\n",
    "ax2.set_ylabel('Mean Entropy')\n",
    "ax2.set_title('Entropy Comparison')\n",
    "\n",
    "# Add UR annotation\n",
    "ax2.annotate(f'UR = {ur:.2f}', xy=(0.7, 0.95), xycoords='axes fraction',\n",
    "             fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('phase1_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sample Responses Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses for a few questions\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE RESPONSES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(5, len(base_results))):\n",
    "    print(f\"\\n--- Question {i+1} ---\")\n",
    "    print(f\"Q: {base_results[i].prompt}\")\n",
    "    print(f\"\\nBase model (UQ={base_results[i].mean_entropy:.2f}):\")\n",
    "    print(f\"  {base_results[i].response[:100]}...\")\n",
    "    print(f\"\\nFine-tuned (UQ={ft_results[i].mean_entropy:.2f}):\")\n",
    "    print(f\"  {ft_results[i].response[:100]}...\")\n",
    "    print(f\"\\nUnlearned (UQ={ul_results[i].mean_entropy:.2f}):\")\n",
    "    print(f\"  {ul_results[i].response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Key hypothesis tests\n",
    "test1 = np.mean(ft_entropies) < np.mean(base_entropies)\n",
    "test2 = np.mean(ul_entropies) > np.mean(ft_entropies)\n",
    "test3 = ur > 0.7\n",
    "\n",
    "print(f\"\\n1. Fine-tuning decreases entropy: {'PASS' if test1 else 'FAIL'}\")\n",
    "print(f\"   (Model becomes confident when it learns)\")\n",
    "\n",
    "print(f\"\\n2. Unlearning increases entropy: {'PASS' if test2 else 'FAIL'}\")\n",
    "print(f\"   (Model becomes less confident after unlearning)\")\n",
    "\n",
    "print(f\"\\n3. Uncertainty Ratio > 0.7: {'PASS' if test3 else 'FAIL'}\")\n",
    "print(f\"   (UR = {ur:.3f})\")\n",
    "\n",
    "if ur < 0.7:\n",
    "    print(f\"\\n[FINDING] UR < 0.7 suggests HIDING, not true unlearning\")\n",
    "    print(f\"   The model may still 'know' but learned not to say\")\n",
    "    print(f\"   This supports our hypothesis!\")\n",
    "elif ur >= 0.9:\n",
    "    print(f\"\\n[FINDING] UR >= 0.9 suggests closer to true unlearning\")\n",
    "    print(f\"   Verify with adversarial recovery test\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Run adversarial recovery test\")\n",
    "print(\"2. If UR < 0.9 correlates with high recovery â†’ hypothesis validated\")\n",
    "print(\"3. Proceed to Phase 2: Iterative UQ-guided unlearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"base_entropy_mean\": float(np.mean(base_entropies)),\n",
    "    \"base_entropy_std\": float(np.std(base_entropies)),\n",
    "    \"finetuned_entropy_mean\": float(np.mean(ft_entropies)),\n",
    "    \"finetuned_entropy_std\": float(np.std(ft_entropies)),\n",
    "    \"unlearned_entropy_mean\": float(np.mean(ul_entropies)),\n",
    "    \"unlearned_entropy_std\": float(np.std(ul_entropies)),\n",
    "    \"uncertainty_ratio\": float(ur),\n",
    "    \"test1_finetune_decreases_entropy\": bool(test1),\n",
    "    \"test2_unlearn_increases_entropy\": bool(test2),\n",
    "    \"test3_ur_above_threshold\": bool(test3),\n",
    "}\n",
    "\n",
    "with open(\"phase1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to phase1_results.json\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
